[
    {
        "site": "docs",
        "rank": 1,
        "documentId": "0b4685a7cf2ff912a6f6a120d9376dde",
        "title": "The big picture",
        "description": "",
        "text": "If you're new to Platform.sh, we recommend starting with Structure and Build \u0026 Deploy, which will get you started on the right track to best leverage Platform.sh.",
        "section": "The big picture",
        "subsections": "",
        "image": "",
        "url": "/overview.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "8142dabde8d4ef556ed9da8dbe0a37fe",
        "title": "Configuration",
        "description": "",
        "text": "Platform.sh projects are configured with three YAML files that define three types of containers in your virtual cluster: one *Router* container, one or more *Application* containers, and a number of optional *Service* containers. See how each of their configuration files are defined below.",
        "section": "Configuration",
        "subsections": "",
        "image": "",
        "url": "/configuration.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "0b1bf164ed9f4c760138f799c33a698e",
        "title": "Introduction",
        "description": "",
        "text": "The easiest way to get started working with Platform.sh is to try it out yourself. Open up a free trial account and explore the Getting Started guides below.",
        "section": "Introduction",
        "subsections": "",
        "image": "",
        "url": "/gettingstarted.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "5c7956314319fad802a71ee12cd46297",
        "title": "Best practices",
        "description": "",
        "text": "Here are some tips for getting the most out of Platform.sh's many features.",
        "section": "Best practices",
        "subsections": "",
        "image": "",
        "url": "/bestpractices.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "e5e8263f309f333d2956c6ac2d8e4ffd",
        "title": "Languages",
        "description": "",
        "text": "We sure do support a lot of runtimes.",
        "section": "Languages",
        "subsections": "",
        "image": "",
        "url": "/languages.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "9e07170edfe4469e380f9c5b8dba1a3e",
        "title": "Development",
        "description": "",
        "text": "This section contains resources related to tools and troubleshooting information related to developing with Platform.sh",
        "section": "Development",
        "subsections": "",
        "image": "",
        "url": "/development.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "049242b18eba78b1288a433742ae15b0",
        "title": "Integrations",
        "description": "",
        "text": "Integrate all the things!",
        "section": "Integrations",
        "subsections": "",
        "image": "",
        "url": "/integrations.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "c2a5a7a1505e690fbeddcea3b0342060",
        "title": "Going live",
        "description": "",
        "text": "The following resources will help you take your application live.",
        "section": "Going live",
        "subsections": "",
        "image": "",
        "url": "/golive.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "23f5e0e3b5aca70a7c3df162dfab6bc3",
        "title": "Administration",
        "description": "",
        "text": "Administration tasks for your Platform.sh projects is accessible from within the management console, as well as through the CLI.",
        "section": "Administration",
        "subsections": "",
        "image": "",
        "url": "/administration.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "63c867b8cb3d97c8f9ef5512f3ecc922",
        "title": "Security and compliance",
        "description": "",
        "text": "Platform.sh takes your privacy seriously. We’re compliant with the European GDPR (DPA available), German BDSG (DPA available), and Canadian PIPEDA.",
        "section": "Security and compliance",
        "subsections": "",
        "image": "",
        "url": "/security.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "8419fb6e269c2c7345a043f81b45a77a",
        "title": "Featured frameworks",
        "description": "",
        "text": "The following section covers specific web frameworks, as well as best practices and important considerations for deploying them on Platform.sh.",
        "section": "Featured frameworks",
        "subsections": "",
        "image": "",
        "url": "/frameworks.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "e39624dd3a8bc4b55caba216c2aa0231",
        "title": "Tutorials",
        "description": "",
        "text": "Here are a set of tutorials for migrating between regions, exporting data, and more.",
        "section": "Tutorials",
        "subsections": "",
        "image": "",
        "url": "/tutorials.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "2ddb9cd69efd6dc26336a1c22abd79ae",
        "title": "Dedicated",
        "description": "",
        "text": "Platform.sh Dedicated is a robust, redundant layer on top of Platform.sh Professional. This section contains all resources concerning the Dedicated product previously found at `ent.docs.platform.sh`.",
        "section": "Dedicated",
        "subsections": "",
        "image": "",
        "url": "/dedicated.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "f20e55772096606cc8f9fe290a1d7d90",
        "title": "Activity reference",
        "description": "",
        "text": " This hook allows you to capture any push events on platform and POST a JSON message describing the activity to the url of your choice. You can use this to further automate your Platform.sh workflow. Activity schema id A unique opaque value to identify the activity. project The Project ID for which the activity was triggered. Use this value if you want to have multiple projects POST to the same URL. type The type property specifies the event that happened. Its value is one of: project.modify.title: The human-friendly title of the project has been changed. project.create: A project has been created. Although it will appear in the activity feed exactly once, it will not be sent via a webhook as it will always happen before a webhook can be configured. project.domain.create: A new domain has been added to the project. project.domain.delete: A domain associated with the project has been removed. project.domain.update: A domain associated with the project has been updated, including modifying it’s SSL certificate. environment.access.add: A new user has been given access to the environment. environment.access.remove: A user has been removed from the environment. environment.backup: A user triggered a backup . environment.restore: A user restored a backup . environment.backup.delete: A user deleted a backup environment.push: A user has pushed code to a branch, either existing or new. environment.branch: A new branch has been created via the management console. (A branch created via a push will show up only as an environment.push.) environment.activate: A branch has been “activated”, and an environment created for it. environment.initialize: The master branch of the project has just been initialized with its first commit. environment.deactivate: A branch has been “deactivated”. The code is still there, but the environment was destroyed. environment.synchronize: An environment has had its data and/or code re-copied from its parent environment. environment.merge: A branch was merged through the management console or Platform.sh API. A basic Git merge will not trigger this event. environment.redeploy: An environment was redeployed. environment.delete: A branch was deleted. environment.route.create: A new route has been created through the management console. This will not fire for route edits made to the routes.yaml file directly. environment.route.delete: A route has been deleted through the management console. This will not fire for route edits made to the routes.yaml file directly. environment.route.update: A route has been modified through the management console. This will not fire for route edits made to the routes.yaml file directly. environment.variable.create: A new variable has been created. environment.variable.delete: A variable has been deleted. environment.variable.update: A variable has been modified. environment.update.http_access: HTTP access rules for an environment have been modified. environment.update.smtp: Sending of emails has been enabled/disabled for an environment. environment.update.restrict_robots: The block-all-robots feature has been enabled/disabled. environment.subscription.update: The master environment has been resized because the subscription has changed. There are no content changes. environment.cron: A cron task just completed. environment.source-operation: A source operation triggered and has completed. integration.bitbucket.fetch: Changes in BitBucket repository have been pulled. integration.bitbucket.register_hooks: Integration hook have been registered on BitBucket. integration.bitbucket_server.fetch: Changes in BitBucket repository have been pulled. integration.bitbucket_server.register_hooks: Integration hook have been registered on BitBucket. integration.github.fetch: Changes in GitHub repository have been pulled. integration.gitlab.fetch: Changes in GitLab repository have been pulled. integration.health.email: Health event sent by email. integration.health.pagerduty: Health event sent to pagerduty. integration.health.slack: Health event sent to slack. integration.webhook: Webhook triggered. integration.hipchat: Event sent to hipchat. integration.script: An activity script has run. environments An array listing the environments that were involved in the activity. This is usually single-value. result Whether the activity was completed successfully or not. It should be success if all went as planned. created_at, started_at, completed_at These values are all timestamps in UTC. If you need only a point in time when the action happened, use completed_at. You can also combine it with started_at to see how long the activity took. log A text description of the action that happened. This is a human-friendly string that may be displayed to a user but should not be parsed for data as its structure is not guaranteed. payload.environment This block contains information about the environment itself, after the action has taken place. The most notable properties of this key are name (the name of the branch) machine_name (the name of the environment) head_commit (the Git commit ID that triggered the event) payload.user The Platform.sh user that triggered the activity. deployment This large block details all information about all services in the environment. That includes the resulting configuration objects derived from routes.yaml , services.yaml , and .platform.app.yaml . Most notably, the deployment.routes object’s keys are all of the URLs made available by the environment. Note that some will be redirects. To find those that are live URLs filter to those objects whose type property is upstream. Example activity The following is an example of a webhook message. Specifically, this one was created by a “push” event. {  id :  774-this-is-an-example-valuexzs4no ,  _links : {  self : {  href :  https://eu.platform.sh/api/projects/sx-this-is-an-example-value-hu/activities/774-this-is-an-example-valuexzs4no ,  meta : {  get : {  responses : {  default : {  schema : {  properties : {  created_at : {  type :  string ,  format :  date-time  },  updated_at : {  type :  string ,  format :  date-time  },  type : {  type :  string  },  parameters : {  properties : { },  required : [ ] },  project : {  type :  string  },  environments : {  items : {  type :  string  },  type :  array  },  state : {  type :  string  },  result : {  type :  string  },  started_at : {  type :  string ,  format :  date-time  },  completed_at : {  type :  string ,  format :  date-time  },  completion_percent : {  type :  integer  },  log : {  type :  string  },  payload : {  properties : { },  required : [ ] } },  required : [  created_at ,  updated_at ,  type ,  parameters ,  project ,  environments ,  state ,  result ,  started_at ,  completed_at ,  completion_percent ,  log ,  payload  ] } } },  parameters : [ ] } } } },  created_at :  2017-12-07T10:45:30.870660\u0026#43;00:00 ,  updated_at :  2017-12-07T10:47:39.369761\u0026#43;00:00 ,  type :  environment.push ,  parameters : {  environment :  master ,  old_commit :  34be31cbabcc0f65d7fd8ec29769947396d0cabd ,  new_commit :  8d2e6003d50136c750fb8b65ec506ee3aa4a5b15 ,  user :  384491da-031e-4c23-b264-9f96040a6e36  },  project :  sx-this-is-an-example-value-hu ,  environments : [  master  ],  state :  complete ,  result :  success ,  started_at :  2017-12-07T10:45:30.996898\u0026#43;00:00 ,  completed_at :  2017-12-07T10:47:39.369741\u0026#43;00:00 ,  completion_percent : 100,  log :  Found 1 new application \u0026#39;myrubyapp\u0026#39; (runtime type: ruby:2.4-rc, tree: Generating runtime Executing build Fetching gem metadata from Resolving Installing concurrent-ruby Installing i18n Installing minitest Installing thread_safe ...more Installing unicorn Using bundler Your bundle is Use `bundle show [gemname]` to see where a bundled gem is Executing pre-flight Compressing Beaming package to its final Reusing existing environment Environment myrubyapp (type: ruby:2.4-rc, size: S, disk: postgresql (type: postgresql:9.3, size: S, disk: mongodb (type: mongodb:3.0, size: S, disk: redis (type: redis:3.2-rc, size: influxdb (type: influxdb:1.2, size: S, disk: elasticsearch (type: elasticsearch, size: S, disk: rabbitmq (type: rabbitmq:3.5, size: S, disk: mysql (type: mysql:10.0, size: S, disk: solr (type: solr:4.10, size: S, disk: Environment http://master-7rqtwti-sx-this-is-an-example-value-hu.eu.platform.sh/ redirects to https://master-7rqtwti-sx-this-is-an-example-value-hu.eu.platform.sh/ is served by application  payload : {  environment : {  status :  active ,  head_commit :  8d2e6003d50136c750fb8b65ec506ee3aa4a5b15 ,  machine_name :  master-7rqtwti ,  name :  master ,  parent : null,  title :  Master ,  created_at :  2017-11-22T14:43:17.154870\u0026#43;00:00 ,  updated_at :  2017-11-22T14:43:17.155103\u0026#43;00:00 ,  clone_parent_on_create : true,  project :  sx-this-is-an-example-value-hu ,  is_dirty : false,  restrict_robots : true,  has_code : true,  enable_smtp : true,  id :  master ,  deployment_target :  local ,  http_access : {  is_enabled : true,  addresses : [ ],  basic_auth : { } },  is_main : true },  commits : [ {  sha :  8d2e6003d50136c750fb8b65ec506ee3aa4a5b15 ,  message :  deploy with release candidates ,  parents : [  34be31cbabcc0f65d7fd8ec29769947396d0cabd  ],  author : {  email :  ori@example.com ,  name :  Ori Pekelman  } } ],  commits_count : 1,  user : {  created_at :  2017-12-07T10:45:13.491553\u0026#43;00:00 ,  display_name :  Ori Pekelman ,  id :  384491da-031e-4c23-b264-9f96040a6e36 ,  updated_at : null },  deployment : {  id :  current ,  services : {  mongodb : {  type :  mongodb:3.0 ,  size :  AUTO ,  disk : 500,  access : { },  configuration : { },  relationships : { } },  redis : {  type :  redis:3.2-rc ,  size :  AUTO ,  disk : null,  access : { },  configuration : { },  relationships : { } },  solr : {  type :  solr:4.10 ,  size :  AUTO ,  disk : 256,  access : { },  configuration : { },  relationships : { } } },  routes : {  https://master-7rqtwti-sx-this-is-an-example-value-hu.eu.platform.sh/ : {  type :  upstream ,  redirects : {  expires :  -1s ,  paths : { } },  original_url :  https://{default}/ ,  cache : {  enabled : true,  default_ttl : 0,  cookies : [  *  ],  headers : [  Accept ,  Accept-Language  ] },  ssi : {  enabled : false },  upstream :  myrubyapp  },  http://master-7rqtwti-sx-this-is-an-example-value-hu.eu.platform.sh/ : {  type :  redirect ,  redirects : {  expires :  -1s ,  paths : { } },  original_url :  http://{default}/ ,  to :  https://master-7rqtwti-sx-this-is-an-example-value-hu.eu.platform.sh/  } },  webapps : {  myrubyapp : {  size :  AUTO ,  disk : 2048,  access : {  ssh :  contributor  },  relationships : {  mongodb :  mongodb:mongodb ,  redis :  redis:redis ,  solr :  solr:solr  },  mounts : {  /public :  shared:files/files  },  timezone : null,  variables : { },  name :  myrubyapp ,  type :  ruby:2.4-rc ,  runtime : { },  preflight : {  enabled : true,  ignored_rules : [ ] },  web : {  locations : {  / : {  root :  public ,  expires :  1h ,  passthru : true,  scripts : true,  allow : true,  headers : { },  rules : { } } },  commands : {  start :  unicorn -l $SOCKET -E production config.ru ,  stop : null },  upstream : {  socket_family :  unix ,  protocol : null },  move_to_root : false },  hooks : {  build :  ruby -e \u0026#39;bundle  deploy : null } } },  workers : { } } } }",
        "section": "Activity scripts",
        "subsections": " Activity schema  id project type environments result created_at, started_at, completed_at log payload.environment payload.user deployment   Example activity  ",
        "image": "",
        "url": "/integrations/activity/reference.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "c838abdab87aa226228abffc45dd608b",
        "title": "External Integrations",
        "description": "",
        "text": " Platform.sh can be integrated with external services. Platform.sh supports native integrations with multiple services, first and foremost Git hosting services such as GitHub, GitLab, or Bitbucket. You can continue to use those tools for your development workflow, and have Platform.sh environments created automatically for your pull requests and branches. You can also add our native integrations with performance monitoring tools such as Blackfire, New Relic, or Tideways, as well as setting up health notifications. Or create your own integration using our webhooks. /integrations/overview.html#validating-integrations Be aware that only a project administrator (someone with admin level access to the project) can add or remove integrations. See User administration for more details. Listing active integrations With the CLI, you can list all your active integrations: platform integrations Info: If you have created your account using the Bitbucket or GitHub oAuth Login, then in order to use the Platform.sh CLI you will need to set up a password by visiting https://accounts.platform.sh/user/password. Validating integrations Once your integration has been configured, you can validate that it is functioning properly with the command: $ platform integration:validate Enter a number to choose an integration: [0] 5aut2djgt6kdd (health.slack) [1] a6535j9qp4sl8 (github) \u0026gt; 1 Validating the integration a6535j9qp4sl8 (type: github)... The integration is valid. Debugging integrations When integrations run, they trigger “activities.” Activities are actions that happen on Platform.sh, and they get logged. Those logs are available via the CLI. In most cases they are not necessary but may be useful for debugging an integration if it is misbehaving for some reason. There are a handful of commands available, all under the integrations section. List all activities The commands platform integration:activity:list or its alias platform integration:activities will list all activities on a given project and integration. For example, for the project for this site, the command platform integration:activity:list outputs: $ platform integration:activities Enter a number to choose an integration: [0] dxr45hfldrkoe (webhook) [1] n2ukd4p7qofg4 (health.email) [2] c4opi5tjv3yfd (github) \u0026gt; 2 Activities on the project Platform.sh | Docs (6b2eocegfkwwg), integration c4opi5tjv3yfd (github): \u0026#43;---------------\u0026#43;---------------------------\u0026#43;-------------------------------------------------------------\u0026#43;----------\u0026#43;---------\u0026#43; | ID | Created | Description | State | Result | \u0026#43;---------------\u0026#43;---------------------------\u0026#43;-------------------------------------------------------------\u0026#43;----------\u0026#43;---------\u0026#43; | 6456zmdtoykxa | 2020-04-14T16:38:09-05:00 | Fetching from https://github.com/platformsh/platformsh-docs | complete | success | | wcwp34yjvydgk | 2020-04-14T16:35:22-05:00 | Fetching from https://github.com/platformsh/platformsh-docs | complete | success | | w2bp3oa5xbfoe | 2020-04-14T16:33:13-05:00 | Fetching from https://github.com/platformsh/platformsh-docs | complete | success | | uqqvdyxmcdmsa | 2020-04-14T16:31:45-05:00 | Fetching from https://github.com/platformsh/platformsh-docs | complete | success | | 7x3wefhh4fwqc | 2020-04-14T16:30:36-05:00 | Fetching from https://github.com/platformsh/platformsh-docs | complete | success | | a46aah3ga65gc | 2020-04-14T16:29:46-05:00 | Fetching from https://github.com/platformsh/platformsh-docs | complete | success | | r7erid2jlixgi | 2020-04-14T16:24:50-05:00 | Fetching from https://github.com/platformsh/platformsh-docs | complete | success | | ieufk3vvde5oc | 2020-04-14T16:24:49-05:00 | Fetching from https://github.com/platformsh/platformsh-docs | complete | success | | bc7ghg36ty4ea | 2020-04-14T15:30:17-05:00 | Fetching from https://github.com/platformsh/platformsh-docs | complete | success | | 4qojtv7a6rk2w | 2020-04-14T15:27:26-05:00 | Fetching from https://github.com/platformsh/platformsh-docs | complete | success | \u0026#43;---------------\u0026#43;---------------------------\u0026#43;-------------------------------------------------------------\u0026#43;----------\u0026#43;---------\u0026#43; You may also specify which integration to display in the command line directly: platform integration:activities c4opi5tjv3yfd. The ID is an internal identifier for the activity event. The Description field is an arbitrary string of text produced by the integration code. The State and Result fields indicate if the activity completed successfully, failed for some reason, or is currently in progress. See the --help output of the command for more options. Showing detailed information on an activity You can call up more detailed information on a specific activity by its ID, using the platform integration:activity:log command. It requires both the integration ID and an activity ID from the list above. It also works best with the -t option to include timestamps. $ platform integration:activity:log c4opi5tjv3yfd 6456zmdtoykxa -t Integration ID: ceopz5tgj3yfc Activity ID: 6456zmdtoykxa Type: integration.github.fetch Description: Fetching from https://github.com/platformsh/platformsh-docs Created: 2020-04-15T08:44:07-05:00 State: complete Log: [2020-04-15T13:44:17-05:00] Waiting for other activities to complete [2020-04-15T13:46:07-05:00] Fetching from GitHub repository platformsh/platformsh-docs [2020-04-15T13:46:09-05:00] No changes since last fetch [2020-04-15T13:46:09-05:00] [2020-04-15T13:46:09-05:00] Synchronizing branches [2020-04-15T13:46:09-05:00] [2020-04-15T13:46:09-05:00] Synchronizing pull requests [2020-04-15T13:46:59-05:00] [2020-04-15T13:46:59-05:00] W: No changes found, scheduling a retry.. That will show the full output of the activity, including timestamps. That can be especially helpful if trying to determine why an integration is not behaving as expected. See the --help output of the command for more options. If you omit the activity ID (the second random-seeming string), the command will default to the most recent activity recorded.",
        "section": "Integrations",
        "subsections": " Listing active integrations Validating integrations Debugging integrations  List all activities Showing detailed information on an activity    ",
        "image": "",
        "url": "/integrations/overview.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "f7974f8cb6da8fa46946d9a0ac2d552d",
        "title": "Activity scripts",
        "description": "",
        "text": " Platform.sh supports custom scripts that can fire in response to any activity. These scripts allow you to take arbitrary actions in response to actions in your project, such as when it deploys, when a new branch is created, etc. A legacy integration is also available for HipChat . Activity scripts Activity scripts are written in a scope-limited version of Javascript ES5 . That means they do not support newer ES6 and later features such as classes, nor do they support installing additional packages. A series of utility functions you can reuse are also available . Installing Activity scripts are configured as integrations. That means they are at the project level, not at the level of an individual environment. While you can store the scripts in your Git repository for easy access, they will have no effect there. To install a new activity script, use the Platform.sh CLI . platform integration:add --type script --file ./my_script.js That will install and enable the my_script.js file as an activity script on the current project. You can get its ID by listing the integrations on the current project: platform integrations +---------------+--------------+--------------+ | ID | Type | Summary | +---------------+--------------+--------------+ | nadbowmhd67do | script | ... | | rcqf6b69jdcx6 | health.email | From: | | | | To: #admins | +---------------+--------------+--------------+ The just-installed script’s ID in this example is nadbowmhd67do. Do not run the integration:add command a second time, or it will install a second integration that happens to have the same code. Updating To update an existing activity script, use the integration:update command. You will need the ID Of the integration to update (as above). platform integration:update --file ./my_script.js nadbowmhd67do That will update the integration in place, permanently overwriting the previous version. Removing To disable an activity script, use the integration:delete command: platform integration:delete nadbowmhd67do Debugging Activity logs are available through their own CLI command, platform integration:activities. Every time your activity script runs it will generate a new log entry, including the output from the script. Any output produced by console.log will be available in the activity log, and that is the recommended way to debug scripts. See the activity log documentation for further details. To get more readable output of a variable you’re trying to debug, you can make JSON.stringify use human-friendly formatting. console.log(JSON.stringify(project, null, 2)); Configuring scripts There are many types of activity to which a script could respond. By default, it will activate only after a successful git push operation. That trigger is configurable via command line switches when adding or updating a script. For example, to have a script trigger any time an environment is activated or deactivated, you would run: platform integration:update --events='environment.activate, environment.deactivate' nadbowmhd67do A complete list of possible events is available in the webhook documentation . Scripts can also trigger only when an action reaches a given state, such as “pending”, “in_progress”, or “complete”. The default is only when they reach “complete”. To have a script execute when a synchronize action first starts, for example, you would run: platform integration:update --events=environment.synchronize --states=in_progress nadbowmhd67do It is also possible to restrict scripts to certain environments by name. Most commonly that is used to have them execute only for the master environment, or for all environments except master. The following example executes only for backup actions on the master environment: platform integration:update --events=environment.backup --environments=master nadbowmhd67do There is also an --exclude-environments switch to blacklist environments by name rather than whitelist. As a general rule, it is better to have an activity script only execute on the specific events and branches you’re interested in rather than firing on all activities and then filtering out undesired use cases in the script itself. Available APIs Activity scripts have a series of APIs available to them to facilitate building out custom functionality. underscore.js Underscore.js 1.9.2 is available out-of-the-box to make writing Activity scripts more pleasant. See Underscore’s documentation for available functions and utilities. activity Every activity script has a global variable activity that contains detailed information about the activity, including embedded, JSON-ified versions of the routes configuration and relevant .platform.app.yaml files. The activity variable is the same as the webhook payload . See the documentation there for details and a complete example. Several of the utility functions below work by pulling out common portions of the activity object. Most notably, scripts can be configured via Project-level variables that can be accessed from the activity object. project The project global variable includes information about the project subscription itself. That includes its ID and name, how many users are associated with the project, it’s SSH public key, and various other values. An example of this object is below: {  attributes : {},  created_at :  2020-04-15T19:50:09.514267+00:00 ,  default_domain : null,  description :   ,  id :  kpyhl5f8nuzef ,  owner :  ... ,  region :  eu-3.platform.sh ,  repository : {  client_ssh_key :  ssh-rsa ... ,  url :  kqyhl5f5nuzky@git.eu-3.platform.sh:kqyhl5f5nuzky.git  },  status : {  code :  provisioned ,  message :  ok  },  subscription : {  environments : 3,  included_users : 1,  license_uri :  ... ,  plan :  development ,  restricted : false,  storage : 5120,  subscription_management_uri :  ... ,  suspended : false,  user_licenses : 1 },  timezone :  Europe/Dublin ,  title :  Activity script examples ,  updated_at :  2020-04-21T17:15:35.526498+00:00  } Storage API Activity scripts have access to a limited key/value storage API to persist values from one execution to another. The API is similar to the Javscript LocalStorage API. // Access the storage API. It is not pre-required. var storage = require( storage ); // Retrieve a stored value. If the value is not set it will return null. var counter = storage.get('counter') || 0; if (counter) { // Generate debug output. console.log( Counter is:   + counter); } // Write a value into the storage. Only string-safe values are supported. // To save an object or array, run JSON.stringify() on it first. storage.set('counter', counter + 1); // Remove a value completely. storage.remove('counter'); // Remove all values in storage, unconditionally. storage.clear(); Fetch API Activity scripts support a modified version of the browser “Fetch API” for issuing HTTP requests. Unlike the typical browser version, however, they only support synchronous requests. That means the return value of fetch() is a response object, not a Promise for one. The API is otherwise essentially the same as that documented by Mozilla . For instance, this example sends a GET request every time it executes: var resp = fetch( http://example.com/site-deployed.php ); // resp.ok is true if the response was a 2xx, false otherwise. if (!resp.ok) { console.log( Well that didn't work. ); } While this example sends a POST request with a JSON string as the body: var body = JSON.stringify({  some :  value , }); var resp = fetch( http://example.com/ , { method:  POST , headers: { 'Content-Type': 'application/json', }, body: body, } ) if (!resp.ok) { console.log( Couldn't POST. ); } See the Mozilla Dev Network link above for more fetch() options. Cryptographic API A minimalist cryptographic API is also available to activity scripts. Its main use is for signing requests to 3rd party APIs. The crypto.createHmac() function allows you to create a secure HMAC hash and digest. var h = crypto.createHmac( sha256 ,  foo ); h.update( bar ); h.digest( hex ) The available hashing functions are 'sha256', 'sha1' and 'md5' as hashing functions. The available digest formats are 'base64', 'hex' or '' (empty). An empty digest will yield a byte string. For example, if you wanted to call an AWS API, you would calculate the signature like so: function HMAC(key, value) { var h = crypto.createHmac( sha256 , key); h.update(value); return h.digest(); } var kSecret =  wJalrXUtnFEMI/K7MDENG+bPxRfiCYEXAMPLEKEY ; HMAC(HMAC(HMAC(HMAC( AWS4  + kSecret, 20150830 ), us-east-1 ), iam ), aws4_request ); Example taken from the AWS documentation for signing API requests .",
        "section": "Integrations",
        "subsections": " Activity scripts  Installing Updating Removing   Debugging Configuring scripts Available APIs  underscore.js activity project Storage API Fetch API Cryptographic API    ",
        "image": "",
        "url": "/integrations/activity.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "43bef003df36edbaa72a852b58d0f5ba",
        "title": "Utility routines",
        "description": "",
        "text": " The following utility routines can help simplify common tasks in your activity scripts. They are free to copy, modify, bend, fold, spindle, and mutilate as needed for your own scripts. They also demonstrate some common patterns for working with the activity and project data structures in ES5 code. General utilities /** * Formats a string, injecting values in for placeholders. * * @param {string} format * A format string with placeholders in the form {0}, {1}, etc. * @param {string} args * A variable number of strings to replace by position. * @return {string} * The formatted string. */ function formatString (format) { var args = Array.prototype.slice.call(arguments, 1); return function(match, number) { return typeof args[number] != \u0026#39;undefined\u0026#39; ? args[number] : match ; }); } /** * Returns a key/value object containing all variables relevant for the activity. * * That includes project level variables, plus any variables visible for * the relevant environment for the activity, if any. * * Note that JSON-encoded values will show up as a string, and need to be * decoded with JSON.parse(). */ function variables() { var vars = {}; activity.payload.deployment.variables.forEach(function(variable) { vars[variable.name] = variable.value; }); return vars; } Route access /** * Returns just those routes that point to a valid upstream. * * This method is similar to routes(), but filters out redirect routes that are rarely * useful for app configuration. If desired it can also filter to just those routes * whose upstream is a given application name. To retrieve routes that point to the * current application where the code is being run, use: * * routes = getUpstreamRoutes(applicationName); * * @param {string|null} appName * The name of the upstream app on which to filter, if any. * @return {object} * An object map of route definitions. */ function getUpstreamRoutes(appName) { var upstreams = {}; Object.keys(activity.payload.deployment.routes).forEach(function (url) { var route = activity.payload.deployment.routes[url]; if (route.type ===  upstream ) { if (!appName || appName === route.upstream.split(\u0026#39;:\u0026#39;)[0]) { route.url = url; upstreams[url] = route; } } }); return upstreams; } /** * Returns the primary route. * * The primary route is the one marked primary in `routes.yaml`, or else * the first non-redirect route in that file if none are marked. * * @return {object} * The route definition. The generated URL of the route is added as a  url  key. */ function getPrimaryRoute() { var primary = {}; Object.keys(activity.payload.deployment.routes).forEach(function (url) { var route = activity.payload.deployment.routes[url]; if (route.primary) { route.url = url; primary = route; } }); return primary; } /** * Returns a single route definition. * * Note: If no route ID was specified in routes.yaml then it will not be possible * to look up a route by ID. * * @param {string} id * The ID of the route to load. * @return {object} * The route definition. The generated URL of the route is added as a  url  key. * @throws {Error} * If there is no route by that ID, an exception is thrown. */ function getRoute(id) { var found = null; Object.keys(activity.payload.deployment.routes).forEach(function (url) { var route = activity.payload.deployment.routes[url]; if (route.id === id) { route.url = url; found = route; } }); if (found) { return found; } throw new Error( No such route id found:   \u0026#43; id); }",
        "section": "Activity scripts",
        "subsections": " General utilities Route access  ",
        "image": "",
        "url": "/integrations/activity/utility.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "9dd6a1f59e9bdeed1313f1ab53b89be9",
        "title": "Source Integrations",
        "description": "",
        "text": "Platform.sh allows you to maintain your code base in a third party repository and link it to your Platform.sh project.",
        "section": "Integrations",
        "subsections": "",
        "image": "",
        "url": "/integrations/source.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "f28b0e09debf0d2e668ca9f7a78c3290",
        "title": "Profiling",
        "description": "",
        "text": "Platform.sh supports a number of profiling services for optimizing your code.",
        "section": "Integrations",
        "subsections": "",
        "image": "",
        "url": "/integrations/profiling.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "530ceed9de239bab0b518bd8a5fdd0bd",
        "title": "Health notifications",
        "description": "",
        "text": " Platform.sh can notify you when various events happen on your project, in any environment. At this time the only notification provided is a low disk space warning, but others may be added in the future. Note: Remember that you must have admin access to a project in order to add or modify an integration. See User administration roles for more details. Default low-disk email notifications When you create a new project, Platform.sh creates a default low-disk email notification for all Project Admins . Note: All projects created prior to 6 April 2020 that did not have any health notifications enabled had an email notification added for admin users. Available notifications Low-disk warning Platform.sh monitors disk space usage on all applications and services in your cluster. If and when available disk space drops below 20%, a warning notification is generated. If and when available disk space drops below 10%, a critical notification is generated. If and when available disk space goes back above 20% after previously having been lower, an all-clear notification is generated. Notifications are generated every 5 minutes, so there may be a brief delay between when the threshold is crossed and when the notification is triggered. Configuring notifications Health notifications can be set up via the Platform.sh CLI , through a number of different channels. Email notifications A notification can trigger an email to be sent, from an address of your choosing to one or more addresses of your choosing. You can view an email notification by running platform integration:get. platform integration:get \u0026#43;--------------\u0026#43;---------------\u0026#43; | Property | Value | \u0026#43;--------------\u0026#43;---------------\u0026#43; | id | abcdefghijklm | | type | health.email | | role | | | from_address | | | recipients | - \u0026#39;#admins\u0026#39; | \u0026#43;--------------\u0026#43;---------------\u0026#43; To edit the recipients that receive the default email notification, use the integration:update command. platform integration:update abcdefghijklm --recipients you@example.com The recipients field may be any valid email address, or one of the following special values. #admins maps to all project admins and up. #viewers maps to everyone with access to the project. To add a new email notification, register a health.email integration as follows: platform integration:add --type health.email --from-address you@example.com --recipients them@example.com --recipients others@example.com The from-address is whatever address you want the email to appear to be from. You must specify one or more recipients, each as its own switch. It is completely fine to use the same email address for both from-address and recipients. Slack notifications A notification can trigger a message to be sent to a Slack bot. First, create a new custom “ bot user ” for your Slack group and configure the channels you wish it to live in. Note the API token is the “Bot User OAuth Access Token” provided by Slack. Then register that Slack bot with Platform.sh using a health.slack integration: platform integration:add --type health.slack --token YOUR_API_TOKEN --channel \u0026#39;#channelname\u0026#39; That will trigger the corresponding bot to post a notification to the #channelname channel in your Slack group. PagerDuty notifications A notification can trigger a message to be sent via PagerDuty, if you are using that service. First, create a new PagerDuty “ integration ” that uses the Events API v2. Copy the “Integration Key” as known as the “routing key” for the integration. Now register a health.pagerduty integration as follows: platform integration:add --type health.pagerduty --routing-key YOUR_ROUTING_KEY Any notification will now trigger an alert in PagerDuty. Webhooks notifications A notification can trigger a message to be sent to a web endpoint. To do so, register a health.webhook integration as follows: platform integration:add --type health.webhook --url=A-URL-THAT-CAN-RECEIVE-THE-POSTED-JSON Any notification will now be posted to the health.webhook URL. In order to let you verify that requests are coming from the integration, you can use the optional shared-key parameter which will add a X-JWS-Signature request header containing the JSON Web Token Signature in JWS Compact Serialization with Unencoded Detached Payload ( RFC7797 ). platform integration:add --type health.webhook --url=A-URL-THAT-CAN-RECEIVE-THE-POSTED-JSON --shared-key JWS-SYMMETRIC-KEY The signature is calculated using the given shared-key and the fixed header: { alg : HS256 , b64 :false, crit :[ b64 ]} A simplified example payload with the corresponding signature might look like the following snippet: POST /health/notifications HTTP/1.0 Host: www.example.com Content-Length: 1495 Content-Type: application/json X-JWS-Signature: eyJhbGciOiJIUzI1NiIsImI2NCI6ZmFsc2UsImNyaXQiOlsiYjY0Il19..fYW9qrjShmEArV17Z1kH6yudoXzpBE3PzJXq_OqrIfM {...request body...} Signature verification is a simple 2 step process: # 1. Compute JWS Compact Serialization with Unencoded Detached Payload from jwcrypto import jws, jwk rfc7797_u_header = \u0026#39;{ alg : HS256 , b64 :false, crit :[ b64 ]}\u0026#39; json_web_key = jwk.JWK(kty= oct , k= JWS-SYMMETRIC-KEY ) sig = jws.JWS(request.body()) sig.add_signature(json_web_key, protected=rfc7797_u_header) sig.detach_payload() # 2. Verify the signature assert sig.serialize(compact=True) == request.headers[ X-JWS-Signature ] Please refer to the JOSE Cookbook for examples about protecting content using JavaScript Object Signing and Encryption (JOSE). Validate the integration You can then verify that your integration is functioning properly using the CLI command $ platform integration:validate",
        "section": "Integrations",
        "subsections": " Default low-disk email notifications Available notifications  Low-disk warning   Configuring notifications  Email notifications Slack notifications PagerDuty notifications Webhooks notifications   Validate the integration  ",
        "image": "",
        "url": "/integrations/notifications.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "1087c8c69a50fe7b603a99393e003068",
        "title": "Introduction",
        "description": "",
        "text": "Try out Platform.sh by either including few configuration files to your existing codebase, or by deploying one of over fifty maintained template projects.",
        "section": "Introduction",
        "subsections": "",
        "image": "",
        "url": "/gettingstarted/gettingstarted.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "d98929356d78fe97ac1d17b712b87937",
        "title": "Alernative Node.js install",
        "description": "",
        "text": " How to use NVM to run different versions of Node.js Node Version Manager or NVM is a tool for managing multiple versions of Node.js in one installation. You can use NVM with any of our container types that have node installed to change or update the version. This may be useful, for example, where a container has a Long Term Release (LTS) version available, but you would like to use the latest. Installing NVM is done in the build hook of your .platform.app.yaml, which some additional calls to ensure that environment variables are set correctly. hooks:build:| unset NPM_CONFIG_PREFIXcurl-o- unset in a .environment file in the root of your project: # This is necessary for nvm to work. unset NPM_CONFIG_PREFIX # Disable npm update notifier; being a read only system it will probably annoy you. export NO_UPDATE_NOTIFIER=1 # This loads nvm for general usage. export NVM_DIR= $PLATFORM_APP_DIR/.nvm  [ -s  $NVM_DIR/nvm.sh  ] \u0026amp;\u0026amp;  $NVM_DIR/nvm.sh ",
        "section": "Node.js",
        "subsections": " How to use NVM to run different versions of Node.js  ",
        "image": "",
        "url": "/languages/nodejs/nvm.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "4c7dc60e710fc82c92d19784740f7519",
        "title": "API tokens",
        "description": "",
        "text": " Obtaining a token The Platform.sh CLI can also be used from CI services or other automation tools, and supports an API Token authentication option for that purpose. An API token can be created through the management console. Go to the “User” page from your account drop-down, then select the “Account Settings” tab, then “API Tokens”. Click the “Create an API Token” link. Enter a name to easily identify your token in the future, in case of multiple tokens (“CLI automated” is one example). Once done, the newly created token will be displayed at the top of the page, and can be copied to the clipboard using the Copy button. After this, you will not be able to view the API token again. Now set that token in an environment variable named PLATFORMSH_CLI_TOKEN on the system where the CLI will run. Consult the documentation for your CI system to see how to do that. Note: If running CLI commands from any automated system, including a Platform.sh cron task, we urge you to use the --no-wait flag on any commands that may take more than a second or two to avoid blocking the process. Machine users For security reasons we recommend creating a dedicated machine user to run automation tasks such as taking backups, renewing SSL certificates or triggering source operations. We also strongly recommend creating a unique machine user for each project to be automated. Like human users, every machine user account needs its own unique email address. The machine user can be given a very restrictive set of permissions limited to just its needed tasks. Backups, for instance, require Admin access but no SSH key, while checking out code from a CI server to run tests on it would require an SSH key but only Reader access. It will also show up in logs and activity streams as a separate entry from human users. Consult the Users documentation for more information about the differences between access levels. Install the CLI on a Platform.sh environment A common use case for an API token is to allow the Platform.sh CLI to be run on an app container, often via a cron hook. An API token is necessary for authentication, but the CLI will be able to auto-detect the current project and environment. First, create a machine user (see above) that you invite to your project. Then, log in as that machine user to obtain an API token. Set this token as the top-level environment variable env:PLATFORMSH_CLI_TOKEN either through the management console or via the CLI, like so: platform variable:create -e master --level environment --name env:PLATFORMSH_CLI_TOKEN --sensitive true --value \u0026#39;your API token\u0026#39; Note: It is important to include the env: so as to expose $PLATFORMSH_CLI_TOKEN on its own as a top level Unix environment variable, rather than as a part of $PLATFORM_VARIABLES like normal environment variables. Second, add a build hook to your .platform.app.yaml file to download the CLI as part of the build process. hooks:build:| curl -sS https://platform.sh/cli/installer | phpThis will download the CLI to a known directory, .platformsh/bin, which will be added to the PATH at runtime (via the .environment file). Because the API token is available, the CLI will now be able to run authenticated commands, acting as the user who created the token. You can now call the CLI from within the shell on the app container, or via a cron hook. Note that if you want a cron to run only on the production environment you will need to wrap it in an if-check on the $PLATFORM_BRANCH variable, like so: crons:backup:spec:\u0026#39;0 5 * * *\u0026#39;cmd:| if [  $PLATFORM_BRANCH  = master ]; thenplatformbackup:create--yes--no-waitfi Note: Seriously, please use --no-wait for all CLI commands placed in a cron hook. Failure to do so may result in long deploy times and site downtime.",
        "section": "CLI (Command line interface)",
        "subsections": " Obtaining a token Machine users Install the CLI on a Platform.sh environment  ",
        "image": "",
        "url": "/development/cli/api-tokens.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "6a306d1f4c635744235c86e403500ba6",
        "title": "Authenticated Composer repositories",
        "description": "",
        "text": " Some PHP projects may need to leverage a private, third party Composer repository in addition to the public Packagist.org repository. Often, such third party repositories require authentication in order to download packages, and not everyone is comfortable putting those credentials into their Git repository source code (for obvious reasons). To handle that situation, you can define a env:COMPOSER_AUTH project variable which allows you to set up authentication as an environment variable. The contents of the variable should be a JSON formatted object containing an http-basic object (see composer-auth specifications ). The advantage is that you can control who in your team has access to those variables. Specify a third party repository in composer.json For this example, consider that there are several packages we want to install from a private repository hosted at my-private-repos.example.com. List that repository in your composer.json file. {  repositories : [ {  type :  composer ,  url :  https://my-private-repos.example.com  } ] } Set the env:COMPOSER_AUTH project variable Set the Composer authentication by adding a project level variable called env:COMPOSER_AUTH as JSON and available only during build time. That can be done through the management console or via the command line, like so: platform variable:create --level project --name env:COMPOSER_AUTH --json true --visible-runtime false --sensitive true --visible-build true --value \u0026#39;{ http-basic : { my-private-repos.example.com : { username :  your-username ,  password :  your-password }}}\u0026#39; The env: prefix will make that variable appear as its own Unix environment variable available by Composer during the build process. The optional --no-visible-runtime flag means the variable will only be defined during the build hook, which offers slightly better security. Note: The authentication credentials may be cached in your project’s build container, so please make sure you clear the Composer cache upon changing any authentication credentials. You can use the platform project:clear-build-cache command. Build your application with Composer You simply need to enable the default Composer build mode in your .platform.app.yaml: build:flavor: composer In that case, Composer will be able to authenticate and download dependencies from your authenticated repository. Private repository hosting Typically, a private dependency will be hosted in a private Git repository. While Platform.sh supports private repositories for the site itself, that doesn’t help for pulling in third party dependencies from private repositories unless they have the same SSH keys associated with them. Fortunately, most private Composer tools (including Satis, Toran Proxy, and Private Packagist ) mirror tagged releases of dependencies and serve them directly rather than hitting the Git repository. Therefore as long as your dependencies specify tagged releases there should be no need to authenticate against a remote Git repository and there should be no authentication issue.",
        "section": "Tutorials",
        "subsections": " Specify a third party repository in composer.json Set the env:COMPOSER_AUTH project variable Build your application with Composer Private repository hosting  ",
        "image": "",
        "url": "/tutorials/composer-auth.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "05c31952e8b65cbf39e79d2ce374629f",
        "title": "Deploy on Platform.sh",
        "description": "",
        "text": " Platform.sh offers a number of project templates as part of the Project Creation Wizard to make bootstrapping a new project easy. However, you can also create arbitrary links to spawn projects on Platform.sh from an arbitrary Git repository or prepared template. There are two ways to create such a link, shown below. In each case, when a user clicks on the link they will be redirected to create a new Platform.sh project, with the template selection step skipped in favor of the template specified. If the user does not have a Platform.sh account yet they will be prompted to create one. You may include the link on your own project’s website, your company’s internal Wiki, or anywhere else a link can go to make launching your code base as simple as possible. Preparation To have a deployable template, you need to first prepare the repository. The Deploy on Platform.sh button will work with any Git repository that is deployable on Platform.sh; that is, it has the necessary .platform.app.yaml , .platform/services.yaml , and .platform/routes.yaml files in place. See the appropriate documentation for how to define those files. The repository must be available at a publicly-accessible Git URL. That may be hosted with GitHub, GitLab, Bitbucket, your own custom Git hosting, or any other publicly-accessible Git URL. (Optional) Make a template definition file You can create a Deploy on Platform.sh button for any compatible repository; however, you can also provide a YAML template definition file. A template definition file is a YAML file that references a Git repository but can also include additional information, such as limiting the resulting project to a certain minimum project size or only allowing it to be deployed in certain regions. Use this mechanism when you want more control over how the template gets deployed. The template definition file may be at any publicly-accessible URL. It can be in the template repository itself or separate. Note that if it is in the template repository then it will be included in every deployed user project from that template. (It won’t hurt anything as it has no effect at runtime, but users will have a copy of the file in their code base and may be confused by it.) A list of all Platform.sh-supported templates is available on GitHub. 3rd party templates are also available. You can also create your own template file and host it anywhere you wish. The template definition file’s format is documented in the 3rd party template repository. Making a button (The easy way) The easiest way to make a Deploy on Platform.sh button is to use our button builder widget . You provide it with either the Git URL of the repository or a URL to a corresponding template definition file. The button builder widget will give you an HTML fragment to copy and paste to wherever you want the button hosted. It will also include a tracking code so we can whose Deploy on Platform.sh button was clicked, but does not add any cookies to the site. Making a button manually Arbitrary Git repository Create a link in the following form: https://console.platform.sh/projects/create-project?template=GIT_URL Where GIT_URL is the URL of a publicly-visible Git repository. For example, to install Platform.sh’s Drupal 8 template on GitHub you would use: https://console.platform.sh/projects/create-project/?template=https://github.com/platformsh-templates/drupal8.git (Note that is the URL of the Git repository as if you were cloning it, NOT the URL of the repository’s home page on GitHub.) A new project will be created and then initialized with whatever code is at the tip of the master branch of that repository. This method will work for any publicly-visible Git repository, provided that it includes the necessary Platform.sh YAML configuration files. If those are missing the project will still initialize but fail to build. Defined Template Create a link in the following form: https://console.platform.sh/projects/create-project?template=TEMPLATE_URL Where TEMPLATE_URL is the URL of a publicly-visible template definition file. For example, to install Platform.sh’s Drupal 8 template you would use: https://console.platform.sh/projects/create-project/?template=https://github.com/platformsh/template-builder/blob/master/templates/drupal8/.platform.template.yaml A new project will be created, initialized with whatever code is at the tip of the master branch of the repository referenced by that file, provided that it includes the necessary Platform.sh YAML configuration files. If those are missing the project will still initialize but fail to build. Listing a repository Platform.sh welcomes project templates produced by the application vendor. If you have a Free Software application you want available in the Platform.sh setup wizard, create a template definition file and submit a pull request against the 3rd party templates repository. The Developer Relations team will review and evaluate the application and template, and may offer feedback before merging. Generally speaking, we welcome any Free Software application that is actively maintained and runs well on Platform.sh. Projects released under a non-Free license will not be accepted.",
        "section": "Featured frameworks",
        "subsections": " Preparation  (Optional) Make a template definition file   Making a button (The easy way) Making a button manually  Arbitrary Git repository Defined Template   Listing a repository  ",
        "image": "",
        "url": "/frameworks/deploy-button.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "e7568a0968d8e29058e876d7483a4127",
        "title": "Developing with Drupal",
        "description": "",
        "text": " Push changes to an environment Here, we’ll see how to make code changes to an environment. Note: You should never be working on the Master branch since it’s supposed to be your production environment. Make sure you’re on a working environment. In this example we’re on the sprint1 branch: $ git checkout sprint1 Now that you’re set up on your working branch, you can start developing on your website by making code changes and pushing those changes to Platform to test them live. There are three common ways you will be making code changes to Platform: Add contributed modules, themes, distributions, third-party libraries in the make file Create custom code (modules, themes, profiles, libraries) and commit them to your Platform codebase Modify the services grid configuration Add contributed projects Each time you push a commit, Platform.sh will rebuild your environment and run the Drush make command if a proper make file has been found. Add a Drupal module Each Drupal module you want to install on your project should be included in the make file. For example, if you want to add Drupal Commerce, you need to add the following lines to your project.make: ; Modules projects[addressfield][version] =  1.0-beta4  projects[addressfield][subdir] =  contrib  projects[ctools][version] =  1.3  projects[ctools][subdir] =  contrib  projects[commerce][version] =  1.8  projects[commerce][subdir] =  contrib  projects[entity][version] =  1.2  projects[entity][subdir] =  contrib  projects[rules][version] =  2.6  projects[rules][subdir] =  contrib  projects[views][version] =  3.7  projects[views][subdir] =  contrib  Add a Drupal theme You’d do the same if you want to add a theme. Add the following lines to your project.make: ; Zen Theme projects[] = zen Add a third-party library You’d do the same if you want to add a third-party library. For our example here, we’re adding the HTML5 Boilerplate library. Add the following lines to your project.make: ; Libraries libraries[html5bp][download][type] =  file  libraries[html5bp][download][url] =  http://github.com/h5bp/html5-boilerplate/zipball/v3.0.2stripped  Add custom code To commit your custom modules, themes or libraries, you need to commit them under a modules, themes or libraries folder at the root of your Git repository. $ ls libraries/ modules/ project.make themes/ When you push your code, Platform will build your environment and move your modules, themes, libraries to the correct location on your site (usually sites/default/). Change the services configuration You can change and define the topology of the services used in an environment, by modifying the configuration files. This means that you’re able to define and configure the services you want to use. Push your changes When you’re done, commit your changes to test them on your online environment. $ git add . $ git commit -m  Made changes to my make file.  $ git push You will see that Platform has found a make file and is starting to rebuild your environment. When it’s completed, you can see your changes on your site by clicking View this website under the name of Sprint1 environment on the Platform.sh management console. Note: The Drush Make processing doesn’t create any file in your Git repository. Your Git repository is the input of the process and not the output. You can see the directory structure that has been created by connecting via SSH to the environment. See the information in the Access information below the title of the environment. Merge code changes to Master Once you’ve got a branch with some changes, you’ll want to be able to push those changes up to your live environment. Platform.sh has a great button called Merge that you can click on and it will push the appropriate changes to master. A dialog box will appear that will provide commands to execute future merges from the command line using the Platform.sh CLI . Just click on the “Merge” button again and all of the commits you made on your branch will be merged into the master environment. Synchronizing data The easiest way to do that is to use Drush and the sql-sync command. You’ll need to have Drush aliases for both your Platform.sh site and your local site. If you are using the CLI and you’ve run platform get [platform_id] for a project, then your Drush aliases have already been set up. With the Drush aliases (depending on how yours are set up), you could use a command similar to this: $ drush sql-sync @platform.master @platform._local An alternate method that is appropriate for larger databases is to use the pipe | to stream the data, instead of making copies. $ drush @platform.master sql-dump | drush @platform._local sqlc",
        "section": "Getting Started",
        "subsections": " Push changes to an environment  Add contributed projects Add custom code Change the services configuration Push your changes   Merge code changes to Master Synchronizing data  ",
        "image": "",
        "url": "/frameworks/drupal7/developing-with-drupal.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "8a8e72c1fa9b6864f9fd403b7f9f439e",
        "title": "Developing with Drupal",
        "description": "",
        "text": " Push changes to an environment Here, we’ll see how to make code changes to an environment. Note: You should never be working on the Master branch since it’s supposed to be your production environment. Make sure you’re on a working environment. In this example we’re on the sprint1 branch: $ git checkout sprint1 Now that you’re set up on your working branch, you can start developing on your website by making code changes and pushing those changes to Platform.sh to test them live. There are three common ways you will be making code changes to Platform: Add contributed modules, themes, distributions, third-party libraries in the make file Create custom code (modules, themes, profiles, libraries) and commit them to your Platform.sh codebase Modify the services grid configuration Add contributed projects Each time you push a commit, Platform.sh will rebuild your environment and run the Composer command if a proper composer.json file has been found. Add a Drupal module or theme Each Drupal module or theme you want to install on your project should be included in your composer.json file. For example: $ composer require drupal/token That will update your composer.json and composer.lock files, which you can then commit. If you’re using Composer, 3rd party PHP libraries can be added in the exact same way as Drupal modules. Add custom code To commit your custom modules, themes, or libraries, add those to the web/modules/custom or web/themes/custom directory and commit them to Git as normal. Change the services configuration You can change and define the topology of the services used in an environment, by modifying the configuration files. This means that you’re able to define and configure the services you want to use. Push your changes When you’re done, commit your changes to test them on your online environment. $ git add . $ git commit -m  Made changes to my files.  $ git push You will see that Platform has found a make file and is starting to rebuild your environment. When it’s completed, you can see your changes on your site by clicking View this website under the name of Sprint1 environment on the Platform.sh management console. Note: The build process makes no changes to your Git repository. Your Git repository is the input of the process. A PHP container containing your code and dependencies is the output. You can see the directory structure that has been created by connecting via SSH to the environment. See the information in the Access information below the title of the environment. Merge code changes to Master Once you’ve got a branch with some changes, you’ll want to be able to push those changes up to your live environment. Platform.sh has a great button called Merge that you can click on and it will push the appropriate changes to master. A dialog box will appear that will provide commands to execute future merges from the command line using the Platform.sh CLI . Just click on the “Merge” button again and all of the commits you made on your branch will be merged into the master environment. Synchronizing data The easiest way to do that is to use Drush and the sql-sync command. You’ll need to have Drush aliases for both your Platform.sh site and your local site. If you are using the CLI and you’ve run platform get [platform_id] for a project, then your Drush aliases have already been set up. With the Drush aliases (depending on how yours are set up), you could use a command similar to this: $ drush sql-sync @platform.master @platform._local An alternate method that is appropriate for larger databases is to use the pipe | to stream the data, instead of making a copy of the dump file. $ drush @platform.master sql-dump | drush @platform._local sqlc",
        "section": "Getting Started",
        "subsections": " Push changes to an environment  Add contributed projects Add custom code Change the services configuration Push your changes   Merge code changes to Master Synchronizing data  ",
        "image": "",
        "url": "/frameworks/drupal8/developing-with-drupal.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "510d6192ab63668f989a8055467930aa",
        "title": "DNS management and Apex domains",
        "description": "",
        "text": " Platform.sh expects you to use a CNAME for all DNS records. However, that is problematic with some DNS registrars. Why CNAMEs? Platform.sh is a cloud hosting provider. That means each individual “site” is not its own computer but a set of containers running on one or more virtual machines, which are themselves running on any number of physical computers, all of which are shared with other customers running the same configuration. An entire region of projects runs behind our dedicated, high-performance edge routers, which are responsible for mapping incoming requests to the particular container on a particular host that is appropriate. All of that logic is quite robust and fast, but it does require that incoming requests all get sent first to the edge routers. While the IP addresses of the edge routers are fairly stable, they are not guaranteed to never change. We also may add or remove routers to help scale the region, or take them offline one at a time for upgrades and maintenance. It is therefore critical that inbound requests always know what the IPs are of the edge routers at the time of the request. All of Platform.sh’s “edge hostnames” (the auto-generated URLs in the form \u0026lt;branch\u0026gt;-\u0026lt;hash\u0026gt;-\u0026lt;project_id\u0026gt;.\u0026lt;region\u0026gt;.platformsh.site) are DNS records we control that resolve to the IP addresses of the edge routers for that region. If an edge router is updated, taken out of rotation, etc. then those domains will update quickly and automatically with no further action required. An A record pointed at the same IP addresses would need to be updated manually every time an edge router changes or is temporarily offline. That means every time Platform.sh is doing routine maintenance or upgrades on the edge routers there’s a significant potential for a site to experience a partial outage if a request comes in for an offline edge router. We don’t want that. You don’t want that. Using a CNAME DNS record pointing at the “edge hostname” will avoid that problem, as it will be updated almost immediately should our edge router configuration change. Why are CNAME records problematic? The DNS specification was originally published in 1987 in RFC 1034 and RFC 1035 , long before name-based HTTP hosting became prevalent. Those RFCs plus the many follow-ups to clarify and expand on it are somewhat vague on the behavior of CNAME, but it’s generally understood that an apex domain (example.com) may not be used as an alias in a CNAME record. That creates a problem if you want to use an apex domain with any container-based managed hosting service like Platform.sh, because of the point above. There’s a detailed thread on the subject that provides more technical detail. Handling Apex domains There are a number of ways of handling the CNAME-on-Apex limitation of DNS. Using a DNS provider with custom records Many DNS providers have found a way around the CNAME-on-Apex limitation. Some DNS registrars now offer custom, non-standard records (sometimes called ANAME or ALIAS) that you can manage like a CNAME but will do their own internal lookup behind the scenes and then respond to DNS lookups as if they were an A record. As these are non-standard their behavior (and quality) can vary, and not all DNS registrars offer such a feature. If you want your site to be accessible with https://example.com and not only https://www.example.com this is the best way to do so. Examples of such workaround records include: CNAME Flattening at CloudFlare ANAME at easyDNS , DNS Made Easy , or Name.com ALIAS at DNSimple or Cloudns Platform.sh recommends ensuring that your DNS Provider supports dynamic apex domains before registering your domain name with them. If you are using a DNS Provider that does not support dynamic apex domains then you will be unable to use example.com with Platform.sh, and will need to use only www.example.com (or similar) instead. (Alternate) Using a DNS provider with apex domain forwarding If you are willing to make the www. version of your site the canonical version (which is recommended), some registrars or DNS providers may provide a domain redirect feature—also known as domain forwarding—from the apex domain example.com to www.example.com. Before looking to change registrars, check whether your current provider supports both domain forwarding for the Apex and the DNS CNAME record to Platform.sh for the www. at the same time. The following DNS providers are known to support both apex forwarding and advanced DNS configurations simultaneously: Namecheap (Alternate) Using a www redirection service If your preferred registrar/DNS provider doesn’t support either custom records or the apex domain forwarding options above, the following free services both allow blind redirects and allow you to use a CNAME record to Platform.sh for www.example.com and an A record to their service at example.com, which will in turn send a redirect. WWWizer redirectssl (Alternate) Using A records If you absolutely cannot use a DNS provider that supports aliases or a redirection service, it is possible to use A records with Platform.sh. They will result in a sub-optimal experience, however. This process has a few limitations: Should we ever need to change one of those IPs your configuration will need to be manually updated. Until it is some requests will be lost. Directly pointing at the edge routers bypasses their load-balancing functionality. Should one of them go offline for maintenance (as happens periodically for upgrades) approximately 1/3 of requests to your site will go to the offline router and be lost, making the site appear offline. For that reason using A records is strongly discouraged and should only be used as a last resort. See the Public IP list for the 3 Inbound addresses for your region. In your DNS provider, configure 3 separate A records for your domain, one for each of those IP addresses. Incoming requests will then pick one of those IPs at random to use for that request.",
        "section": "Going Live - Steps",
        "subsections": " Why CNAMEs? Why are CNAME records problematic? Handling Apex domains  Using a DNS provider with custom records (Alternate) Using a DNS provider with apex domain forwarding (Alternate) Using a www redirection service (Alternate) Using A records    ",
        "image": "",
        "url": "/golive/steps/dns.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "54356fc25fb1d69e29ae7a2087a256e3",
        "title": "Download the code",
        "description": "",
        "text": " If you have already pushed your code to Platform.sh, then you should already have a local repository that you can build from. Otherwise, it will be necessary to download a local copy of your project first. Get project ID You will need the your project ID. You can retrieve this ID at any time using the CLI commands platform or platform project:list. Get a copy of the repository locally Next, use the CLI to download the code in your Platform.sh project using the command platform get \u0026lt;project id\u0026gt; Now that you have a local copy of your application that is configured to the Platform.sh remote repository, you can create a new . Back I have a local copy of my code",
        "section": "Getting started",
        "subsections": "",
        "image": "",
        "url": "/gettingstarted/developing/dev-environments/download-code.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "f71b9b0dd624699141f8ced05415fcf2",
        "title": "Download the code",
        "description": "",
        "text": " If you have already pushed your code to Platform.sh, then you should already have a local repository that you can build from. Otherwise, it will be necessary to download a local copy of your project first. Get project ID You will need the your project ID. You can retrieve this ID at any time using the CLI command platform. Get a copy of the repository locally Next, use the CLI to download the code in your Platform.sh project using the command platform get \u0026lt;project id\u0026gt; Next you can now connect to its services and build it on your machine. Back I have a local copy of my code",
        "section": "Getting started",
        "subsections": "",
        "image": "",
        "url": "/gettingstarted/developing/local-development/download-code.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "d84b4a45a729c5c6a88a44a778191f7f",
        "title": "Elasticsearch (Search service)",
        "description": "",
        "text": " Elasticsearch is a distributed RESTful search engine built for the cloud. See the Elasticsearch documentation for more information. Supported versions Grid Dedicated 6.5 7.2 5.2 6.5 Deprecated versions The following versions are available but are not receiving security updates from upstream, so their use is not recommended. They will be removed at some point in the future. Grid Dedicated 0.9 1.4 1.7 2.4 5.2 5.4 1.7 2.4 Relationship The format exposed in the $PLATFORM_RELATIONSHIPS environment variable : {  cluster :  rjify4yjcwxaa-master-7rqtwti ,  fragment : null,  host :  elasticsearch.internal ,  host_mapped : false,  hostname :  gnenv2b23ltik7mrvu3fyrlybq.elasticsearch.service._.eu-3.platformsh.site ,  ip :  169.254.252.137 ,  password : null,  path : null,  port : 9200,  public : false,  query : [],  rel :  elasticsearch ,  scheme :  http ,  service :  elasticsearch ,  type :  elasticsearch:7.2 ,  username : null } Usage example In your .platform/services.yaml: searchelastic:type:elasticsearch:7.2disk:256 In your .platform.app.yaml: relationships:essearch: searchelastic:elasticsearch  Note: You will need to use the elasticsearch type when defining the service # .platform/services.yamlservice_name:type:elasticsearch:versiondisk:256 and the endpoint elasticsearch when defining the relationship # .platform.app.yamlrelationships:relationship_name:“service_name:elasticsearch” Your service_name and relationship_name are defined by you, but we recommend making them distinct from each other. You can then use the service in a configuration file of your application with something like: Java Nodejs PHP Python package sh.platform.languages.sample; import org.elasticsearch.action.admin.indices.refresh.RefreshRequest; import org.elasticsearch.action.admin.indices.refresh.RefreshResponse; import org.elasticsearch.action.delete.DeleteRequest; import org.elasticsearch.action.index.IndexRequest; import org.elasticsearch.action.search.SearchRequest; import org.elasticsearch.action.search.SearchResponse; import org.elasticsearch.client.RequestOptions; import org.elasticsearch.client.RestHighLevelClient; import org.elasticsearch.index.query.QueryBuilders; import org.elasticsearch.search.SearchHit; import org.elasticsearch.search.builder.SearchSourceBuilder; import sh.platform.config.Config; import sh.platform.config.Elasticsearch; import java.io.IOException; import java.util.Arrays; import java.util.HashMap; import java.util.List; import java.util.Map; import java.util.function.Supplier; import static java.util.concurrent.ThreadLocalRandom.current; public class ElasticsearchSample implements Supplier\u0026lt;String\u0026gt; { @Override public String get() { StringBuilder logger = new StringBuilder(); // Create a new config object to ease reading the Platform.sh environment variables. // You can alternatively use getenv() yourself. Config config = new Config(); Elasticsearch elasticsearch = config.getCredential( elasticsearch , Elasticsearch::new); // Create an Elasticsearch client object. RestHighLevelClient client = elasticsearch.get(); try { String index =  animals ; String type =  mammals ; // Index a few document. final List\u0026lt;String\u0026gt; animals = Arrays.asList( dog ,  cat ,  monkey ,  horse ); for (String animal : animals) { Map\u0026lt;String, Object\u0026gt; jsonMap = new HashMap\u0026lt;\u0026gt;(); jsonMap.put( name , animal); jsonMap.put( age , current().nextInt(1, 10)); jsonMap.put( is_cute , current().nextBoolean()); IndexRequest indexRequest = new IndexRequest(index, type) .id(animal).source(jsonMap); client.index(indexRequest, RequestOptions.DEFAULT); } RefreshRequest refresh = new RefreshRequest(index); // Force just-added items to be indexed RefreshResponse refreshResponse = client.indices().refresh(refresh, RequestOptions.DEFAULT); // Search for documents. SearchSourceBuilder sourceBuilder = new SearchSourceBuilder(); sourceBuilder.query(QueryBuilders.termQuery( name ,  dog )); SearchRequest searchRequest = new SearchRequest(); searchRequest.indices(index); searchRequest.source(sourceBuilder); SearchResponse search = client.search(searchRequest, RequestOptions.DEFAULT); for (SearchHit hit : search.getHits()) { String id = hit.getId(); final Map\u0026lt;String, Object\u0026gt; source = hit.getSourceAsMap(); logger.append(String.format( result id %s source: %s , id, } // Delete documents. for (String animal : animals) { client.delete(new DeleteRequest(index, type, animal), RequestOptions.DEFAULT); } } catch (IOException exp) { throw new RuntimeException( An error when execute Elasticsearch:   \u0026#43; exp.getMessage()); } return logger.toString(); } } const elasticsearch = require(\u0026#39;elasticsearch\u0026#39;); const config = require( platformsh-config ).config(); exports.usageExample = async function() { const credentials = config.credentials(\u0026#39;elasticsearch\u0026#39;); var client = new elasticsearch.Client({ host: `${credentials.host}:${credentials.port}`, }); let index = \u0026#39;my_index\u0026#39;; let type = \u0026#39;People\u0026#39;; // Index a few document. let names = [\u0026#39;Ada Lovelace\u0026#39;, \u0026#39;Alonzo Church\u0026#39;, \u0026#39;Barbara Liskov\u0026#39;]; let message = { refresh:  wait_for , body: [] }; names.forEach((name) =\u0026gt; { message.body.push({index: {_index: index, _type: type}}); message.body.push({name: name}); }); await client.bulk(message); // Search for documents. const response = await client.search({ index: index, q: \u0026#39;name:Barbara Liskov\u0026#39; }); let output = \u0026#39;\u0026#39;; if(response.hits.total.value \u0026gt; 0) { output \u0026#43;= `\u0026lt;table\u0026gt; \u0026lt;thead\u0026gt; \u0026lt;tr\u0026gt;\u0026lt;th\u0026gt;ID\u0026lt;/th\u0026gt;\u0026lt;th\u0026gt;Name\u0026lt;/th\u0026gt;\u0026lt;/tr\u0026gt; \u0026lt;/thead\u0026gt; \u0026lt;tbody\u0026gt;`; response.hits.hits.forEach((record) =\u0026gt; { output \u0026#43;= }); output \u0026#43;= } else { output =  No records found. ; } // Clean up after ourselves. response.hits.hits.forEach((record) =\u0026gt; { client.delete({ index: index, type: type, id: record._id, }); }); return output; }; \u0026lt;?php declare(strict_types=1); use use // Create a new config object to ease reading the Platform.sh environment variables. // You can alternatively use getenv() yourself. $config = new Config(); // Get the credentials to connect to the Elasticsearch service. $credentials = $config-\u0026gt;credentials(\u0026#39;elasticsearch\u0026#39;); try { // The Elasticsearch library lets you connect to multiple hosts. // On Platform.sh Standard there is only a single host so just // register that. $hosts = [ [ \u0026#39;scheme\u0026#39; =\u0026gt; $credentials[\u0026#39;scheme\u0026#39;], \u0026#39;host\u0026#39; =\u0026gt; $credentials[\u0026#39;host\u0026#39;], \u0026#39;port\u0026#39; =\u0026gt; $credentials[\u0026#39;port\u0026#39;], ] ]; // Create an Elasticsearch client object. $builder = ClientBuilder::create(); $builder-\u0026gt;setHosts($hosts); $client = $builder-\u0026gt;build(); $index = \u0026#39;my_index\u0026#39;; $type = \u0026#39;People\u0026#39;; // Index a few document. $params = [ \u0026#39;index\u0026#39; =\u0026gt; $index, \u0026#39;type\u0026#39; =\u0026gt; $type, ]; $names = [\u0026#39;Ada Lovelace\u0026#39;, \u0026#39;Alonzo Church\u0026#39;, \u0026#39;Barbara Liskov\u0026#39;]; foreach ($names as $name) { $params[\u0026#39;body\u0026#39;][\u0026#39;name\u0026#39;] = $name; $client-\u0026gt;index($params); } // Force just-added items to be indexed. $client-\u0026gt;indices()-\u0026gt;refresh(array(\u0026#39;index\u0026#39; =\u0026gt; $index)); // Search for documents. $result = $client-\u0026gt;search([ \u0026#39;index\u0026#39; =\u0026gt; $index, \u0026#39;type\u0026#39; =\u0026gt; $type, \u0026#39;body\u0026#39; =\u0026gt; [ \u0026#39;query\u0026#39; =\u0026gt; [ \u0026#39;match\u0026#39; =\u0026gt; [ \u0026#39;name\u0026#39; =\u0026gt; \u0026#39;Barbara Liskov\u0026#39;, ], ], ], ]); if (isset($result[\u0026#39;hits\u0026#39;][\u0026#39;hits\u0026#39;])) { print \u0026lt;\u0026lt;\u0026lt;TABLE\u0026lt;table\u0026gt; \u0026lt;thead\u0026gt; \u0026lt;tr\u0026gt;\u0026lt;th\u0026gt;ID\u0026lt;/th\u0026gt;\u0026lt;th\u0026gt;Name\u0026lt;/th\u0026gt;\u0026lt;/tr\u0026gt; \u0026lt;/thead\u0026gt; \u0026lt;tbody\u0026gt; TABLE; foreach ($result[\u0026#39;hits\u0026#39;][\u0026#39;hits\u0026#39;] as $record) { $record[\u0026#39;_id\u0026#39;], $record[\u0026#39;_source\u0026#39;][\u0026#39;name\u0026#39;]); } print } // Delete documents. $params = [ \u0026#39;index\u0026#39; =\u0026gt; $index, \u0026#39;type\u0026#39; =\u0026gt; $type, ]; $ids = array_map(function($row) { return $row[\u0026#39;_id\u0026#39;]; }, $result[\u0026#39;hits\u0026#39;][\u0026#39;hits\u0026#39;]); foreach ($ids as $id) { $params[\u0026#39;id\u0026#39;] = $id; $client-\u0026gt;delete($params); } } catch (Exception $e) { print $e-\u0026gt;getMessage(); } import elasticsearch from platformshconfig import Config def usage_example(): # Create a new Config object to ease reading the Platform.sh environment variables. # You can alternatively use os.environ yourself. config = Config() # Get the credentials to connect to the Elasticsearch service. credentials = config.credentials(\u0026#39;elasticsearch\u0026#39;) try: # The Elasticsearch library lets you connect to multiple hosts. # On Platform.sh Standard there is only a single host so just register that. hosts = {  scheme : credentials[\u0026#39;scheme\u0026#39;],  host : credentials[\u0026#39;host\u0026#39;],  port : credentials[\u0026#39;port\u0026#39;] } # Create an Elasticsearch client object. client = elasticsearch.Elasticsearch([hosts]) # Index a few documents es_index = \u0026#39;my_index\u0026#39; es_type = \u0026#39;People\u0026#39; params = {  index : es_index,  type : es_type,  body : { name : \u0026#39;\u0026#39;} } names = [\u0026#39;Ada Lovelace\u0026#39;, \u0026#39;Alonzo Church\u0026#39;, \u0026#39;Barbara Liskov\u0026#39;] ids = {} for name in names: params[\u0026#39;body\u0026#39;][\u0026#39;name\u0026#39;] = name ids[name] = client.index(index=params[ index ], doc_type=params[ type ], body=params[\u0026#39;body\u0026#39;]) # Force just-added items to be indexed. client.indices.refresh(index=es_index) # Search for documents. result = client.search(index=es_index, body={ \u0026#39;query\u0026#39;: { \u0026#39;match\u0026#39;: { \u0026#39;name\u0026#39;: \u0026#39;Barbara Liskov\u0026#39; } } }) table = \u0026#39;\u0026#39;\u0026#39;\u0026lt;table\u0026gt; \u0026lt;thead\u0026gt; \u0026lt;tr\u0026gt;\u0026lt;th\u0026gt;ID\u0026lt;/th\u0026gt;\u0026lt;th\u0026gt;Name\u0026lt;/th\u0026gt;\u0026lt;/tr\u0026gt; \u0026lt;/thead\u0026gt; \u0026lt;tbody\u0026gt;\u0026#39;\u0026#39;\u0026#39; if result[\u0026#39;hits\u0026#39;][\u0026#39;hits\u0026#39;]: for record in result[\u0026#39;hits\u0026#39;][\u0026#39;hits\u0026#39;]: table \u0026#43;= record[\u0026#39;_source\u0026#39;][\u0026#39;name\u0026#39;]) table \u0026#43;= # Delete documents. params = {  index : es_index,  type : es_type, } for name in names: client.delete(index=params[\u0026#39;index\u0026#39;], doc_type=params[\u0026#39;type\u0026#39;], id=ids[name][\u0026#39;_id\u0026#39;]) return table except Exception as e: return e Note: When you create an index on Elasticsearch, you should not specify number_of_shards and number_of_replicas settings in your Elasticsearch API call. These values will be set automatically based on available resources. Authentication By default, Elasticsearch has no authentication. No username or password is required to connect to it. Starting with Elasticsearch 7.2 you may optionally enable HTTP Basic authentication. To do so, include the following in your services.yaml configuration: search:type:elasticsearch:7.2disk:2048configuration:authentication:enabled:trueThat will enable mandatory HTTP Basic auth on all requests. The credentials will be available in any relationships that point at that service, in the username and password properties, respectively. This functionality is generally not required if Elasticsearch is not exposed on it own public HTTP route. However, certain applications may require it, or it allows you to safely expose Elasticsearch directly to the web. To do so, add a route to routes.yaml that has search:http as its upstream (where search is whatever you named the service in services.yaml). Plugins The Elasticsearch 2.4 and later services offer a number of plugins. To enable them, list them under the configuration.plugins key in your services.yaml file, like so: search:type: elasticsearch:7.2 disk:1024configuration:plugins:- analysis-icu- lang-pythonIn this example you’d have the ICU analysis plugin and Python script support plugin. If there is a publicly available plugin you need that is not listed here, please contact our support team. Available plugins This is the complete list of official Elasticsearch plugins that can be enabled: Plugin Description 2.4 5.2 5.4 6.5 7.2 analysis-icu Support ICU Unicode text analysis * * * * * analysis-nori Integrates Lucene nori analysis module into Elasticsearch * * analysis-kuromoji Japanese language support * * * * * analysis-smartcn Smart Chinese Analysis Plugins * * * * * analysis-stempel Stempel Polish Analysis Plugin * * * * * analysis-phonetic Phonetic analysis * * * * * analysis-ukrainian Ukrainian language support * * * * cloud-aws AWS Cloud plugin, allows storing indices on AWS S3 * delete-by-query Support for deleting documents matching a given query * discovery-multicast Ability to form a cluster using TCP/IP multicast messages * ingest-attachment Extract file attachments in common formats (such as PPT, XLS, and PDF) * * * * ingest-user-agent Extracts details from the user agent string a browser sends with its web requests * * * lang-javascript Javascript language plugin, allows the use of Javascript in Elasticsearch scripts * * lang-python Python language plugin, allows the use of Python in Elasticsearch scripts * * * mapper-annotated-text Adds support for text fields with markup used to inject annotation tokens into the index * * mapper-attachments Mapper attachments plugin for indexing common file types * * * mapper-murmur3 Murmur3 mapper plugin for computing hashes at index-time * * * * * mapper-size Size mapper plugin, enables the _size meta field * * * * * repository-s3 Support for using S3 as a repository for Snapshot/Restore * * * * Upgrading The Elasticsearch data format sometimes changes between versions in incompatible ways. Elasticsearch does not include a data upgrade mechanism as it is expected that all indexes can be regenerated from stable data if needed. To upgrade (or downgrade) Elasticsearch you will need to use a new service from scratch. There are two ways of doing that. Destructive In your services.yaml file, change the version of your Elasticsearch service and its name. Then update the name in the .platform.app.yaml relationships block. When you push that to Platform.sh, the old service will be deleted and a new one with the name name created, with no data. You can then have your application reindex data as appropriate. This approach is simple but has the downside of temporarily having an empty Elasticsearch instance, which your application may or may not handle gracefully, and needing to rebuild your index afterward. Depending on the size of your data that could take a while. Transitional For a transitional approach you will temporarily have two Elasticsearch services. Add a second Elasticsearch service with the new version a new name and give it a new relationship in .platform.app.yaml. You can optionally run in that configuration for a while to allow your application to populate indexes in the new service as well. Once you’re ready to cut over, remove the old Elasticsearch service and relationship. You may optionally have the new Elasticsearch service use the old relationship name if that’s easier for your application to handle. Your application is now using the new Elasticsearch service. This approach has the benefit of never being without a working Elasticsearch instance. On the downside, it requires two running Elasticsearch servers temporarily, each of which will consume resources and need adequate disk space. Depending on the size of your data that may be a lot of disk space.",
        "section": "Configure services",
        "subsections": " Supported versions  Deprecated versions   Relationship Usage example Authentication Plugins  Available plugins   Upgrading  Destructive Transitional    ",
        "image": "",
        "url": "/configuration/services/elasticsearch.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "01a8fa795000ebab32a6f3ec0bf49c7d",
        "title": "Extensions",
        "description": "",
        "text": " You can define the PHP extensions you want to enable or disable: # .platform.app.yamlruntime:extensions:- http- redis- ssh2disabled_extensions:- sqlite3The following extensions are enabled by default: bcmath bz2 (7.1 and later) common (7.1 and later) curl dba (7.1 and later) enchant (7.1 and later) gd interbase (7.1 and later) intl json (5.6 and later) ldap (7.1 and later) mbstring (7.1 and later) mcrypt (5.6 and earlier) mysql mysqli (not in 7.1) mysqlnd (not in 7.1) odbc (7.1 and later) openssl pdo (not in 7.1) pdo_mysql (not in 7.1) pdo_sqlite (not in 7.1) pgsql (7.1 and later) pspell (7.1 and later) readline (7.1 and later) recode (7.1 and later) snmp (7.1 and later) soap (7.1 and later) sodium (7.2) sqlite3 sockets (7.0 and later) sybase (7.1 and later) tidy (7.1 and later) xml (7.1 and later) xmlrpc (7.1 and later) zendopcache (5.4 only) / opcache (5.5 and later) zip (7.1 and later) You can disable those by adding them to the disabled_extensions list. This is the complete list of extensions that can be enabled: Extension 5.4 5.5 5.6 7.0 7.1 7.2 7.3 7.4 amqp * * * * * apc * * apcu * * * * * * * apcu_bc * * * * * applepay * * * bcmath * * * * * blackfire * * * * * * * * bz2 * * * * * calendar * * * * * ctype * * * * * curl * * * * * * * * dba * * * * * dom * * * * * enchant * * * * * * * * event * * * * exif * * * * * ffi * fileinfo * * * * * ftp * * * * * gd * * * * * * * * gearman * * * geoip * * * * * * * * gettext * * * * * gmp * * * * * * * * http * * * iconv * * * * * igbinary * * * * * imagick * * * * * * * imap * * * * * * * * interbase * * * * * * * * intl * * * * * * * * ioncube * * * json * * * * * * ldap * * * * * * * * mailparse * * * * mbstring * * * * * mcrypt * * * * * memcache * * * memcached * * * * * * * mongo * * * mongodb * * * * msgpack * * * * * mssql * * * mysql * * * mysqli * * * * * * * * mysqlnd * * * * * * * * newrelic * * * * * * oauth * * * * * odbc * * * * * * * * opcache * * * * * * * openssl pdo * * * * * * * * pdo_dblib * * * * * * * * pdo_firebird * * * * * * * * pdo_mysql * * * * * * * * pdo_odbc * * * * * * * * pdo_pgsql * * * * * * * * pdo_sqlite * * * * * * * * pdo_sqlsrv * * * * http * * pgsql * * * * * * * * phar * * * * * pinba * * * posix * * * * * propro * pspell * * * * * * * * pthreads * raphf * * readline * * * * * * * * recode * * * * * * * * redis * * * * * * * shmop * * * * * simplexml * * * * * snmp * * * * * * * * soap * * * * * sockets * * * * * sodium * * * sourceguardian * * spplus * * sqlite3 * * * * * * * * sqlsrv * * * * ssh2 * * * * * * * * sysvmsg * * * * * sysvsem * * * * * sysvshm * * * * * tideways * * * * * tideways-xhprof * * * * * tidy * * * * * * * * tokenizer * * * * * uuid * * * * wddx * * * * * xcache * * xhprof * * * xml * * * * * xmlreader * * * * * xmlrpc * * * * * * * * xmlwriter * * * * * xsl * * * * * * * * yaml * * * * zbarcode * * * * zendopcache * zip * * * * * Note: You can check out the output of ls /etc/php5/mods-available to see the up-to-date complete list of extensions after you SSH into your environment. For PHP 7, use ls /etc/php/*/mods-available. Custom PHP extensions It is possible to use an extension not listed here but it takes slightly more work: Download the .so file for the extension as part of your build hook using curl or similar. It can also be added to your Git repository if the file is not publicly downloadable, although committing large binary blobs to Git is generally not recommended. Provide a custom php.ini file in the application root (as a sibling of your .platform.app.yaml file) that loads the extension using an absolute path. For example, if the extension is named spiffy.so and is in the root of your application, you would have a php.ini file that reads: extension=/app/spiffy.so",
        "section": "PHP",
        "subsections": " Custom PHP extensions  ",
        "image": "",
        "url": "/languages/php/extensions.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "825951fcb54f90e0357f9249fad3567e",
        "title": "eZ Platform Enterprise with Fastly",
        "description": "",
        "text": " eZ Platform Enterprise is a “commercial extended” version of ez Platform that includes, among other things, support for push-based purging on the Fastly CDN. Remove Varnish configuration As of eZ Platform 1.13.5, 2.4.3 and 2.5.0, Varnish is enabled by default when deploying on Platform.sh. In order to use Fastly, Varnish must be disabled: Remove environment variable SYMFONY_TRUSTED_PROXIES:  TRUST_REMOTE  in .platform.app.yaml Remove the Varnish service in .platform/services.yaml In .platform/routes.yaml , change routes to use app instead of the varnish service you removed in previous step:  https://{default}/ : type: upstream - upstream:  varnish:http  \u0026#43; upstream:  app:http  Setting up eZ Platform to use Fastly eZ Platform’s documentation includes instructions on how to configure eZ Platform for Fastly . Follow the steps there to prepare eZ Platform for Fastly. Set credentials on Platform.sh The best way to provide the Fastly credentials and configuration to eZ Platform on Platform.sh is via environment variables. That way private credentials are never stored in Git. Using the CLI, run the following commands to set the configuration on your master environment. (Note that they will inherit to all other environments by default unless overridden.) platform variable:create -e master --level environment env:HTTPCACHE_PURGE_TYPE --value \u0026#39;fastly\u0026#39; platform variable:create -e master --level environment env:FASTLY_SERVICE_ID --value \u0026#39;YOUR_ID_HERE\u0026#39; platform variable:create -e master --level environment env:FASTLY_KEY --value \u0026#39;YOUR_ID_HERE\u0026#39; Replacing YOUR_ID_HERE with the Fastly Service ID and Key obtained from Fastly. Note: On a Platform.sh Dedicated Cluster, set those values on the production branch instead: platform variable:set -e production env:HTTPCACHE_PURGE_TYPE fastly platform variable:set -e production env:FASTLY_SERVICE_ID YOUR_ID_HERE platform variable:set -e production env:FASTLY_KEY YOUR_ID_HERE Configure Fastly and Platform.sh See the alternate Go-live process for Fastly on Platform.sh. This process is the same for any application.",
        "section": "eZ Platform",
        "subsections": " Remove Varnish configuration Setting up eZ Platform to use Fastly Set credentials on Platform.sh Configure Fastly and Platform.sh  ",
        "image": "",
        "url": "/frameworks/ez/fastly.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "8367814ee69067a64df014978d01ed92",
        "title": "Fastly",
        "description": "",
        "text": " In some cases you may want to opt to use a CDN such as Fastly rather than the Platform.sh router’s cache. Using a CDN can offer a better time-to-first-byte for cached content across a wider geographic region at the cost of the CDN service. A Fastly CDN is included for Platform.sh Dedicated instances. Platform.sh does not offer an integrated CDN on self-service Grid projects at this time, but it is a common choice for customers to self-configure. Launching a Platform.sh site with Fastly in front of it is nearly the same as launching normally. There are only two notable differences. Note that individual applications may have their own Fastly setup instructions or additional modules. Consult the documentation for your application for specific details. Set the Platform.sh domain on Fastly Rather than create a DNS CNAME for your Platform.sh master branch (for instance master-7rqtwti-qwertyqwerty.eu.platform.sh), configure Fastly to respond to requests for your domain name and to treat the Platform.sh master branch as its backend server. Be sure to enable TLS for the backend connection to Platform.sh. Then configure your DNS to point your domain at Fastly instead. See the Fastly documentation for further details. DNS TXT records If using the Fastly CDN that is included with a Platform.sh Enterprise subscription, You will need to obtain a DNS TXT record from your Customer Support Engineer prior to going live. You will need to enter that as a DNS TXT record with your domain registrar. This step should be done well in advance of the actual go-live. Anycast You have the option of using either a CNAME or a set of Anycast IP addresses . Fastly prefers that you use the CNAME but either work. If using the Anycast IP addresses on a Dedicated production environment, open a support ticket with the new A records to provide to our support team.",
        "section": "Content Delivery Networks",
        "subsections": " Set the Platform.sh domain on Fastly DNS TXT records Anycast  ",
        "image": "",
        "url": "/golive/cdn/fastly.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "2c9ba456146a67059fc31187ffd5ac78",
        "title": "First steps",
        "description": "",
        "text": " Before you take your site live, there are a few steps that will help you prepare the project. Register your domain and choose a suitable DNS provider If you plan on serving exclusively from a subdomain such as the historically common www. subdomain, you will be able use any DNS provider that supports CNAME records. If you wish to use the apex domain, eg. https://site.com, with no www. subdomain, choose one of the specialized DNS providers that allow you to use ALIAS or ANAME records . Make sure to do this before moving on to the next steps, as the CLI will reject attempts to add domains that do not allow CNAMEs. Test your routes Test your application and make sure that all of your routes are functioning as you intended. Consult the routes documentation as well to verify that your routes.yaml has been properly written. If any access restrictions have been enabled during development, be sure to remove them as well. (Optional) Obtain 3rd party SSL if needed Let’s Encrypt SSL certificates are automatically issued for Platform.sh projects at no charge to you. If you instead would like to use a 3rd party SSL certificate , make sure that you have purchased it and that it is active prior to going live. Additionally, if your application uses wildcard routes , it will require custom certificates for them, as Let’s Encrypt does not support wildcard certificates. After you have gone through the following checklist your application is ready to be taken live! Back I\u0026#39;m ready to go live",
        "section": "Getting started",
        "subsections": "",
        "image": "",
        "url": "/gettingstarted/next-steps/going-live/first-steps.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "e3e63c152cb6e8996712109db5345431",
        "title": "GDPR Overview Page",
        "description": "",
        "text": " Platform.sh has taken numerous steps to ensure GDPR compliance. As part of our measures we have implemented the following: Data Protection Officer: Appointment of a Security Officer who also holds the Data Protection Officer (DPO) role. Data Breach Policy: We have updated our data breach policy and procedures and have reviewed that all our suppliers are compliant with breach notifications. Consent: We’ve confirmed that all of our customer communication, both business-related and marketing-related, is opt-in and no information is shared with us without a customer’s consent. Data Governance: We have internally audited all of our suppliers on their internal security and their GDPR compliance status and can confirm that our in-scope suppliers are GDPR compliant. Data Protection by design: We’ve implemented policies in the company to ensure all of our employees follow the necessary training and protocols around security. In addition, privacy protection is part of every project during instantiation. Enhanced Rights: The GDPR provides rights to individuals such as the right to portability, right of rectification, and the right to be forgotten. We’ve made sure we comply with these rights. Nearly all information can be edited through a user’s account, and we can delete accounts upon request. Personally identifiable information (PII): We’ve audited our systems to confirm that we encrypt and protect your personal data. Data Flows: We have identified data, mapped the high level data flow, and mapped data shared with vendors - including cross-border transfers. PIA: We have performed an internal Privacy Impact Analysis (PIA) using the CNIL’s PIA Software to ensure we comply with the GDPR. Security: We have created https://platform.sh/security to document our security features. Data Collection: We’ve documented information about what data we collect . Data Retention: We documented information about our data retention . DPA: We have revised our Terms of Service and Privacy Policy to align with the GDPR and we offer a pre-signed DPA agreement that can be downloaded at the top of the Privacy Policy",
        "section": "Security and compliance",
        "subsections": "",
        "image": "",
        "url": "/security/gdpr.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "d7b7e6b79418de441c413fb39a820835",
        "title": "Getting Started",
        "description": "",
        "text": " Structure your files Platform.sh is very flexible and allows you to structure your files as you wish within your Git repository, and will build your project based on how your files are organized. Here are the three build modes that can be used: Profile: Platform.sh builds your project like Drupal.org does for distributions. Project: Platform.sh builds your make file using drush make. You don’t need any Drupal core files nor any contributed modules, themes or libraries within your Git repository. Vanilla: Platform.sh builds your project as it is in your Git repository. You can push all Drupal files and contributed modules, themes or libraries. Profile mode If your repository contains a .profile file, Platform.sh builds your project in profile mode. This is similar to what Drupal.org does to build distributions. Everything you have in your repository will be copied to your profiles/[name] folder. This build mode supports having a project.make file for your contributed modules, themes or libraries. Note: When building as a profile, you need a make file for Drupal core called: project-core.make. See drush make files. .git/ project.make project-core.make my_profile.info my_profile.install my_profile.profile modules/ features/ my_feature_01/ ... custom/ my_custom_module/ ... themes/ custom/ my_custom_theme/ ... libraries/ custom/ my_custom_libraries/ ... translations/ ... Project mode If your repository doesn’t contain a .profile file, but contains a make file called: project.make (or even drupal-org.make), Platform.sh builds your project using Drush make. Everything you have in your repository will be copied to your sites/default folder. .git/ project.make modules/ features/ my_feature_01/ ... custom/ my_custom_module/ ... themes/ custom/ my_custom_theme/ ... libraries/ custom/ my_custom_libraries/ ... translations/ ... Vanilla mode In Vanilla mode, Platform.sh just takes your repository as-is without any additional reorganization. This is the behavior when there is no .make or .profile file, or when the build mode is set to none or composer rather than to drupal. It’s best to keep your docroot separate from your repository root, as that allows you to store private files outside of the docroot when needed. For example, your repository layout will likely resemble the following: .git/ private/ web/ index.php ... (other Drupal core files) sites/ all/ modules/ themes/ default/ If you already have a Drupal 7 site built from a tar.gz download from Drupal.org, this is likely the best path forward. Configuring Platform.sh for Drupal The ideal .platform.app.yaml file will vary from project project, and you are free to customize yours as needed. A recommended baseline Drupal 7 configuration is listed below, and can also be found in our Drupal 7 template project or Drupal 7 vanilla template project . Note: Your database for Drupal must be named “database” in the relationships. # This file describes an application. You can have multiple applications# in the same project.## See https://docs.platform.sh/user_guide/reference/platform-app-yaml.html# The name of this app. Must be unique within a project.name:'app'# The runtime the application uses.type:'php:7.2'# Configuration of the build of this application.build:flavor:drupal# The build-time dependencies of the app.dependencies:php: drush/drush :  ^8.0 # The relationships of the application with services or other applications.## The left-hand side is the name of the relationship as it will be exposed# to the application in the PLATFORM_RELATIONSHIPS variable. The right-hand# side is in the form `\u003cservice name\u003e:\u003cendpoint name\u003e`.relationships:database:'db:mysql'# The configuration of app when it is exposed to the web.web:# Specific parameters for different URL prefixes.locations:'/':# The folder from which to serve static assets, for this location.## This is a filesystem path, relative to the application root.root:'public'# How long to allow static assets from this location to be cached.## Can be a time in seconds, or -1 for no caching. Times can be# suffixed with  s  (seconds),  m  (minutes),  h  (hours),  d # (days),  w  (weeks),  M  (months, as 30 days) or  y  (years, as# 365 days).expires:5m# Whether to forward disallowed and missing resources from this# location to the application.## Can be true, false or a URI path string.passthru:'/index.php'# Deny access to static files in this location.allow:false# Rules for specific URI patterns.rules:# Allow access to common static Allow access to all files in the public files directory.allow:trueexpires:5mpassthru:'/index.php'root:'public/sites/default/files'# Do not execute PHP scripts.scripts:falserules:# Provide a longer TTL (2 weeks) for aggregated CSS and JS files.'^/sites/default/files/(css|js)':expires:2w# The size of the persistent disk of the application (in MB).disk:2048# The mounts that will be performed when the package is deployed.mounts:'/public/sites/default/files':source:localsource_path:'files''/tmp':source:localsource_path:'tmp''/private':source:localsource_path:'private''/drush-backups':source:localsource_path:'drush-backups'# The hooks executed at various points in the lifecycle of the application.hooks:# We run deploy hook after your application has been deployed and started.deploy:| set -ecdpublicdrush-yupdatedb# The configuration of scheduled execution.crons:drupal:spec:'*/20 * * * *'cmd:'cd public ; drush core-cron'",
        "section": "Featured frameworks",
        "subsections": " Structure your files  Profile mode Project mode Vanilla mode   Configuring Platform.sh for Drupal  ",
        "image": "",
        "url": "/frameworks/drupal7.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "ec8f3a3b9a988a75eb947e6bc77adcfb",
        "title": "Glossary",
        "description": "",
        "text": " Active Environment An environment which is deployed. You can deactivate an active environment from the environment configuration page on the Platform.sh management console. Cluster Every active environment is deployed as a cluster, that is, a collection of independent containers representing different services that make up your web application. That may include a database container, an Elasticsearch container, a container for your application, etc. They are always deployed together as a single unit. Drush Drush is a command-line shell and scripting interface for Drupal. Drush aliases Drush site aliases allow you to define short names that let you run Drush commands on specific local or remote Drupal installations. The Platform.sh CLI configures Drush aliases for you on your local environment (via platform get or platform drush-aliases). You can also configure them manually. Inactive environment An environment which is not deployed. You can activate an inactive environment from the environment configuration page on the Platform.sh management console. Live Environment An environment which is deployed from the master branch under a production plan. PaaS A Platform as a Service is an end-to-end hosting solution that includes workflow tools, APIs, and other functionality above and beyond basic hosting. The best example is Platform.sh (although we are a little biased). Production plan A subscription level which allows you to host your production website by adding a domain and a custom SSL certificate. TLS Transport Layer Security is the successor of SSL (Secure Socket Layer). It provides the cryptographic “S” in HTTPS. It’s often still referred to as SSL even though it has largely replaced SSL for online encrypted connections.",
        "section": "Platform.sh",
        "subsections": " Active Environment Cluster Drush Drush aliases Inactive environment Live Environment PaaS Production plan TLS  ",
        "image": "",
        "url": "/other/glossary.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "bdab87fc4b2a98e0864b07f5703a61c9",
        "title": "Going Live - Pre-Launch Checklist",
        "description": "",
        "text": " Before you can take your site live there are a few preparation steps to take. 1. Register a domain name with a supported provider You have a domain name registered for your site with a Registrar of your choice. The registrar must allow you to use CNAMEs for your domain. (Some registrars may call these Aliases or similar.). If your domain is currently active elsewhere, the Time-To-Live (TTL) on your domain is set to the lowest possible value in order to minimize transition time. Note: You will not be able to use a A record. Verify your DNS provider supports CNAMES. (If it does not you will want to run away from it anyway). Also you will be much happier if it supports Apex domains (more in the next chapter). 2. Test your site! Make sure your site is running and configured as you want it to be, on your master branch. In particular, see the Routes documentation . You will need your routes configured appropriately before you begin. Make sure you have turned off basic-authentication if it was turned on during development. If your production environment is on a Dedicated instance, ensure that the code is up to date in both your staging and production branches, as those are what will be mirrored to the Dedicated instances. Also ensure that the data on the production instance is up to date and ready to launch. 3. Optionally obtain a 3rd party TLS certificate Platform.sh automatically provides TLS certificates for all sites issued by Let’s Encrypt at no charge. In most cases this is sufficient and no further action is necessary. However, if you want to use a 3rd party TLS certificate to encrypt your production site you can obtain one from any number of 3rd party TLS issuers. Platform.sh does not charge for using a 3rd party TLS certificate, although the issuer may. Platform.sh supports all kinds of certificates including domain-validated certificates, extended validation (EV) certificates, high-assurance certificates and wildcard certificates. The use of HA or EV certificates is the main reason why you may wish to use a third party issuer rather than the default certificate. You will also need a custom certificate if you use wildcard routes, as Let’s Encrypt does not support wildcard certificates. If you do wish to use a 3rd party certificate, ensure it is purchased and active prior to going live. 4. Optionally configure your CDN If you are using a CDN, either one included with an Enterprise plan or one you provide for a self-service Grid project, ensure that your CDN account is registered and configured in advance. That includes setting the upstream on your CDN to point to the Platform.sh production instance. For a Grid-based project, that will be the master-xxxx domain. Run platform environment:info edge_hostname to get the domain name to use. For a Dedicated project, the upstream to use will be provided by your Platform.sh onboarding representative. Consult your CDN’s documentation for how to set the CDN’s upstream address. For Enterprise plans you may need to obtain a DNS TXT record from your Platform.sh support representative by opening a ticket. Consult the documentation for your CDN provider and our own CDN guide . Domain name is registered? Your DNS TTL is set as low as possible? Your code and data is tested and ready to launch on the master (Grid) or production (Dedicated) branch? Your custom TLS certificate is purchased, if you’re using one? Your CDN is configured to serve from Platform.sh, if you’re using one? Time to Go Live .",
        "section": "Going live",
        "subsections": " 1. Register a domain name with a supported provider 2. Test your site! 3. Optionally obtain a 3rd party TLS certificate 4. Optionally configure your CDN  ",
        "image": "",
        "url": "/golive/checklist.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "0dfa0071f415db0ea243e26092caab66",
        "title": "HTTP caching",
        "description": "",
        "text": " There are several different “levels” at which you could configure HTTP caching for your site on Platform.sh. Which one you want to use depends on your specific use case. You should use only one of these at a time and disable any others. Mixing them together will most likely result in stale and unclearable caches. The Platform.sh Router cache . Every project includes a router instance, which includes optional HTTP caching. It is reasonably configurable and obeys HTTP cache directives, but does not support push-based clearing. If you are uncertain what caching tool to use, start with this one. It is more than sufficient for the majority of use cases. A Content Delivery Network (CDN). Platform.sh is compatible with most commercial CDNs. If your Platform.sh Enterprise project has a Dedicated production environment it will typically come with the Fastly CDN . A CDN will generally offer the best performance as it is the only option that includes multiple geographic locations, but it also tends to be the most expensive. Functionality will vary widely depending on the CDN. Setup instructions for Fastly and Cloudflare are available, and will be similar for most other CDNs. Varnish . Platform.sh offers a Varnish service that you can declare as part of your application and insert between the router and your application. Performance will be roughly comparable to the Router cache. Varnish is more configurable than the Router cache as you are able to customize your VCL file, but make sure you are comfortable with Varnish configuration. Platform.sh does not provide assistance with VCL configuration and a misconfiguration may cause difficult to debug behavior. Generally speaking, you should use Varnish only if your application requires push-based clearing or relies on Varnish-specific business logic. Application-specific caching. Many web applications and frameworks include a built-in web cache layer that mimics what Varnish or the Router cache would do. Most of the time they will be slower than a dedicated caching service as they still require invoking the application server, and only serve as a fallback for users that do not have a dedicated caching service available. Generally speaking the only reason to use an application-specific web cache is if it includes some application-specific business logic that you depend on, such as application-sensitive selective cache clearing or partial page caching. Note that this refers only to HTTP level caching. Many applications have an internal application cache for data objects or similar. That should remain active regardless of the HTTP cache in use. Cookies and caching HTTP-based caching systems generally default to including cookie values in cache keys so as to avoid serving authenticated content to the wrong user. While a safe default, it also has the side effect that any cookie will effectively disable the cache, including mundane cookies like analytics. The solution is to whitelist the cookies that should impact the cache and include only the application session cookie(s). For the Router cache see our documentation . For other cache systems consult their documentation.",
        "section": "Best practices",
        "subsections": " Cookies and caching  ",
        "image": "",
        "url": "/bestpractices/http-caching.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "e45bed09f02f1fe3cf547fdc980076dd",
        "title": "HTTPS",
        "description": "",
        "text": " Let’s Encrypt All environments on Platform.sh support both HTTP and HTTPS automatically. Production SSL certificates are provided by Let’s Encrypt . You may alternatively provide your own SSL certificate from a 3rd party issuer of your choice at no charge from us. Note: Let’s Encrypt certificate renewals are attempted each time your environment is deployed. If your project does not receive regular code commits, you will need to manually issue a re-deployment to ensure the certificate remains valid. We suggest that you do so when your project doesn’t receive any updates for over 1 month. This can be done by pushing a code change via git or issuing the following command from your local environment: platform redeploy Alternatively, see the section below on automatically redeploying the site in order to renew the certificate. Platform.sh recommends using HTTPS requests for all sites exclusively. Doing so provides better security, access to certain features that web browsers only permit over HTTPS, and access to HTTP/2 connections on all sites which can greatly improve performance. How HTTPS redirection is handled depends on the routes you have defined. Platform.sh recommends specifying all HTTPS routes in your routes.yaml file. That will result in all pages being served over SSL, and any requests for an HTTP URL will automatically be redirected to HTTPS.  https://{default}/ :type:upstreamupstream: app:http  https://www.{default}/ :type:redirectto: https://{default}/ Specifying only HTTP routes will result in duplicate HTTPS routes being created automatically, allowing the site to be served from both HTTP and HTTPS without redirects. Although Platform.sh does not recommend it, you can also redirect HTTPS requests to HTTP explicitly to serve the site over HTTP only. The use cases for this configuration are few.  http://{default}/ :type:upstreamupstream: app:http  http://www.{default}/ :type:redirectto: http://{default}/  https://{default}/ :type:redirectto: http://{default}/  https://www.{default}/ :type:redirectto: http://{default}/ Of course, more complex routing logic is possible if the situation calls for it. However, we recommend defining HTTPS routes exclusively. TLS configuration Optionally, it’s possible to further refine how secure TLS connections are handled on your cluster via the tls route property. https://{default}/:type:upstreamupstream:app:httptls:# ...min_version Note: This directive was put into place when Platform.sh supported older versions of TLS for customers. Currently only TLS v1.2 is supported. Support for TLS v1.3 will be added in the near future. Setting a minimum version of TLS will cause the server to automatically reject any connections using an older version of TLS. Rejecting older versions with known security vulnerabilities is necessary for some security compliance processes. tls:min_version:TLSv1.2The above configuration will result in requests using older TLS versions to be rejected. Legal values are TLSv1.2. Note that if multiple routes for the same domain have different min_versions specified, the highest specified will be used for the whole domain. strict_transport_security HTTP Strict Transport Security (HSTS) is a mechanism for telling browsers to use HTTPS exclusively with a particular website. You can toggle it on for your site at the router level without having to touch your application, and configure it’s behavior from routes.yaml. tls:strict_transport_security:enabled:trueinclude_subdomains:truepreload:trueThere are three sub-properties for the strict_transport_security property: enabled: Can be true, false, or null. Defaults to null. If false, the other properties wil be ignored. include_subdomains: Can be true or false. Defaults to false. If true, browsers will be instructed to apply HSTS restrictions to all subdomains as well. preload: Can be true or false. Defaults to false. If true, Google and others may add your site to a lookup reference of sites that should only ever be connected to over HTTPS. Many although not all browsers will consult this list before connecting to a site over HTTP and switch to HTTPS if instructed. Although not part of the HSTS specification it is supported by most browsers. If enabled, the Strict-Transport-Security header will always be sent with a lifetime of 1 year. The Mozilla Developer Network has more detailed information on HSTS. Note: If multiple routes for the same domain specify different HSTS settings, the entire domain will still use a shared configuration. Specifically, if any route on the domain has strict_transport_security.enabled set to false, HSTS will be disabled for the whole domain. Otherwise, it will be enabled for the whole domain if at least one such route has enabled set to true. As this logic may be tricky to configure correctly we strongly recommend picking a single configuration for the whole domain and adding it on only a single route. Client authenticated TLS In some non-browser applications (such as mobile applications, IoT devices, or other restricted-client-list use cases), it is beneficial to restrict access to selected devices using TLS. This process is known as client-authenticated TLS, and functions effectively as a more secure alternative to HTTP Basic Auth. By default, any valid SSL cert issued by one of the common certificate issuing authorities will be accepted. Alternatively, you can restrict access to SSL certs issued by just those certificate authorities you specify, including a custom authority. (The latter is generally only applicable if you are building a mass-market IoT device or similar.) To do so, set client_authentication required and then provide a list of the certificates of the certificate authorities you wish to allow. tls:client_authentication: require client_certificate_authorities:- !includetype:stringpath:root-ca1.crt- !includetype:stringpath:root-ca2.crtIn this case, the certificate files are resolved relative to the .platform directory. Alternatively, the certificates can be specified inline in the file: tls:client_authentication: require client_certificate_authorities:- | -----BEGIN CERTIFICATE-----### Several lines of random characters here ###-----ENDCERTIFICATE----- - |-----BEGINCERTIFICATE----- ### Several lines of different random characters here ###-----ENDCERTIFICATE-----Automated SSL certificate renewal using Cron If the Let’s Encrypt certificate is due to expire in less than one month then it will be renewed automatically during a deployment. That makes it feasible to set up regular auto-renewal of the Let’s Encrypt certificate. The caveat is that, like any deploy, there is a very brief downtime (a few seconds, usually) so it’s best to do during off-hours. You will first need to install the CLI in your application container. See the section on API tokens for instructions on how to do so. Note: Automated SSL certificate renewal using cron requires you to get an API token and install the CLI in your application container. Once the CLI is installed in your application container and an API token configured you can add a cron task to run twice a month to trigger a redeploy. For example: crons:renewcert:# Force a redeploy at 10 am (UTC) on the 1st and 15th of every month.spec:\u0026#39;0 10 1,15 * *\u0026#39;cmd:| if [  $PLATFORM_BRANCH  = master ]; thenplatformredeploy--yes--no-waitfiThe above cron task will run on the 1st and 15th of the month at 10 am (UTC), and, if the current environment is the master branch, it will run platform redeploy on the current project and environment. The --yes flag will skip any user-interaction. The --no-wait flag will cause the command to complete immediately rather than waiting for the redeploy to complete. We recommend adjusting the cron schedule to whenever is off-peak time for your site, and to random days within the month. Warning: It is very important to include the --no-wait flag. If you do not, the cron process will block waiting on the deployment to finish, but the deployment will be blocked by the running cron task. That will take your site offline until you log in and manually terminate the running cron task. You want the --no-wait flag. We’re not joking. The certificate will not renew unless it has less than one month remaining; trying twice a month is sufficient to ensure a certificate is never less than 2 weeks from expiring. As the redeploy does cause a momentary pause in service we recommend running during non-peak hours for your site. Let’s Encrypt limits and branch names You may encounter Let’s Encrypt certificates failing to provision after the build hook has completed: Provisioning certificates Validating 2 new domains E: Error provisioning the new certificate, will retry in the background. (Next refresh will be at 2020-02-13 14:29:22.860563\u0026#43;00:00.) Environment certificates W: Missing certificate for domain www.\u0026lt;PLATFORM_ENVIRONMENT\u0026gt;-\u0026lt;PLATFORM_PROJECT\u0026gt;.\u0026lt;REGION\u0026gt;.platformsh.site W: Missing certificate for domain \u0026lt;PLATFORM_ENVIRONMENT\u0026gt;-\u0026lt;PLATFORM_PROJECT\u0026gt;.\u0026lt;REGION\u0026gt;.platformsh.site One reason that this can happen has to do with the limits of Let’s Encrypt itself, which caps off at 64 characters for URLS. If your TLS certificates are not being provisioned, it’s possible that the names of your branches are too long, and the environment’s generated URL goes over that limit. At this time, generated URLs have the following pattern: \u0026lt;PLATFORM_ENVIRONMENT\u0026gt;-\u0026lt;PLATFORM_PROJECT\u0026gt;.\u0026lt;REGION\u0026gt;.platformsh.site PLATFORM_ENVIRONMENT = PLATFORM_BRANCH \u0026#43; 7 character hash PLATFORM_PROJECT = 13 characters REGION = 2-4 characters, depending on the region platformsh.site = 15 characters extra characters (. \u0026amp; -) = 4 characters This breakdown leaves you with 21-23 characters to work with naming your branches (PLATFORM_BRANCH) without going over the 64 character limit, dependent on the region. Since this pattern for generated URLs will remain similar, but could change slightly over time, it’s our recommendation to use branch names with a maximum length between 15 and 20 characters.",
        "section": "Configure routes",
        "subsections": " Let\u0026rsquo;s Encrypt TLS configuration  min_version strict_transport_security Client authenticated TLS   Automated SSL certificate renewal using Cron Let\u0026rsquo;s Encrypt limits and branch names  ",
        "image": "",
        "url": "/configuration/routes/https.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "93169d1b8f69ad2ffa3d7078ea48ab26",
        "title": "Java featured frameworks",
        "description": "",
        "text": " Hibernate Hibernate ORM is an object-relational mapping tool for the Java programming language. It provides a framework for mapping an object-oriented domain model to a relational database. Hibernate handles object-relational impedance mismatch problems by replacing direct, persistent database accesses with high-level object handling functions. Hibernate Best Practices Jakarta EE/ Eclipse MicroProfile Eclipse MicroProfile is a semi-new community dedicated to optimizing the Enterprise Java mission for microservice-based architectures. Now Enterprise Java has been standardized under the Eclipse Foundation as Jakarta EE . Jakarta EE/ Eclipse MicroProfile Best Practices Templates Apache Tomee Thorntail KumuluzEE Helidon Open Liberty Payara Payara Micro is an Open Source, lightweight Java EE (Jakarta EE) microservices deployments. Templates Payara Micro References Article Links Search Source NoSQL Source JPA Source Hello World Source Quarkus QuarkusIO , the Supersonic Subatomic Java, promises to deliver small artifacts, extremely fast boot time, and lower time-to-first-request. Templates Quarkus References Article Links Panache MongoDB Source Command Mode Application Source Hibernate Search With Elasticsearch Source PostgreSQL With Panache Source PostgreSQL with JPA Source Hello World Source Spring The Spring Framework provides a comprehensive programming and configuration model for modern Java-based enterprise applications - on any kind of deployment platform. Platform.sh is flexible, and allows you to use Spring Framework in several flavors such as Spring MVC and Spring Boot . Spring Best Practices Templates Spring Boot MySQL Spring Boot MongoDB References Article Link Intro to Spring Data MongoDB Reactive and How to Move It to the Cloud Code Introduction of Spring Webflux and How to Apply Cloud on It Code Spring Data Redis in the cloud Code Simplify your script build with Gradle code Elasticsearch vs. Solr: have both with Spring Data and Platform.sh Elasticsearch and Solr Spring MVC and MongoDB: a match made in Platform.sh heaven Code Java: Hello, World! at Platform.sh Tomcat Apache Tomcat is an open-source implementation of the Java Servlet, JavaServer Pages, Java Expression Language and WebSocket technologies. Templates Tomcat Micronaut Micronaut is a modern, JVM-based, full-stack framework for building modular, easily testable microservice and serverless applications. Templates micronaut",
        "section": "Java",
        "subsections": " Hibernate Jakarta EE/ Eclipse MicroProfile  Templates   Payara  Templates References   Quarkus  Templates References   Spring  Templates References   Tomcat  Templates   Micronaut  Templates    ",
        "image": "",
        "url": "/languages/java/frameworks.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "3ff5ef8902e64cdc39a6e4a5a4aa05a3",
        "title": "Local development",
        "description": "",
        "text": " Now that you have a project on Platform.sh, it would be helpful to run the same build process on your local machine so that you can develop and test new features before pushing them. This guide will take you through the steps of connecting remotely to your services and building your application locally. These steps assume that you have already: Signed up for a free trial account with Platform.sh. Started either a template project or pushed your own code to Platform.sh. If you have not completed these steps by now, click the links and do so before you begin. Get started!",
        "section": "Introduction",
        "subsections": "",
        "image": "",
        "url": "/gettingstarted/developing/local-development.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "f1ae5aa6b6be802f122ea30d75cf86f5",
        "title": "Name",
        "description": "",
        "text": " The name is the unique identifier of the application. Platform.sh supports multiple applications within a project, so each application must have a unique name within a project. The name may only be composed of lower case alpha-numeric characters (a-z0-9). Warning: Changing the name of your application after it has been deployed will destroy all storage volumes and result in the loss of all persistent data. This is typically a Very Bad Thing to do. It could be useful under certain circumstances in the early stages of development but you almost certainly don’t want to change it on a live project. This name is used in the .platform/routes.yaml file to define the HTTP upstream (by default php:http). For instance, if you called your application app you will need to use app:http in the upstream field. You can also use this name in multi-application relationships.",
        "section": "Configure your application",
        "subsections": "",
        "image": "",
        "url": "/configuration/app/name.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "7a8b1a9a655fb8e138742eb8038513d7",
        "title": "Open a free trial account",
        "description": "",
        "text": " The best way to understand a tool is to use it. That’s why Platform.sh offers a free one month trial. Visit the Platform.sh accounts page and fill out your information to set up your trial account. Alternatively, you can sign up using an existing GitHub, Bitbucket, or Google account. If you choose this option, you will be able to set a password for your Platform.sh account later. Back I\u0026#39;ve set up my free trial account",
        "section": "Getting started",
        "subsections": "",
        "image": "",
        "url": "/gettingstarted/gettingstarted/own-code/free-trial.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "ee34d2b27ae3be96f8d1ff03aa161cd6",
        "title": "Open a free trial account",
        "description": "",
        "text": " The best way to understand a tool is to use it. That’s why Platform.sh offers a free one month trial.Visit the Platform.sh accounts page and fill out your information to set up your trial account. Alternatively, you can sign up using an existing GitHub, Bitbucket, or Google account. If you choose this option, you will be able to set a password for your Platform.sh account later. Back I\u0026#39;ve made a free trial account",
        "section": "Getting started",
        "subsections": "",
        "image": "",
        "url": "/gettingstarted/gettingstarted/template/free-trial.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "df0211a5f4bc39c19839d83d8413b0ce",
        "title": "Platform.sh Dedicated",
        "description": "",
        "text": " Platform.sh Dedicated is a robust, redundant layer on top of Platform.sh Professional. It is well-suited for those who like the Platform.sh development experience but need more resources and redundancy for their production environment. It is available only with an Enterprise contract. Platform.sh Dedicated consists of two parts: The Development Environment and the Dedicated Cluster. The Development Environment The Development Environment is a normal Platform.sh Grid account, with all of the capabilities and workflows of Platform.sh Professional. The one difference is that the master branch will not be associated with a domain and thus will never be “production”. The Dedicated Cluster The Dedicated Cluster is a three-Virtual Machine redundant configuration provisioned by Platform.sh for each customer. Every service is replicated across all three virtual machines in a failover configuration (as opposed to sharding), allowing a site to remain up even if one of the VMs is lost entirely. The build process for your application is identical for both the Development Environment and the Dedicated Cluster. However, because the VMs are provisioned by Platform.sh, not as a container, service configuration must be done by Platform.sh’s Customer Success team. By and large the same flexibility is available but only via filing a support ticket.",
        "section": "Dedicated",
        "subsections": " The Development Environment The Dedicated Cluster  ",
        "image": "",
        "url": "/dedicated/overview.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "b9e86538ace39448a5d7ff6d29489b3b",
        "title": "Project configuration",
        "description": "",
        "text": " You can access the project-wide configuration settings by selecting the project from your list of projects, then click the Settings tab at the top of the screen. General From the first page of the project settings, General, you can update the project name, or navigate to other project settings options on the left side of the screen. Access The Access screen allows you to manage users’ access on your project. You can invite new users to your project by clicking the Add button and entering their email address, or modify permissions of existing users by clicking the Edit link when hovering over the user. Note: Currently, permissions changes that grant or revoke SSH access to an environment take effect only after the next time that environment is deployed. Selecting a user will allow you to either edit that user’s permissions or delete the user’s access to the project entirely. If you check the Project admin box, this user will be an administrator of the project and will have fulll access on all environments. If you uncheck the box, you’ll have the option of adjusting the user’s permissions on each environment. Note: The Account owner is locked and you can’t change its permissions. Domains The Domains screen allows you to manage your domains that your project will be accessible at. More information on how to setup your domain . Note: Platform.sh expects an ASCII representation of the domain here. In case you want to use an internationalized domain name you can use the conversion tool provided by Verisign to convert your IDN domain to ASCII. Certificates The Certificates screen allows you to manage your project’s TLS certificates that enable HTTPS. You can view current certificates by hovering over one on the list and clicking the View link that appears, or you can add a new certificate by clicking the Add button a the top of the page. All projects get TLS certificates provided by Let’s Encrypt automatically. In most cases no user action is required. You will only need to add certificates on this page if you are using TLS certificates provided by a third party. Deploy Key The Deploy Key page provides the SSH key that Platform.sh will use when trying to access external private Git repository during the build process. This is useful if you want to reuse some code components across multiple projects and manage those components as dependencies of your project. Variables The Variables screen allows you to define the variables that will be available project-wide - that is, in each environment. It also allows you define variables that will be available during the build process.",
        "section": "Management console",
        "subsections": " General Access Domains Certificates Deploy Key Variables  ",
        "image": "",
        "url": "/administration/web/configure-project.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "0a622b40cc128ffe7b6d8edc89168152",
        "title": "Project templates",
        "description": "",
        "text": " You can initialize your projects using any of our pre-made template repositories. You can click the Deploy on Platform.sh button to launch a new project using a template, or you can visit and clone the repository and push to an empty project you have created using the CLI or in the management console. C#/.NET Core View the C#/.NET Core documentation . ASP.NET Core ASP.NET Core This template builds the ASP.NET Core framework for Platform.sh. It includes a minimalist application skeleton for demonstration, but you are free to alter it as needed. ASP.NET Core is an open-source and cross-platform .NET framework for building modern cloud-based web applications. Services: .NET 2.2 MariaDB 10.2 Redis 5.0 View the repository on GitHub. Go View the Go documentation . Basic Go Basic Go This template provides the most basic configuration for running a custom Go project. Go is a statically typed, compiled language with an emphasis on easy concurrency and network services. Services: Go 1.14 MariaDB 10.2 View the repository on GitHub. Beego Beego This template builds the Beego framework for Platform.sh. It includes a minimalist application skeleton for demonstration, but you are free to alter it as needed. Beego is a popular web framework written in Go. Services: Go 1.14 MariaDB 10.2 View the repository on GitHub. Echo Echo This template builds the Echo framework for Platform.sh. It includes a minimalist application skeleton for demonstration, but you are free to alter it as needed. Echo is a lightweight, minimalist web framework written in Go. Services: Go 1.14 MariaDB 10.2 View the repository on GitHub. Gin Gin This template builds the Gin framework for Platform.sh. It includes a minimalist application skeleton for demonstration, but you are free to alter it as needed. Gin is a lightweight web framework written in Go that emphasizes performance. Services: Go 1.14 MariaDB 10.2 View the repository on GitHub. Hugo Hugo This template provides a basic Hugo skeleton. All files are generated at build time, so at runtime only static files need to be served. Hugo is a static site generator written in Go, using Go\u0026#39;s native template packages for formatting. Services: Go 1.14 View the repository on GitHub. Mattermost Mattermost This template builds Mattermost on Platform.sh, configuring the deployment through user-defined environment variables. Mattermost is an open-source messaging framework written in Go and React. Services: Go 1.14 PostgreSQL 12 Elasticsearch 7.2 View the repository on GitHub. Java View the Java documentation . Apache Tomcat Apache Tomcat This project provides a starter kit for Apache Tomcat hosted on Platform.sh. Apache Tomcat is an open-source implementation of the Java Servlet, JavaServer Pages, Java Expression Language and WebSocket technologies. Services: Java 8 Maven Eclipse MicroProfile Apache Tomcat View the repository on GitHub. Apache TomEE Apache TomEE This project provides a starter kit for Apache TomEE Eclipse MicroProfile projects hosted on Platform.sh. Apache TomEE is the Eclipse MicroProfile implementation that uses several Apache Project flavors such as Apache Tomcat, Apache OpenWebBeans and so on. Services: Java 8 Maven Eclipse MicroProfile Apache TomEE View the repository on GitHub. Helidon Helidon This project provides a starter kit for Helidon Eclipse MicroProfile projects hosted on Platform.sh. Helidon is a collection of Java libraries for writing microservices that run on a fast web core powered by Netty. Helidon is designed to be simple to use, with tooling and examples to get you going quickly. Since Helidon is just a collection of libraries running on a fast Netty core, there is no extra overhead or bloat. Services: Java 8 Maven Eclipse MicroProfile Helidon View the repository on GitHub. Jenkins Jenkins This project provides a starter kit for Jenkins projects hosted on Platform.sh. Jenkins is an open source automation server written in Java. Jenkins helps to automate the non-human part of the software development process, with continuous integration and facilitating technical aspects of continuous delivery. Services: Java 8 Jenkins View the repository on GitHub. Jetty Jetty Eclipse Jetty provides a Web server and javax.servlet container, plus support for HTTP/2, WebSocket, OSGi, JMX, JNDI, JAAS and many other integrations. These components are open source and available for commercial use and distribution. Eclipse Jetty is used in a wide variety of projects and products, both in development and production. Jetty can be easily embedded in devices, tools, frameworks, application servers, and clusters. Services: Java 8 Maven Eclipse Jetty View the repository on GitHub. KumuluzEE KumuluzEE This project provides a starter kit for KumuluzEE Eclipse MicroProfile projects hosted on Platform.sh. KumuluzEE is a lightweight framework for developing microservices using standard Java, Java EE / Jakarta EE technologies and migrating existing Java applications to microservices. KumuluzEE packages microservices as standalone JARs. KumuluzEE microservices are lightweight and optimized for size and start-up time. Services: Java 8 Maven Eclipse MicroProfile KumuluzEE View the repository on GitHub. Micronaut Micronaut This project provides a starter kit for Micronaut projects hosted on Platform.sh. Micronaut is a modern, JVM-based, full-stack framework for building modular, easily testable microservice and serverless applications. Services: Java 8 Maven Micronaut View the repository on GitHub. Open Liberty Open Liberty This project provides a starter kit for Open Liberty Eclipse MicroProfile projects hosted on Platform.sh. Open Liberty is a highly composable, fast to start, dynamic application server runtime environment. Services: Java 8 Maven Eclipse MicroProfile Open Liberty View the repository on GitHub. Payara Micro Payara Micro This project provides a starter kit for Payara Micro projects hosted on Platform.sh. Payara Micro is an Open Source, lightweight Java EE (Jakarta EE) microservices deployments. Services: Java 8 Maven Eclipse MicroProfile Payara Micro View the repository on GitHub. Quarkus Quarkus QuarkusIO, the Supersonic Subatomic Java, promises to deliver small artifacts, extremely fast boot time, and lower time-to-first-request. Services: Java 8 Maven Eclipse MicroProfile Quarkus View the repository on GitHub. Spring Boot, Gradle, Mysql Spring Boot, Gradle, Mysql This project provides a starter kit for Spring Boot Gradle with MySQL projects hosted on Platform.sh. The Spring Framework is an application framework and inversion of control container for the Java platform. Services: Java 8 Gradle Spring Boot Oracle MySQL 8.0 View the repository on GitHub. Spring Boot, Maven, Mysql Spring Boot, Maven, Mysql This project provides a starter kit for Spring Boot Maven with MySQL projects hosted on Platform.sh. The Spring Framework is an application framework and inversion of control container for the Java platform. Services: Java 8 Maven Spring Boot Oracle MySQL 8.0 View the repository on GitHub. Spring MVC, Maven, MongoDB Spring MVC, Maven, MongoDB This project provides a starter kit for Spring MVC Maven with MongoDB projects hosted on Platform.sh. The Spring Framework is an application framework and inversion of control container for the Java platform. Services: Java 8 Maven Spring MVC MongoDB View the repository on GitHub. Spring, Kotlin, Maven Spring, Kotlin, Maven This project provides a starter kit for Spring Boot Maven with Kotlin projects hosted on Platform.sh. The Spring Framework is an application framework and inversion of control container for the Java platform. Services: Java 8 View the repository on GitHub. Thorntail Thorntail This project provides a starter kit for Thorntail Eclipse MicroProfile projects hosted on Platform.sh. Thorntail offers an innovative approach to packaging and running Java EE applications by packaging them with just enough of the server runtime to  java -jar  your application. It\u0026#39;s MicroProfile compatible, too. Services: Java 8 Maven Eclipse MicroProfile Thorntail View the repository on GitHub. xwiki xwiki This project provides a starter kit for XWiki projects hosted on Platform.sh. XWiki is a free wiki software platform written in Java with a design emphasis on extensibility. XWiki is an enterprise wiki. It includes WYSIWYG editing, OpenDocument based document import/export, semantic annotations and tagging, and advanced permissions management. Services: Java 8 View the repository on GitHub. Lisp View the Lisp documentation . Lisp Hunchentoot Lisp Hunchentoot This template is a simple Lisp Hunchentoot web server on Platform.sh. It includes a minimalist application for demonstration, but you are free to alter it as needed. Hunchentoot is a web server written in Common Lisp and at the same time a toolkit for building dynamic websites. Services: Lisp 1.5 View the repository on GitHub. Node.js View the Node.js documentation . Express Express This template builds the Express framework for Platform.sh. It includes a minimalist application skeleton for demonstration, but you are free to alter it as needed. Express is a minimalist web framework written in Node.js. Services: Node.js 10 MariaDB 10.2 View the repository on GitHub. Gatsby Gatsby This template builds a simple application using Gatsby hosted on Platform.sh. Gatsby is a free and open source framework based on React that helps developers build blazing fast websites and apps. Services: Node.js View the repository on GitHub. Gatsby with Wordpress Gatsby with Wordpress This template builds a multi-app project using Gatsby as its frontend and a Wordpress to store content. Gatsby is a free and open source framework based on React that helps developers build statically-generated websites and apps, and WordPress is a blogging and lightweight CMS written in PHP. Services: Node.js 12 PHP 7.3 MariaDB 10.4 View the repository on GitHub. Koa Koa This template builds a Koa project on Platform.sh. Koa is a lightweight web microframework for Node.js. Services: Node.js 10 MariaDB 10.2 View the repository on GitHub. Node.js Node.js This template builds a simple application using the Node.js built-in `http` web server. It includes a minimalist application skeleton for demonstration, but you are free to alter it as needed. Node.js is an open-source JavaScript runtime built on Chrome\u0026#39;s V8 JavaScript engine. Services: Node.js 10 MariaDB 10.4 View the repository on GitHub. Probot Probot This template builds a simple GitHub App using Probot. Probot is a framework for building GitHub Apps in Node.js. Services: Node.js 12 View the repository on GitHub. strapi strapi This template builds a Strapi backend for Platform.sh. It does not include a frontend application, but you can add one of your choice and access Strapi by defining it in a relationship in your frontend\u0026#39;s .platform.app.yaml file. Strapi is a Headless CMS framework written in Node.js. Services: Node.js 12 PostgreSQL 11 View the repository on GitHub. PHP View the PHP documentation . Backdrop Backdrop This template builds a Backdrop site, with the entire site committed to Git. Backdrop is a PHP-based CMS, originally forked from Drupal 7. Services: PHP 7.3 MariaDB 10.4 View the repository on GitHub. Basic PHP Basic PHP This template provides the most basic configuration for running a custom PHP project. PHP is a high-performance scripting language especially well suited to web development. Services: PHP 7.3 MariaDB 10.2 View the repository on GitHub. Drupal 8 Drupal 8 This template builds Drupal 8 using the  Drupal Recommended  Composer project. It also includes configuration to use Redis for caching, although that must be enabled post-install in `.platform.app.yaml`. Drupal is a flexible and extensible PHP-based CMS framework. Services: PHP 7.4 MariaDB 10.4 Redis 5.0 View the repository on GitHub. Drupal 8 Multisite Drupal 8 Multisite This template builds Drupal 8 in a multisite configuration using the  Drupal Recommended  Composer project. It also includes configuration to use Redis for caching, although that must be enabled post-install per-site. Drupal is a flexible and extensible PHP-based CMS framework capable of hosting multiple sites on a single code base. Services: PHP 7.4 MariaDB 10.4 Redis 5.0 View the repository on GitHub. Drupal 9 Drupal 9 This template builds Drupal 9 Beta using the  Drupal Recommended  Composer project. It also includes configuration to use Redis for caching, although that must be enabled post-install in `.platform.app.yaml`. Drupal is a flexible and extensible PHP-based CMS framework. Services: PHP 7.4 MariaDB 10.4 Redis 5.0 View the repository on GitHub. GovCMS 8 GovCMS 8 This template builds the Australian government\u0026#39;s GovCMS Drupal 8 distribution using the Drupal Composer project for better flexibility. It also includes configuration to use Redis for caching, although that must be enabled post-install in .platform.app.yaml. GovCMS is a Drupal distribution built for the Australian government, and includes configuration optimized for managing government websites. Services: PHP 7.2 MariaDB 10.2 Redis 5.0 View the repository on GitHub. Laravel Laravel This template provides a basic Laravel skeleton. It comes pre-configured to use a MariaDB database and Redis for caching and sessions. Laravel is an opinionated, integrated rapid-application-development framework for PHP. Services: PHP 7.3 MariaDB 10.2 Redis 5.0 View the repository on GitHub. Magento 2 Community Edition Magento 2 Community Edition This template builds Magento 2 CE on Platform.sh. It includes additional scripts to customize Magento to run effectively in a build-and-deploy environment. Magento is a fully integrated ecommerce system and web store written in PHP. This is the Open Source version. Services: PHP 7.2 MariaDB 10.2 Redis 3.2 View the repository on GitHub. Mautic Mautic TThis template provides a basic Mautic installation. Mautic is an Open SOurce marketing automation tool built on Symfony. Services: PHP 7.2 MariaDB 10.4 RabbitMQ 3.7 View the repository on GitHub. Nextcloud Nextcloud This template builds Nextcloud on Platform.sh. Nextcloud is a PHP-based groupware server with installable apps, file synchronization, and federated storage. An adminstrative user will be created automatically. See the deploy log after the project is installed for the name and password. Services: PHP 7.4 MariaDB 10.4 Redis 5.0 View the repository on GitHub. Opigno Opigno This template builds the Opigno Drupal 8 distribution using the Drupal Composer project for better flexibility. It also includes configuration to use Redis for caching, although that must be enabled post-install in .platform.app.yaml. Opigno is a Learning Management system built as a Drupal distribution. Services: PHP 7.3 MariaDB 10.4 Redis 5.0 View the repository on GitHub. Pimcore Pimcore Pimcore Digital Platform for Enterprises Services: PHP 7.4 MariaDB 10.2 Redis 5.0 View the repository on GitHub. Sculpin Sculpin This template provides a basic Sculpin skeleton. All files are generated at build time, so at runtime only static files need to be served. Sculpin is a static site generator written in PHP and using the Twig templating engine. Services: PHP 7.3 View the repository on GitHub. Symfony 3 Symfony 3 This template provides a basic Symfony 3 skeleton. It is configured for Production mode by default so the usual Symfony  welcome  page will not appear. Symfony is a high-performance loosely-coupled PHP web development framework. Version 3 is the legacy support version. Services: PHP 7.2 MariaDB 10.2 View the repository on GitHub. Symfony 4 Symfony 4 This template provides a basic Symfony 4 skeleton. It is configured for Production mode by default so the usual Symfony  welcome  page will not appear. That can be adjusted in .platform.app.yaml. Symfony is a high-performance loosely-coupled PHP web development framework. Services: PHP 7.3 MariaDB 10.4 View the repository on GitHub. Symfony 5 Symfony 5 This template provides a basic Symfony 5 skeleton. It is configured for Production mode by default so the usual Symfony  welcome  page will not appear. That can be adjusted in .platform.app.yaml. Symfony is a high-performance loosely-coupled PHP web development framework. Services: PHP 7.3 MariaDB 10.4 View the repository on GitHub. TYPO3 TYPO3 This template provides a basic TYPO3 installation. TYPO3 is a PHP-based Content Management System Services: PHP 7.4 MariaDB 10.4 Redis 5.0 View the repository on GitHub. Wordpress Wordpress This template builds WordPress on Platform.sh using the johnbolch/wordpress  Composer Fork  of WordPress. Plugins and themes should be managed with Composer exclusively. WordPress is a blogging and lightweight CMS written in PHP. Services: PHP 7.3 MariaDB 10.2 View the repository on GitHub. Python View the Python documentation . Basic Python 2 Basic Python 2 This template provides the most basic configuration for running a custom Python 2.7 project. It launches a Python application directly rather than using a runner. Python is a general purpose scripting language often used in web development. Services: Python 2 MariaDB 10.2 Redis 5 View the repository on GitHub. Basic Python 3 Basic Python 3 This template provides the most basic configuration for running a custom Python 3.7 project. It launches a Python application directly rather than using a runner. Python is a general purpose scripting language often used in web development. Services: Python 3 MariaDB 10.2 Redis 5 View the repository on GitHub. Django 1 Django 1 This template builds Django 1 on Platform.sh, using the gunicorn application runner. New projects should be built using Django 2, but this project is a reference for existing migrating sites. Django is a Python-based web application framework with a built-in ORM. Version 1 is the legacy support version. Services: Python 2 PostgreSQL 10 View the repository on GitHub. Django 2 Django 2 This template builds Django 2 on Platform.sh, using the gunicorn application runner. Django is a Python-based web application framework with a built-in ORM. Services: Python 3.7 PostgreSQL 10 View the repository on GitHub. Django 3 Django 3 This template builds Django 3 on Platform.sh, using the gunicorn application runner. Django is a Python-based web application framework with a built-in ORM. Services: Python 3.7 PostgreSQL 10 View the repository on GitHub. Flask Flask This template builds a Flask project on Platform.sh, run natively without a separate runner. Flask is a lightweight web microframework for Python. Services: Python 3.7 MariaDB 10.2 Redis 5.0 View the repository on GitHub. MoinMoin MoinMoin This template builds a Moin Moin wiki on Platform.sh. The project doesn\u0026#39;t include Moin Moin itself; rather, it includes build and deploy scripts that will download Moin Moin on the fly. Moin Moin is a Python-based Wiki system that uses flat files on disk for storage. Services: Python 2 View the repository on GitHub. Pelican Pelican This template provides a basic Pelican skeleton. All files are generated at build time, so at runtime only static files need to be served. Pelican is a static site generator written in Python and using Jinja for templating. Services: Python 3.7 View the repository on GitHub. Pyramid Pyramid This template builds Pyramid on Platform.sh. It includes some basic example code to demonstrate how to connect to the database. Pyramid is a web framework written in Python. Services: Python 3.7 View the repository on GitHub. Python 3 running UWSGI Python 3 running UWSGI This template provides the most basic configuration for running a custom Python 3.7 project. It launches the application using the UWSGI application runner. Python is a general purpose scripting language often used in web development. Services: Python 3.7 MariaDB 10.2 Redis 5.0 View the repository on GitHub. Wagtail Wagtail This template builds the Wagtail CMS on Platform.sh, using the gunicorn application runner. Wagtail is a web CMS built using the Django framework for Python. Services: Python 3.7 PostgreSQL 10 View the repository on GitHub. Ruby View the Ruby documentation . Ruby on Rails Ruby on Rails This template builds Ruby on Rails 5 on Platform.sh. It includes a bridge library that will auto-configure most databases and services. Rails is an opinionated rapid application development framework written in Ruby. Services: Ruby 2.6 Postgresql 11 View the repository on GitHub.",
        "section": "Development",
        "subsections": " C#/.NET Core Go Java Lisp Node.js PHP Python Ruby  ",
        "image": "",
        "url": "/development/templates.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "ff6cf53b92a5848537f00d89a719d8e9",
        "title": "Sponsored community sites",
        "description": "",
        "text": " Platform.sh provides sponsored hosting for Free Software projects and tech community events and organizations as part of our effort to support the Free Software community. Eligibility A non-trivial software project released under a Free and Open Source license (*GPL, BSD, MIT, Apache, etc.) A community-oriented conference or similar event run either by the community or a related non-profit corporation. For-profit events and organizations are not eligible. An offline IT community organization, such as a local user group for a Free and Open Source project. An online non-profit community project that supports one of the above. The Platform.sh Developer Relations team manages the program and collectively has final say on what projects are eligible for sponsorship. Platform.sh will periodically reevaluate sponsored sites to ensure they remain eligible, and may terminate the free service at its own will. Platform.sh will provide a month notice to allow users to migrate in these cases. Offering An eligible project is entitled to one (1) Standard instance, with the standard 3 environments and 5 GB of storage. Any number of users may be added as needed for the project at no additional cost. The site is to be used exclusively for content supporting and promoting the project. (Brochureware site, documentation, user forums, etc. are all acceptable.) There is no limit on the number of domains or application containers (subject to resource limits of the Standard Plan) that a project may use, although the project is expected to register and manage its own domain(s). Platform.sh will provision SSL certificates for free (as may be limited by the Let’s Encrypt API) but the hosted site may provide their own. A project is under no obligation to use the application container of their project. (Eg, a PHP project is free to use Jekyll to produce a static site using Platform.sh, a Python project is free to host a PHPBB site, etc.) Expectations In return, eligible projects are expected to include an HTML snippet conspicuously on each page of the site. That may be in a sidebar or footer, with text no smaller than the standard body text size for the site. The image may be scaled via CSS as appropriate as long as the text is fully readable. The badge markup will be provided by Platform.sh when setting up the site and will include a link to Platform.sh and an analytics tracking value so we know where links are coming from. It does not include any cookies. Platform.sh may from time to time update the widget’s contents and the sponsored site will update the contents in a timely fashion. The sponsored site may ask, and Platform.sh shall accept variations of the widget as long as these fall within the branding guidelines of Platform.sh. Platform.sh may announce on its own media presence both online and offline that it hosts the said project, and kindly ask the sponsored site communicate at least once over its usual social media presence (Twitter, Mailing list, etc) the sponsored hosting. If the eligible project is a registered non-profit organization in the US or Germany, it is also expected to provide paperwork to register an in-kind donation to the organization of the equivalent price of the service ($50 USD/month or the EUR equivalent), for tax purposes. Sponsored projects will receive the same level of support as any other self-service hosted customer site. Contact If you are an eligible Free Software project or community group, contact our Developer Relations team and let us know that you are interested.",
        "section": "Pricing",
        "subsections": " Eligibility Offering Expectations Contact  ",
        "image": "",
        "url": "/overview/pricing/sponsored.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "28b7c8d84715f165ec4f9784b859ebf6",
        "title": "Start with a template",
        "description": "",
        "text": " Welcome to Platform.sh! Getting started is as easy as opening a free trial account and initializing a template project . There are no requirements on your part at this point. This guide will take you from zero to hero - from first glance to a deployed application entirely from your browser. Get started!",
        "section": "Introduction",
        "subsections": "",
        "image": "",
        "url": "/gettingstarted/gettingstarted/template.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "f9eddd35ac682933bc7570fd91fc54c4",
        "title": "Structure",
        "description": "",
        "text": " Every application you deploy on Platform.sh is built as a virtual cluster, containing a set of containers. The master branch of your Git repository is always deployed as the production cluster. Any other branch can be deployed as a development cluster. By default, you can have up to three live development clusters at once, but you can buy more on a per-project basis. There are three types of containers within your cluster: one Router one or more Application containers zero or more Service containers All of those containers are managed by three special files in your Git repository: .platform/routes.yaml .platform/services.yaml .platform.app.yaml In most cases, that means your repository will look like this: yourproject/ .git/ .platform/ services.yaml routes.yaml .platform.app.yaml \u0026lt;your application files\u0026gt; Router There is always exactly one Router per cluster. The Router of a cluster is a single nginx process. It is configured by the routes.yaml file. It maps incoming requests to the appropriate Application container and provides basic caching of responses, if so configured. It has no persistent storage. Service Service containers are configured by the services.yaml file. There may be zero or more Service containers in a cluster, depending on the services.yaml file. The code for a Service is provided by Platform.sh in a pre-built container image, along with a default configuration. Depending on the service it may also include user-provided configuration in the services.yaml file. Examples of services include MySQL/MariaDB, Elasticsearch, Redis, and RabbitMQ. Application There always must be one Application container in a cluster, but there may be more. Each Application container corresponds to a .platform.app.yaml file in the repository. If there are 3 .platform.app.yaml files, there will be three Application containers. Application containers hold the code you provide via your Git repository. Application containers are always built off of one of the Platform.sh-provided language-specific images, such as “PHP 5.6”, “PHP 7.2”, or “Python 3.7”. It is also possible to have multiple Application containers running different languages or versions. For typical applications, there is only one .platform.app.yaml file, which is generally placed at the repository root.",
        "section": "The big picture",
        "subsections": " Router Service Application  ",
        "image": "",
        "url": "/overview/structure.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "b6ba5f5c2756291ed287b05c7efe0395",
        "title": "Symfony Frequently Asked Questions (FAQ)",
        "description": "",
        "text": " How do I store my session files? If you get the following error: failed: Read-only file system (30) in /app/app/cache/dev/classes.php line 420 that’s because Symfony is trying to write into: /var/lib/php5/ which is read-only. The solution is to mount a sessions folder into Platform.sh and write sessions in that folder. Simply edit your .platform.app.yaml and add a mounts there: mounts:... app/sessions :source:localsource_path:sessions...Then, add this line at the top of your app_dev.php: ini_set(\u0026#39;session.save_path\u0026#39;, __DIR__.\u0026#39;/../app/sessions\u0026#39; ); Why does my newly cloned Symfony install throw errors? You may encounter the WSOD (white screen of death) when you first clone a new Symfony project from your platform. This is likely because of missing dependencies. You will need to install composer first and then run the following command: cd my_project_name/ composer install Why do I get ‘Permission denied’ in a deploy hook? If you get the following error during a deploy hook: Launching hook \u0026#39;app/console cache:clear\u0026#39;. /bin/dash: 1: app/console: Permission denied This means that you might have committed the executable file (in this case app/console) without the execute bit set. Run this to fix the problem: chmod a\u0026#43;x app/console git add app/console git commit -m  Fix the console script execute permission. ",
        "section": "Symfony - Getting started",
        "subsections": " How do I store my session files? Why does my newly cloned Symfony install throw errors? Why do I get \u0026lsquo;Permission denied\u0026rsquo; in a deploy hook?  ",
        "image": "",
        "url": "/frameworks/symfony/faq.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "8eef2783c62c78fd72d3592be2ce5fe6",
        "title": "Tethered",
        "description": "",
        "text": " Tethered Local The simplest way to run a project locally is to use a local web server, but keep all other services on Platform.sh and connect to them over an SSH tunnel. This approach requires very little setup, but depending on the speed of your connection and how I/O intensive your application is may not be performant enough to use regularly. It will also require an active Internet connection, of course. Quick Start In your application directory run platform tunnel:open \u0026amp;\u0026amp; export PLATFORM_RELATIONSHIPS= $(platform tunnel:info --encode) . This will open an SSH tunnel to your current Platform.sh environment and expose a local environment variable that mimics the relationships array on Platform.sh. You can now run your application locally (for example by running php -d variables_order=EGPCS -S localhost:8001 for PHP), assuming it is configured to read its configuration from the Platform.sh environment variables. Note that other Platform.sh environment configuration such as the routes or application secret value will still not be available. Also be aware that the environment variable exists only in your current shell. If you are starting multiple local command shells you will need to rerun the export command above in each of them. Local web server For the local web server the approach will vary depending on your language. For a self-serving language (Go or Node.js), simply run the program locally. For PHP, you may install your own copy of Nginx (or Apache) and PHP-FPM, or simply use the built-in PHP web server. Be aware however that by default the PHP web server will ignore environment variables by default. You will need to explicitly instruct it to read them, like so: php -S -d variables_order=EGPCS localhost:8001. That will start a basic web server capable of running PHP, serving the current directory, on port 8001, using available environment variables. See the PHP manual for more information. For other languages it is recommended that you install your own copy of Nginx or Apache. A virtual machine or Docker image is also a viable option. SSH tunneling Now that the code is running, it needs to connect it to its services. For that, open an SSH tunnel to the current project. $ platform tunnel:open SSH tunnel opened on port 30000 to relationship: redis SSH tunnel opened on port 30001 to relationship: database Logs are written to: ~/.platformsh/tunnels.log List tunnels with: platform tunnels View tunnel details with: platform tunnel:info Close tunnels with: platform tunnel:close Now you can connect to the remote database normally, as if it were local. $ mysql --host=127.0.0.1 --port=30001 --user=\u0026#39;user\u0026#39; --password=\u0026#39;\u0026#39; --database=\u0026#39;main\u0026#39; The specific port that each service uses is not guaranteed, but is unlikely to change unless you add an additional service or connect to multiple projects at once. In most cases it’s safe to add a local-configuration file for your application that connects to, in this case, localhost:30001 for the SQL database and localhost:30000 for Redis. After the tunnel(s) are opened, you can confirm their presence: platform tunnel:list You can show more information about the open tunnel(s) with: platform tunnel:info and you can close tunnels with: platform tunnel:close Note: The platform tunnel:open command requires the pcntl and posix PHP extensions. Run php -m | grep -E \u0026#39;posix|pcntl\u0026#39; to check if they’re there. If you don’t have these extensions installed, you can use the platform tunnel:single command to open one tunnel at a time. This command also lets you specify a local port number. Local environment variables Alternatively, you can read the relationship information directly from Platform.sh and expose it locally in the same form. From the command line, run: export PLATFORM_RELATIONSHIPS= $(platform tunnel:info --encode)  That will create a PLATFORM_RELATIONSHIPS environment variable locally that looks exactly the same as the one you’d see on Platform.sh, but pointing to the locally mapped SSH tunnels. Whatever code you have that looks for and decodes the relationship information from that variable (which is what runs on Platform.sh) will detect it and use it just as if you were running on Platform.sh. Note that the environment variable is set globally so you cannot use this mechanism to load mutiple tethered Platform.sh projects at the same time. If you need to run multiple tethered environments at once you will have to read the relationships information for each one from the application code, like so: \u0026lt;?php if ($relationships_encoded = shell_exec(\u0026#39;platform tunnel:info --encode\u0026#39;)) { $relationships = json_decode(base64_decode($relationships_encoded, TRUE), TRUE); // ... } {%- language name= Python , type= py  -%} import json import base64 import subprocess encoded = subprocess.check_output([\u0026#39;platform\u0026#39;, \u0026#39;tunnel:info\u0026#39;, \u0026#39;--encode\u0026#39;]) if (encoded): json.loads(base64.b64decode(relationships).decode(\u0026#39;utf-8\u0026#39;)) # ...",
        "section": "Set up your local development environment",
        "subsections": " Quick Start Local web server SSH tunneling  Local environment variables    ",
        "image": "",
        "url": "/development/local/tethered.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "5d3c9dd44a61cd3d93654846f6ba5431",
        "title": "TYPO3 Frequently Asked Questions (FAQ)",
        "description": "",
        "text": " Why are there warnings in the install tool? The TYPO3 install tool doesn’t yet fully understand when you are working on a cloud envirionment and may warn you that some folders are not writable. Don’t worry, your TYPO3 installation will be fully functional. How do I add extensions? TYPO3 extension can easily be added using composer. Just use the platform.sh CLI tool to run composer, as follows: platform get \u0026lt;project id\u0026gt; -e \u0026lt;branch name\u0026gt; composer require typo3-ter/[extension name] git add composer.* git commit git push",
        "section": "TYPO3 - Getting started",
        "subsections": " Why are there warnings in the install tool? How do I add extensions?  ",
        "image": "",
        "url": "/frameworks/typo3/faq.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "9ade93b8dcb9b5af7838d604e9a5262a",
        "title": "User administration",
        "description": "",
        "text": " Every Platform.sh user has a role which controls access and improves security on your project. Different roles are authorized to do different things with your applications, environments and users. You can use your collection of Roles to manage how users interact with Platform.sh. User roles At the project level: Project Administrator - A project administrator can change settings and execute actions on any environment. Project Viewer - A project reader can view all environments within a project but cannot execute any actions on them. A Project Reader can have a specific role on different environments. At the environment level: Environment Administrator - An environment administrator can change settings and execute actions on this environment. Environment Contributor - An environment contributor can push code to this environment and branch the environment. Environment Viewer - An environment reader can only view this environment. Important: After a user is added to (or deleted from) an environment, it will be automatically redeployed, after which the new permissions will be fully updated. When adding users at the project level, however, redeployments do not occur automatically, and you will need to trigger a redeployments to update those settings for each environment using the CLI command platform redeploy. Otherwise, user access will not be updated on those environments until after the next build and deploy commit. When a development team works on a project, the team leader can be the project administrator and decide which roles to give his team members. One team member can contribute to one environment, another member can administer a different environment and the customer can be a reader of the master environment. If you want your users to be able to see everything (Reader), but only commit to a specific branch, change their permission level on that environment to “Contributor”. SSH Access Control: An environment contributor can push code to the environment and has SSH access to the environment. You can change this by specifying user types with SSH access. Note: The project owner - the person licensed to use Platform.sh - doesn’t have special powers. A project owner usually has a project administrator role. Manage user permissions at the project level From your list of projects, select the project where you want to view or edit user permissions. At this point, you will not have selected a particular environment. Click the Settings tab at the top of the page, then click the Access tab on the left to show the project-level users and their roles. The Access tab shows project-level users and their roles. Selecting a user will allow you either to edit that user’s permissions or delete the user’s access to the project entirely. Add a new user by clicking on the Add button. If you select the “Viewer” role for the user, you’ll have the option of adjusting the user’s permissions at the environment level. From this view, you can assign the user’s access. Selecting them to become a “Project admin” will give them “Admin” access to every environment in the project. Alternatively, you can give the user “Admin”, “Viewer”, “Contributor”, or “No Access” to each environment separately. If you select the “Viewer” role for the user, you’ll have the option of adjusting the user’s permissions at the environment level. Once this has been done, if the user does not have a Platform.sh account, they will receive an email asking to confirm their details and register an account name and a password. In order to push and pull code (or to SSH to one of the project’s environments) the user will need to add an SSH key. If the user already has an account, they will receive an email with a link to the project. Manage user permissions at the environment level From within a project select an environment from the ENVIRONMENT pull-down menu. Click the Settings tab at the top of the screen and then click the Access tab on the left hand side. The Access tab shows environment-level users and their roles. Selecting a user will allow you either to edit that user’s permissions or delete the user’s access to the environment entirely. Add a new user by clicking on the Add button. Note: Remember the user will only be able to access the environment once it has been rebuilt (after a git push). Manage users with the CLI You can user the Platform.sh command line client to fully manage your users and integrate this with any other automated system. Available commands: user:add Add a user to the project user:delete Delete a user user:list (users) List project users user:role View or change a user’s role For example, the following command would add the ‘admin’ role to alice@example.com in the current project. platform user:add This will present you with an interactive wizard that will allow you to choose precisely what rights you want to give the new user. $ platform user:add Email address: alice@example.com The user\u0026#39;s project role can be \u0026#39;viewer\u0026#39; (\u0026#39;v\u0026#39;) or \u0026#39;admin\u0026#39; (\u0026#39;a\u0026#39;). Project role [V/a]: The user\u0026#39;s environment-level roles can be \u0026#39;viewer\u0026#39;, \u0026#39;contributor\u0026#39;, or \u0026#39;admin\u0026#39;. development environment role [V/c/a]: sprint1 environment role [V/c/a]: hot-fix environment role [V/c/a]: master environment role [V/c/a]: pr-2 environment role [V/c/a]: pr-3 environment role [V/c/a]: Summary: Email address: alice@example.com Project role: viewer development: viewer sprint1: viewer hot-fix: viewer pr-2: viewer pr-3: viewer Adding users can result in additional charges. Are you sure you want to add this user? [Y/n] User alice@example.com created Once this has been done, the user will receive an email asking her to confirm her details and register an account name and a password. To give Alice the ‘contributor’ role on the environment ‘development’, you could run: platform user:role alice@example.com --level environment --environment development --role contributor Use platform list to get the full list of commands. Transfer ownership If you want to transfer ownership of a project to a different user, first invite that user as a project administrator and then submit a support ticket from the current project owner to ask for the transfer. This action will automatically transfer the subscription charges to the new owner.",
        "section": "Administration",
        "subsections": " User roles Manage user permissions at the project level Manage user permissions at the environment level Manage users with the CLI Transfer ownership  ",
        "image": "",
        "url": "/administration/users.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "e72ac4ff37210cb6b603c3ebfed4df5f",
        "title": "Using Redis with WordPress",
        "description": "",
        "text": " There are a number of Redis libraries for WordPress, only some of which are compatible with Platform.sh. We have tested and recommend devgeniem/wp-redis-object-cache-dropin , which requires extremely little configuration. Requirements Add a Redis service First you need to create a Redis service. In your .platform/services.yaml file, add the following: rediscache:type:redis:5.0That will create a service named rediscache, of type redis, specifically version 5.0. Expose the Redis service to your application In your .platform.app.yaml file, we now need to open a connection to the new Redis service. Under the relationships section, add the following: relationships:redis: rediscache:redis The key (left side) is the name that will be exposed to the application in the PLATFORM_RELATIONSHIPS variable . The right hand side is the name of the service we specified above (rediscache) and the endpoint (redis). If you named the service something different above, change rediscache to that. Add the Redis PHP extension Because the Redis extension for PHP has been known to have BC breaks at times, we do not bundle a specific verison by default. Instead, we provide a script to allow you to build your desired version in the build hook. See the PHP-Redis page for a simple-to-install script and instructions. Add the Redis library If using Composer to build WordPress, you can install the WP-Redis library with the following Composer command: composer require devgeniem/wp-redis-object-cache-dropin Then commit the resulting changes to your composer.json and composer.lock files. Configuration To enable the WP-Redis cache the object-cache.php file needs to be copied from the downloaded package to the wp-content directory. Add the following line to the bottom of your build hook: cp -r wp-content/wp-redis-object-cache-dropin/object-cache.php web/wp/wp-content/object-cache.php It should now look something like: hooks:build:| set -ebashinstall-redis.sh5.1.1cp-rwp-content/wp-redis-object-cache-dropin/object-cache.phpweb/wp/wp-content/object-cache.phpNext, place the following code in the wp-config.php file, somewhere before the final require_once(ABSPATH . \u0026#39;wp-settings.php\u0026#39;); line. \u0026lt;?php if (!empty($_ENV[\u0026#39;PLATFORM_RELATIONSHIPS\u0026#39;]) \u0026amp;\u0026amp; extension_loaded(\u0026#39;redis\u0026#39;)) { $relationships = json_decode(base64_decode($_ENV[\u0026#39;PLATFORM_RELATIONSHIPS\u0026#39;]), true); $relationship_name = \u0026#39;redis\u0026#39;; if (!empty($relationships[$relationship_name][0])) { $redis = $relationships[$relationship_name][0]; define(\u0026#39;WP_REDIS_CLIENT\u0026#39;, \u0026#39;pecl\u0026#39;); define(\u0026#39;WP_REDIS_HOST\u0026#39;, $redis[\u0026#39;host\u0026#39;]); define(\u0026#39;WP_REDIS_PORT\u0026#39;, $redis[\u0026#39;port\u0026#39;]); } } That will define 3 constants that the WP-Redis extension will look for in order to connect to the Redis server. If you used a different name for the relationship above, change $relationship_name accordingly. This code will have no impact when run on a local development environment. That’s it. There is no Plugin to enable through the WordPress administrative interface. Commit the above changes and push. Verifying Redis is running Run this command in a SSH session in your environment redis-cli -h redis.internal info. You should run it before you push all this new code to your repository. This should give you a baseline of activity on your Redis installation. There should be very little memory allocated to the Redis cache. After you push this code, you should run the command and notice that allocated memory will start jumping.",
        "section": "Wordpress",
        "subsections": " Requirements  Add a Redis service Expose the Redis service to your application Add the Redis PHP extension Add the Redis library   Configuration  Verifying Redis is running    ",
        "image": "",
        "url": "/frameworks/wordpress/redis.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "f8f002328663e62ad4ef9f18391abdbb",
        "title": "YAML",
        "description": "",
        "text": " YAML (“YAML Ain’t Markup Language”) is a human-readable data file format, well suited to human-edited configuration files. Nearly all aspects of your project’s build and deploy pipeline are controlled via YAML files. YAML is a whitespace-sensitive format that is especially good at key/value type configuration, such as that used by Platform.sh. There are many good YAML tutorials online, and the format is reasonably self-documenting. We especially recommend: GravCMS’s YAML tutorial Learn YAML in Y Minutes The following is only a cursory look at YAML itself. The tutorials above will provide a more in-depth introduction. Basic YAML A YAML file is a text file that ends in .yaml. (Some systems use an alternative .yml extension, but Platform.sh uses the four-letter extension.) It consists primarily of key value pairs, and supports nesting. For example: name:\u0026#39;app\u0026#39;type:\u0026#39;php:7.4\u0026#39;build:flavor:\u0026#39;composer\u0026#39;disk:1024This example defines a key name with value app, a key type with value php:7.1, a key disk with a value 1024, and a key build that is itself a nested set of key/value pairs, of which there is only one: flavor, whose value is composer. Informally, nested values are often referenced using a dotted syntax, such as build.flavor, and that format is used in this documentation in various places. Keys are always strings, and may be quoted or not. Values may be strings, numbers, booleans, or further nested key/value pairs. Alphanumeric strings may be quoted or not. More complex strings (with punctuation, etc.) must be quoted. Numbers should not be quoted. The boolean values true and false should never be quoted. For quoted values, both single quotes (\u0026#39;) and double quotes ( ) are valid. Double quotes, however, will interpolate common escape characters such as and so forth. For that reason using single quotes is generally recommended unless you want escape characters to be processed rather than taken literally. In general the order of keys in a YAML file does not matter. Neither do blank lines. Indentation may be with any number of spaces, as long as it is consistent throughout the file. Platform.sh examples by convention use four-space indentation. Multi-line strings In case of long, multi-line strings, the | character tells the YAML parser that the following, indented lines are all part of the same string. That is, this: hooks:build:| set -ecpa.txtb.txtcreates a nested property hooks.build, which has the value set a.txt b.txt. (That is, a string with a line break in it.) That is useful primarily for hooks, which allow the user to enter small shell scripts within the YAML file. Includes YAML allows for special “tags” on values that change their meaning. These tags may be customized for individual applications so may vary from one system to another. The main Platform.sh “local tag” is !include, which allows for external files to be logically embedded within the YAML file. The referenced file is always relative to the YAML file’s directory. string The string type allows an external file to be inlined in the YAML file as though it had been entered as a multi-line string. For example, given this file on disk named build.sh: set -e cp a.txt b.txt Then the following two YAML fragments are exactly equivalent: hooks:build:| set -ecpa.txtb.txthooks:build:!includetype:stringpath:build.shThat is primarily useful for breaking longer build scripts or inlined configuration files out to a separate file for easier maintenance. binary The binary type allows an external binary file to be inlined in the YAML file. The file will be base64 encoded. For example: properties:favicon:!includetype:binarypath:favicon.icowill reference the favicon.ico file, which will be provided to Platform.sh’s management system. yaml Finally, the yaml type allows an external YAML file to be inlined into the file as though it had been typed in directly. That can help simplify more complex files, such a .platform.app.yaml file with many highly-customized web.locations blocks. The yaml type is the default, meaning it may reference a file inline without specifying a type. For example, given this file on disk named main.yaml: the following three location definitions are exactly equivalent: Another custom tag available is !archive, which specifies a value is a reference to a directory on disk, relative to the location of the YAML file. Essentially it defines the value of key as “this entire directory”. Consider this services.yaml fragment: mysearch:type:solr:8.0disk:1024configuration:conf_dir:!archive solr/conf In this case, the mysearch.configuration.conf_dir value is not the string “solr/conf”, but the contents of the solr/conf directory (relative to the services.yaml file). On Platform.sh, that is used primarily for service definitions in services.yaml to provide a directory of configuration files for the service (such as Solr in this case). Platform.sh will use that directive to copy the entire specified directory into our management system so that it can be deployed with the specified service. Anchors YAML supports internal named references, known as “anchors.” They can be referenced using an “alias.” That allows you to have a large block of YAML that gets repeated multiple times in different places within a single file without having to copy-paste the whole block. An anchor is defined by appending \u0026amp;name to a segment, where “name” is some unique identifier. For example: relationships:\u0026amp;relsdatabase:\u0026#39;mysqldb:db1\u0026#39;cache:\u0026#39;rediscache:redis\u0026#39;search:\u0026#39;searchserver:elasticsearch\u0026#39;This block defines an anchor called rels, the contents of which is the 3 key/value pairs for database, cache, and search. An anchor can be referenced using an alias like *name, which will inject the anchored value into the file at that point. That is, the following two snippets are logically equivalent: foo:\u0026amp;foothing:stuffmany:{\u0026#39;stuff\u0026#39;,\u0026#39;here\u0026#39;}bar:*foofoo:thing:stuffmany:{\u0026#39;stuff\u0026#39;,\u0026#39;here\u0026#39;}bar:thing:stuffmany:{\u0026#39;stuff\u0026#39;,\u0026#39;here\u0026#39;}By default, aliases will inject their child contents entirely. If you want to overwrite a specific child key of an anchor there is a different alias syntax you must use: foo:\u0026amp;foothing:stuffmany:{\u0026#39;stuff\u0026#39;,\u0026#39;here\u0026#39;}bar:\u0026lt;\u0026lt;:*foothing:otherWhich is equivalent to: foo:thing:stuffmany:{\u0026#39;stuff\u0026#39;,\u0026#39;here\u0026#39;}bar:thing:othermany:{\u0026#39;stuff\u0026#39;,\u0026#39;here\u0026#39;}Be aware that aliases have sometimes non-obvious requirements around their whitespace formatting. In particular, when aliasing a anchor into a YAML array the alias reference must be at the same indentation level as any overrides. That is: - \u0026amp;mylistlist:ofvalues:here- \u0026lt;\u0026lt;: *mylist # These two lines must start at the same indentation.values:thereAnchor example Anchors and aliases are mainly useful when you want to repeat a given block of configuration. For example, the following snippet will define three identical worker instances within a .platform.app.yaml file: workers:queue1:\u0026amp;runnersize:\u0026#39;S\u0026#39;commands:start:pythonqueue-worker.pyvariables:env:type:\u0026#39;worker\u0026#39;queue2:*runnerqueue3:*runner",
        "section": "Configuration",
        "subsections": " Basic YAML Multi-line strings Includes  string binary yaml !archive   Anchors  Anchor example    ",
        "image": "",
        "url": "/configuration/yaml.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "d7299aacdddd49ea1f964e5c96b378a8",
        "title": "(Optional) Configure a third-party TLS certificate",
        "description": "",
        "text": " Platform.sh automatically provides standard TLS certificates issued by Let’s Encrypt to all production instances. No further action is required to use TLS-encrypted connections beyond specifying HTTPS routes in your routes.yaml file. Alternatively, you may provide your own third party TLS certificate from the TLS issuer of your choice at no charge from us. Please consult your TLS issuer for instructions on how to generate an TLS certificate. A custom certificate is not necessary for development environments. Platform.sh automatically provides wildcard certificates that cover all *.platform.sh domains, including development environments. Note: The private key should be in the old style, which means it should start with BEGIN RSA PRIVATE KEY. If it starts with BEGIN PRIVATE KEY that means it is bundled with the identifier for key type. To convert it to the old-style RSA key: openTLS rsa -in private.key -out private.rsa.key Adding a custom certificate through the management console You can add a custom certificate via the Platform.sh management console . In the management console for the project go to Settings and click Certificates on the left hand side. You can add a certificate with the Add button at the top of the page. You can then add your private key, public key certificate and optional certificate chain. Adding a custom certificate through the CLI Example: platform domain:add secure.example.com --cert=/etc/TLS/private/secure-example-com.crt --key=/etc/TLS/private/secure-example-com.key See platform help domain:add for more information. Success!: Your site should now be live, and accessible to the world (as soon as the DNS propagates). If something is not working see the troubleshooting guide for common issues. If that doesn’t help, feel free to contact support.",
        "section": "Going Live - Steps",
        "subsections": "   Adding a custom certificate through the management console Adding a custom certificate through the CLI    ",
        "image": "",
        "url": "/golive/steps/tls.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "43df3375130e2cd56d5ee8d06ffa5824",
        "title": "Backup and restore",
        "description": "",
        "text": " Backups are triggered directly via the management console or via the CLI. The backup creates a complete snapshot of the environment. It includes all persistent data from all running services (MySQL, Solr,…) and any files stored on the mounted volumes. Backups You need to have the “admin” role in order to create a backup of an environment. Backups on Platform.sh Professional are retained for at least 7 days. They will be purged between 7 days and 6 months, at Platform.sh’s discretion. Please see the data retention page for more information. Note: We advise you to make backups of your live environment before merging an environment to the live environment, or each time you increase the storage space of your services. Using the CLI: $ platform backup:create Please be aware that triggering a backup will cause a momentary pause in site availability so that all requests can complete, allowing the backup to be taken against a known consistent state. The total interruption is usually only 15 to 30 seconds and any requests during that time are held temporarily, not dropped. Restore You will see the backup in the activity feed of your environment in the Platform.sh management console. You can trigger the restore by clicking on the restore link. You can also restore the backup to a different environment using the CLI. You can list existing backups with the CLI as follows: $ platform backups Finding backups for the environment master \u0026#43;---------------------\u0026#43;------------\u0026#43;----------------------\u0026#43; | Created | % Complete | Backup name | \u0026#43;---------------------\u0026#43;------------\u0026#43;----------------------\u0026#43; | 2015-06-19 17:11:42 | 100 | 2ca4d90639f706283fee | | 2015-05-28 15:05:42 | 100 | 1a1fbcb9943849706ee6 | | 2015-05-21 14:38:40 | 100 | 7dbdcdb16f41f9e1c061 | | 2015-05-20 15:29:58 | 100 | 4997900d2804d5b2fc39 | | 2015-05-20 13:31:57 | 100 | c1f2c976263bec03a10e | | 2015-05-19 14:51:18 | 100 | 71051a8fe6ef78bca0eb | You can then restore a specific backup with the CLI as follows: $ platform backup:restore 2ca4d90639f706283fee Or even restore the backup to a different branch with the CLI as follows: $ platform backup:restore --target=RESTORE_BRANCH 2ca4d90639f706283fee For this to work, it’s important to act on the active branch on which the backup was taken. Restoring a backup from develop when working on the staging branch is impossible. Switch to the acting branch and set your --target as above snippet mentions. If no branch already exists, you can specify the parent of the branch that will be created to restore your backup to as follows: $ platform backup:restore --branch-from=PARENT_BRANCH 2ca4d90639f706283fee Note: You need “admin” role to restore your environment from a backup. Be aware that the older US and EU regions do not support restoring backups to different environments. If your project is on one of the older regions you may file a support ticket to ask that a backup be restored to a different environment for you, or migrate your project to one of the new regions that supports this feature. Backups and downtime A backup does cause a momentary pause in service. We recommend running during non-peak hours for your site. Automated backups Backups are not triggered automatically on Platform.sh Professional. Backups may be triggered by calling the CLI from an automated system such as Jenkins or another CI service, or by installing the CLI tool into your application container and triggering the backup via cron. Automated backups using Cron Note: Automated backups using cron requires you to get an API token and install the CLI in your application container. We ask that you not schedule a backup task more than once a day to minimize data usage. Once the CLI is installed in your application container and an API token configured you can add a cron task to run once a day and trigger a backup. The CLI will read the existing environment variables in the container and default to the project and environment it is running on. In most cases such backups are only useful on the master production environment. A common cron specification for a daily backup on the master environment looks like this: crons:backup:spec:\u0026#39;0 5 * * *\u0026#39;cmd:| if [  $PLATFORM_BRANCH  = master ]; thenplatformbackup:create--yes--no-waitfiThe above cron task will run once a day at 5 am (UTC), and, if the current environment is the master branch, it will run platform backup:create on the current project and environment. The --yes flag will skip any user-interaction. The --no-wait flag will cause the command to complete immediately rather than waiting for the backup to complete. Note: It is very important to include the --no-wait flag. If you do not, the cron process will block and you will be unable to deploy new versions of the site until the backup creation process is complete. Retention Please see our Data Retention Page .",
        "section": "Administration",
        "subsections": " Backups Restore Backups and downtime Automated backups  Automated backups using Cron Retention    ",
        "image": "",
        "url": "/administration/backup-and-restore.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "646460e528f26d680702036756ff2d5c",
        "title": "Build \u0026 Deploy",
        "description": "",
        "text": " Every time you push to a live branch (a git branch with an active environment attached to it) or activate an environment for a branch, there are two main processes that happen: Build and Deploy.” The build process looks through the configuration files in your repository and assembles the necessary containers. The deploy process makes those containers live, replacing the previous versions, with virtually no interruption in service. Always Be Compiling Interpreted languages like PHP or Node.js may not seem like they have to be compiled, but with modern package management tools like Composer or npm, as well as the growing use of CSS preprocessors such as Sass, most modern web applications need a “build” step between their source code and their production execution code. At Platform.sh, we aim to make that easy. The build step includes the entire application container—from language version to build tools to your code—rebuilt every time. That build process is under your control and runs on every Git push. Every Git push is a validation not only of your code, but of your build process. Whether that’s installing packages using Composer or Bundler, compiling TypeScript or Sass, or building your application code in Go or Java, your build process is vetted every time you push. Whenever possible, you should avoid committing build assets to your repository that can be regenerated at build time. Depending on your application, that may include 3rd party libraries and frameworks, generated and optimized CSS and JS files, generated source code, etc. The following two constraints make sure you have fast, repeatable builds: The build step should be environment-independent. This is paramount to ensure development environments are truly perfect copies of production. This means you can not connect to services (like the database) during the build. The final built application must be read-only. If your application requires writing to the filesystem, you can specify the directories that require Read/Write access. These should not be directories that have code, because that would be a security risk. Building the application After you push your code, the first build step is to validate your configuration files (i.e. .platform.app.yaml, .platform/services.yaml, and .platform/routes.yaml). The Git server will issue an error if validation fails, and nothing will happen on the server. Note: While most projects have a single .platform.app.yaml file, Platform.sh supports multiple applications in a single project. It will scan the repository for .platform.app.yaml files in subdirectories and each directory containing one will be built as an independent application. A built application will not contain any directories above the one in which it is found. The system is smart enough not to rebuild applications that have already been built, so if you have multiple applications, only changed applications will be rebuilt and redeployed. The live environment is composed of multiple containers—both for your application(s) and for the services it depends on. It also has a virtual network connecting them, as well as a router to route incoming requests to the appropriate application. Based on your application type the system will select one of our pre-built container images and run the following: First, any dependencies specified in the .platform.app.yaml file are installed. Those include tools like Sass, Gulp, Drupal Console, or any others that you may need. Then, depending on the “build flavor” specified in the configuration file, we run a series of standard commands. The default for PHP containers, for example, is simply to run composer install. Finally, we run the “build hook” from the configuration file. The build hook comprises one or more shell commands that you write to finish creating your production code base. That could be compiling Sass files, running a Gulp or Grunt script, rearranging files on disk, compiling an application in a compiled language, or whatever else you want. Note that, at this point, all you are able to access is the file system; there are no services or other databases available. Once all of that is completed, we freeze the file system and produce a read-only container image. That image is the final build artifact: a reliable, repeatable snapshot of your application, built the way you want, with the environment you want. Because container configuration (both for your application and its underlying services) is exclusively based on your configuration files, and your configuration files are managed through Git, we know that a given container has a 1:1 relationship with a Git commit. That means builds are always repeatable. It also means that if we detect that there are no changes that would affect a given container, we can skip the build step entirely and reuse the existing container image, saving a great deal of time. In practice, the entire build process usually takes less than a minute. Deploying the application Deploying the application also has several steps, although they’re much quicker. First, we pause all incoming requests and hold them so that there’s no interruption of service. Then we disconnect the current containers from their file system mounts, if any, and connect the file systems to the new containers instead. If it’s a new branch and there is no existing file system, we clone it from the parent branch. We then open networking connections between the various containers, but only those that were specified in the configuration files (using the relationships key). No configuration, no connection. That helps with security, as only those connections that are actually needed even exist. The connection information for each service is available in an application as environment variables. Now that we have working, running containers there are two more steps to run. First, if there is a “start” command specified in your .platform.app.yaml file to start the application, we run that. (With PHP, this is optional.) Then we run your deploy hook. Just like the build hook, the deploy hook is any number of shell commands you need to prepare your application. Usually this includes clearing caches, running database migrations, and so on. You have complete access to all services here as your application is up and running, but the file system where your code lives will now be read-only. Finally, once the deploy hooks are done, we open the floodgates and let incoming requests through to your newly deployed application. You’re done! In practice, the deploy process takes only a few seconds, plus whatever time is required for your deploy hook, if any.",
        "section": "The big picture",
        "subsections": " Always Be Compiling Building the application Deploying the application  ",
        "image": "",
        "url": "/overview/build-deploy.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "cc8e6c04be55eb4698bbee443553be4b",
        "title": "Changelog",
        "description": "",
        "text": " Look here for all the most recent additions to Platform.sh. 2020 April 2020 We now offer Xdebug on PHP containers. Custom activity scripts are now available, in alpha. March 2020 Go 1.14: We now support Go 1.14 . Ruby 2.7: We now support Ruby 2.7 . .NET Core: We now support .NET Core 3.1 . Memcached 1.6: We now support Memcached 1.6 . Solr 8.4: We now support Solr 8.4 . February 2020 Memcached 1.5: We now support Memcached 1.5 . Character set and collation are now configurable on MySQL/MariaDB . January 2020 RabbitMQ: We now support RabbitMQ virtual host configuration 2019 December 2019 Python 3.8: We now support Python 3.8 . Node.js 12: We now support Node.js 12 . RabbitMQ 3.8: We now support RabbitMQ 3.8 . October 2019 PHP 7.4: We now support PHP 7.4 . October 2019 Projects can now have larger application containers in development environments. September 2019 “Dark mode” now available in the Console. Go 1.13: We now support Go 1.13 . July 2019 Elasticsearch 7.2: We now support Elasticsearch 7.2 . Elasticsearch 5.2 and 5.4 support is now deprecated Kafka 2.2: We now support Kafka 2.2 . Java 13: We now support Java 13 . June 2019 Java: We support and documented the use of Java runtimes 8, 11, and 12, that includes examples that use the Java Config Reader . Headless Chrome: Users can now define a Headless Chrome service to access a service container with a headless browser, which can be used for automated UI testing. May 2019 InfluxDB: We now support InfluxDB 1.7. Solr 7 \u0026amp; 8: We now support Solr 7.7 and 8.0. April 2019 Network storage service: Users can now define a Network storage service for sharing files between containers. Kafka message queue service: Users can now define a Kafka service for storing, reading and analysing streaming data. Management Console: Images and wording updated throughout entire documentation alongside Management Console release. March 2019 Ruby 2.6: A new version of Ruby is now available. Go 1.12: We now support Go 1.12 . Elasticsearch 6.5: We now support Elasticsearch 6.5 . January 2019 RabbitMQ 3.7: We now support RabbitMQ 3.7 . Solr 7: We now support Solr 7.6 . Varnish: We now offer Varnish 5.2 and 6.0. 2018 December 2018 Elasticsearch 5.4: We now offer Elasticsearch 5.4 . Improved Bash support: Bash history on application containers now persists between logins. PHP 7.3: We now support PHP 7.3 . PostgreSQL 10.0 and 11.0: We now support PostgreSQL 10.0 and 11.0 with an automated upgrade path. Ruby 2.5 out of beta: We now fully support Ruby 2.5 . October 2018 Redis updates: Redis 4.0 and 5.0 are now supported. Go language support: Go is now a fully supported language platform. September 2018 Python 3.7 support: We now support Python 3.7 . August 2018 New public Canadian region: Our new Canadian region is now open for business. July 2018 Security and Compliance: We have created a new “Security and Compliance” section to help customers address common questions relating to GDPR, Data Collection, Data Retention, Encryption, and similar topics. June 2018 Node.js 10: We now offer Node.js version 10 . All releases in the 10.x series will be included in that container. MongoDB 3.6: We now offer MongoDB 3.2, 3.4, and 3.6 . Note that upgrading from MongoDB 3.0 requires upgrading through all intermediary versions. March 2018 Web Application Firewall (WAF): Platform.sh is securing your applications and you don’t need to change anything. Read more on our blog post . February 2018 post_deploy hook added: Projects can now run commands on deploy but without blocking new requests . 2017 December 2017 New project subdomains: The routes generated for subdomains and literal domains in development environments will now use . instead of translating them to ---, for projects created after this date. !include tag support in YAML files: All YAML configuration files now support a generic !include tag that can be used to embed one file within another. Extended mount definitions: A new syntax has been added for defining mount points that is more self-descriptive and makes future extension easier. Blocking older TLS versions: It is now possible to disable support for HTTPS requests using older versions of TLS. TLS 1.0 is known to be insecure in some circumstances and some compliance standards require a higher minimum supported version. {all} placeholder for routes: A new placeholder is available in routes.yaml files that matches all configured domains. GitLab source code integration: Synchronize Git repository host on GitLab to Platform.sh. November 2017 PHP 7.2 supported: With the release of PHP 7.2.0, Platform.sh now offers PHP 7.2 containers on Platform Professional. September 2017 Health notifications: Low-disk warnings will now trigger a notification via email, Slack, or PagerDuty. August 2017 Worker instances: Applications now support worker instances . July 2017 Node.js 8.2: Node.js 8.2 is now available. June 2017 Memcache 1.4: Memcache 1.4 is now available as a caching backend. Custom static headers in .platform.app.yaml: Added support for setting custom headers for static files in .platform.app.yaml. See the example for more information. May 2017 Code-driven variables in .platform.app.yaml: Added support for setting environment variables via .platform.app.yaml . Python 3.6, Ruby 2.4, Node.js 6.10: Added support for updated versions of several languages. April 2017 Support for automatic SSL certificates: All production environments are now issued an SSL certificate automatically through Let’s Encrypt. See the routing documentation for more information. MariaDB 10.1: MariaDB 10.1 is now available (accessible as mysql:10.1). Additionally, both MariaDB 10.0 and 10.1 now use the Barracuda file format with innodb_large_prefix enabled, which allows for much longer indexes and resolves issues with some UTF-8 MB use cases. March 2017 Elasticsearch 2.4 and 5.2 with support for plugins: Elasticsearch 2.4 and 5.2 are now available. Both have a number of optional plugins avaialble. See the Elasticsearch documentation for more information. InfluxDB 1.2: A new service type is available for InfluxDB 1.2, a time-series database. See the InfluxDB documentation for more information. February 2017 HHVM 3.15 and 3.18: Two new HHVM versions are now available. January 2017 Support for Multiple MySQL databases and restricted users: MySQL now supports multiple databases, and restricted users per MySQL service. See the MySQL documentation for details or read our blog post . Support for Persistent Redis services: Added a redis-persistent service that is appropriate for persistent key-value data. The redis service is still available for caching. See the Redis documentation for details. Support Apache Solr 6.3 with multiple cores: Added an Apache 6.3 service, which can be configured with multiple cores. See the Solr documentation for details. Support for HTTP/2: Any site configured with HTTPS will now automatically support HTTP/2. Read more on our blog post . 2016 December 2016 Support Async PHP: Deploy applications like ReactPHP and Amp which allow PHP to run as a single-process asynchronous process. Read more on our blog post . Pthreads: Multithreaded PHP: Our PHP 7.1 containers are running PHP 7.1 ZTS, and include the Pthreads extension. Read more on our blog post . PHP 7.1: Service is documented here . Support .environment files: This file will get sourced as a bash script by the system when a container boots up, as well as on all SSH logins. Feature is documented here . Support web.commands.start for PHP: That option wasn’t available for PHP as PHP only has one applicable application runner, PHP-FPM. It is now available for PHP. Read more on our blog post . November 2016 Customizable build flavor: Added a none build flavor which will not run any specific command during the build process. Use it if your application requires a custom build process which can be defined in your build hook. Read more in our blog post . October 2016 PostgreSQL 9.6: Service is documented here . PostgreSQL extensions: Read more in our blog post . Node.js 6.8: Language is documented here . September 2016 Python 2.7 \u0026amp; 3.5: Language is documented here . Ruby 2.3: Language is documented here . August 2016 Support Gitflow: Read more in our blog post . July 2016 Block Httpoxy security vulnerability: We bypass the Httpoxy security vulnerability by blocking the Proxy header from incoming HTTP headers. Read more in our blog post . Remove default configuration files: We are removing the default configuration files that were previously used if your project didn’t include one. You now need to include configuration files to deploy your applications on Platform.sh. Read more in our blog post . June 2016 June update is summarized in our blog post . New PLATFORM_PROJECT_ENTROPY variable: New variable which has a random value, stable throughout the project’s life. It can be used for Drupal hash salt for example (in our Drupal 8 example ). It is documented here Extend PLATFORM_RELATIONSHIPS variable: Expose the hostname and IP address of each service in the PLATFORM_RELATIONSHIPS environment variable. Services updates: Update MongoDB client to 3.2.7, Node.js to 4.4.5, Blackfire plugin to 1.10.6, Nginx to 1.11.1. May 2016 May update is summarized in our blog post . Pre-warms Composer cache before executing Composer: The composer build flavor now pre-warms the Composer cache before executing Composer. New image processing packages (advancecomp, jpegoptim, libjpeg-turbo-progs, optipng, pngcrush): Various image processing packages were added: advancecomp, jpegoptim, libjpeg-turbo-progs, optipng, pngcrush. Security updates: Including imagetragick, glibc issue, various Java, OpenSSL and OpenSSH issues, along with some Git CLI vulnerabilities. April 2016 White label capabilities (Magento Enterprise Cloud Edition): Support for Platform.sh white label offering. First launch at Magento Imagine 2016 in Las Vegas of Magento Enterprise Cloud Edition . March 2016 CloudWatt deployment: Platform.sh is now available on Cloudwatt Orange Business Services hosted infrastructure. Read more in our blog post . January 2016 Redis 3.0: Service is documented here . 2015 December 2015 Node.js 0.12, 4.4 \u0026amp; 6.2: Read more in our blog post November 2015 Java Ant \u0026amp; Maven build scripts: Java Ant and Maven build scripts is supported for PHP 5.6 and up. Your application can pull and use most of Java dependency. Read more in our blog post October 2015 MariaDB/MySQL 10.0: Service is documented here . MongoDB 3.0: Service is documented here . September 2015 PHP 5.4, 5.5 \u0026amp; 5.6: Read more in our blog post RabbitMQ 3.5: Service is documented here . Read more in our blog post HHVM 3.9 \u0026amp; 3.12: Read more in our blog post July 2015 Documentation 3.0 release: Read more in our blog post . June 2015 Bitbucket integration: This add-on allows you to deploy any branch or pull request on a fully isolated Platform.sh environment with a dedicated URL. Read more in Bitbucket’s blog post . PostgreSQL 9.3: Service is documented here . May 2015 UI 2.0 release: Read more in our blog post . February 2015 Blackfire integration: PHP applications come pre-installed with the Blackfire Profiler developed by SensioLabs . Read more in our blog post . January 2015 Redis 2.8: Service is documented here . 2014 November 2014 Read more about this release in our blog post . HTTP caching per route: Support for HTTP caching at the web server level, finely configurable on a per-route basis. Custom PHP configurations: Support for tweaking the PHP configuration, by enabling / disabling extensions and shipping your own php.ini. Build dependencies: Support for specifying build dependencies, i.e. PHP, Python, Ruby or Node.js tools (like sass, grunt, uglifyjs and more) that you want to leverage to build your PHP application. Elasticsearch 0.90, 1.4 \u0026amp; 1.7: Service is documented here . October 2014 Automated protective block: Platform.sh provides a unique approach to protect your applications from known security issues. An automated protective blocking system which works a bit like an antivirus: it compares the code you deploy on Platform.sh with a database of signatures of known security issues in open source projects. This feature is documented here . Read more in our blog post . Solr 4.10: Service is documented here . July 2014 MariaDB/MySQL 5.5: Service is documented here . Solr 3.6: Service is documented here .",
        "section": "Platform.sh",
        "subsections": " 2020 2019 2018 2017 2016 2015 2014  ",
        "image": "",
        "url": "/other/changelog.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "18989c6838541b94952b0f2f93ef8362",
        "title": "Cloudflare configuration",
        "description": "",
        "text": " One of the main features that a modern DNS provider needs to have in order to work well with Platform.sh is colloquially known as “Cname Flattening”. This solves the problem of being able to point your “root domain” (example.com) to a domain name (CNAME) rather than an IP address (A record). This post explains it well. In order to correctly point DNS to your Platform.sh project, you’ll need at the very least the master environment CNAME, in other words the domain of your site before you add a custom domain on the management console for that project (or otherwise in the CLI). This is the value you would get from Step 2 of the pre-launch checklist . Assuming that you are using both a www. subdomain as well as the bare domain, you’ll want to point both of those DNS entries to the same place. Whether you choose the bare domain version or the www subdomain doesn’t make any practical difference, as they both will reach Platform.sh and be handled correctly. Enable “Full SSL” option in the Cloudflare admin Cloudflare also makes it very simple to use their free TLS/SSL service to secure your site via HTTPS, while also being behind their CDN if you so choose. If you decide to use Cloudflare’s CDN functionality in addition to their DNS service, you should be sure to choose the “Full SSL” option in the Cloudflare admin. This means that traffic to your site is encrypted from the client (browser) to Cloudflare’s servers using their certificate, and also between Cloudflare’s servers and your project hosting here at Platform.sh, mostly like using your project’s Let’s Encrypt certificate. # Cloudflare\u0026#39;s Full SSL option https https User \u0026lt;---------------\u0026gt; Cloudflare \u0026lt;-------------\u0026gt; Platform.sh The other option known as “Flexible SSL” will cause issues if you intend to redirect all traffic to HTTPS. The “Flexible SSL” option will use Cloudflare’s TLS/SSL certificate to encrypt traffic between your users and the CDN, but will pass requests from the CDN back to your project at Platform.sh via HTTP. This can make it easy for sites that don’t have a TLS/SSL certificate to begin ofering their users a more secure experience, by at the least eliminating the unencrypted attack vector on the the “last mile” to the user’s browser. # Cloudflare\u0026#39;s Flexible SSL option https http User \u0026lt;---------------\u0026gt; Cloudflare \u0026lt;-------------\u0026gt; Platform.sh This will cause all traffic from Cloudflare to your project to be redirected to HTTPS, which will set off an endless loop as HTTPS traffic will be presented as HTTP to your project no matter what. In short: Always use “Full SSL” unless you have a very clear reason to do otherwise",
        "section": "Content Delivery Networks",
        "subsections": " Enable \u0026ldquo;Full SSL\u0026rdquo; option in the Cloudflare admin  ",
        "image": "",
        "url": "/golive/cdn/cloudflare.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "45584cb66ba776d1feda9da54211c588",
        "title": "Composer manager",
        "description": "",
        "text": " Drupal 7 does not natively support installing packages via Composer. Although there is a Drupal Composer project for Drupal 7 , many projects are still built using a vanilla Drupal download or with Drush Make. For sites built without Drupal Composer that still want to use modules that have Composer dependences, the most widely used option is the Composer Manager module. Because of the read-only file system, however, there are specific configuration parameters necessary on Platform.sh. 1. Install and patch Composer Manager Install the Composer Manager module in the manner appropriate for your site. There are also 2 patch files needed that have not been committed to the module yet. If you’re installing it via Drush Make, add the appropriate lines to your .make file: projects[composer_manager][version] =  1.8  projects[composer_manager][patch][] =  https://www.drupal.org/files/issues/composer_manager-2620348-3.patch  projects[composer_manager][patch][] =  https://www.drupal.org/files/issues/composer_manager-relative_realpath-2864297-5.patch  If you’re checking the entire codebase into Git, do so with Composer Manager as well. Then download and apply the two patches in the issues above and commit the result. It’s important to uncheck the two “Automatically…” options at the config page on admin/config/system/composer-manager/settings. If checked, Drupal tries to update the composer folder. Since this isn’t a writable mount, installation of composer based modules will fail due to these writing permissions. 2. Configure file locations Composer Manager works by using a Drush command to aggregate all module-provided composer.json files into a single file, which can then be installed via a normal Composer command. Both the generated file and the resulting vendor directory must be in the application portion of the file system, that is, not in a writable file mount. As that is not the default configuration for Composer Manager it will need to be changed. Add the following lines to your settings.php file: $conf[\u0026#39;composer_manager_vendor_dir\u0026#39;] = \u0026#39;../composer/vendor\u0026#39;; $conf[\u0026#39;composer_manager_file_dir\u0026#39;] = \u0026#39;../composer\u0026#39;; The above lines will direct Composer Manager to put the generated composer.json file into ../composer/, and to autoload Composer-based packages from the ../composer/vendor directory. The paths are relative to the Drupal root. You may use another location if desired provided that they are not in a writable file mount, and provided the vendor directory is a sibling of wherever the composer.json file will be. Then, manually create the composer directory and place a .gitignore file inside it, containing the following, and commit it to Git: # Exclude Composer dependencies. /vendor 3. Update the build hook Create a build hook in your .platform.app.yaml file that will install Composer dependencies: hooks:build:| # Install Composer dependencies.cdcomposercomposerinstall--no-interaction--optimize-autoloader--no-devReplace composer with whatever the path to the composer directory is. Note that if using the drupal build flavor with Drush Make, then the composer directory may have been moved into the same directory as your modules, like public/sites/default. It can be moved back via another line in the build hook: hooks:build:| # Move the Composer directory (back) to the application root.mvpublic/sites/default/composercomposer# Install Composer dependencies.cdcomposercomposerinstall--no-interaction--optimize-autoloader--no-devThe composer install command may also be further customized as desired. 4. Generate and commit the composer.* files locally From the Drupal root on your local system, run drush composer-json-rebuild to generate the aggregated composer.json file. Then, change to the composer directory and run composer install yourself, to generate the composer.lock file and download all dependencies. The composer.json and composer.lock files must be committed to the repository. The vendor directory must not be. That way, during build the Composer command in the build hook will download the exact version of all dependent packages listed in the composer.lock file, which helps keep builds consistent and predictable. Any time a new module with Composer dependencies is added, or a new version of a dependent library is available, repeat step 4 only.",
        "section": "Getting Started",
        "subsections": " 1. Install and patch Composer Manager 2. Configure file locations 3. Update the build hook 4. Generate and commit the composer.* files locally  ",
        "image": "",
        "url": "/frameworks/drupal7/composer-manager.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "b399d3e553aaf012dab49f7bd4293100",
        "title": "Configure routes",
        "description": "",
        "text": " Platform.sh allows you to define the routes used in your environments. A route describes how an incoming HTTP request is going to be processed by Platform.sh. The routes are defined using .platform/routes.yaml file in your Git repository. If you don’t have one, use the commands below to create it: $ mkdir .platform $ touch .platform/routes.yaml Route templates The YAML file is composed of a list of routes and their configuration. A route can either be an absolute URL or a URL template that looks like: http://www.{default}/ or https://{default}/blog where {default} will be substituted by the default fully qualified domain name configured in the project. So if your default domain is example.com, these routes will be resolved to http://www.example.com/ and https://example.com/blog in the master environment. Platform.sh will also generate a domain for every active development environment. It will receive a domain name based on the region, project ID, branch name, and a per-environment random string. The domain name itself is not guaranteed stable, although the pattern is consistent. Note: Platform.sh supports running multiple applications per environment. The .platform/routes.yaml file defines how to route requests to different applications. Route configuration Each route can be configured separately. It has the following properties type can be: upstream serves an application It will then also have an upstream property which will be the name of the application (as defined in .platform.app.yaml), followed by “:http” (see examples below). redirect redirects to another route It will then be followed by a to property, this defines a HTTP 301 redirect to any URL or another route (see examples below). cache controls caching behavior of the route . ssi controls whether Server Side Includes are enabled. For more information: see SSI . redirects controls redirect rules associated with the route. Note: For the moment, the value of upstream is always in the form: \u003capplication-name\u003e:http. \u003capplication-name\u003e is the name defined in .platform.app.yaml file. :php is a deprecated application endpoint; use :http instead. In the future, Platform.sh will support multiple endpoints per application. Route limits Although there is no fixed limit on the number of routes that can be defined, there is a cap on the size of generated route information. This limitation comes from the Linux kernel, which caps the size of environment variables. The kernel limit on environment variables is 32 pages. Each page is 4k on x86 processors, resulting in a maximum environment variable length of 131072 bytes. If your routes.yaml file would result in too large of a route information value it will be rejected. The full list of generated route information is often much larger than what is literally specified in the routes.yaml file. For example, by default all HTTPS routes will be duplicated to create an HTTP redirect route. Also, the {all} placeholder will create two routes (one HTTP, one HTTPS) for each domain that is configured. As a general rule we recommend keeping the defined routes under 100. Should you find your routes.yaml file rejected due to an excessive size the best alternative is to move any redirect routes to the application rather than relying on the router, or collapsing them into a regular expression-based redirect within a route definition. Let’s Encrypt also limits an environment to 100 configured domains. If you try to add more than that some of them will fail to get an SSL certificate. Routes examples Here is an example of a basic .platform/routes.yaml file:  https://{default}/ :type:upstreamupstream: app:http  https://www.{default}/ :type:redirectto: https://{default}/ In this example, we will route both the apex domain and the www subdomain to an application called “app” , the www subdomain being redirected to the apex domain using an HTTP 301 Moved Permanently response. In the following example, we are not redirecting from the www subdomain to the apex domain but serving from both:  https://{default}/ :type:upstreamupstream: app:http  https://www.{default}/ :type:upstreamupstream: app:http Route placeholders You can configure any number of domains on a project when you are ready to make it live. Only one of them may be set as the “default” domain. In the routes.yaml file a route can be defined either literally or using one of two special placeholders: {default} and {all}. The magic value {default} will be replaced with the production domain name configured as the default on your account in the production branch. In a non-production branch it will be replaced with the project ID and environment ID so that it is always unique. The magic value {all} will be replaced by all of the domain names configured on the account. That is, if two domains example1.com and example2.com are configured, then a route named https://www.{all}/ will result in two routes in production: https://www.example1.com and https://www.example2.com. That can be useful in cases when a single application is serving two different websites simultaneously. In a non-production branch it will be replaced with the project ID and environment ID for each domain, in the same fashion as a static route below. If there are no domains defined on a project (as is typical in development before launch) then the {all} placeholder will behave exactly like the {default} placeholder. It’s also entirely possible to use an absolute URL in the route. In that case, it will be used as-is in a production environment. On a development environment it will be mangled to include the project ID and environment name. For example:  https://www.example.com/ :type:upstreamupstream: app:http  https://blog.example.com/ :type:upstreamupstream: blog:http In this case, there are two application containers app and blog. In a production environment, they would be accessible at www.example.com and blog.example.com, respectively. On a development branch named sprint, however, they would be accessible at URLs something like: https://www.example.com.sprint-7onpvba-tvh56f275i3um.eu-2.platformsh.site/ https://blog.example.com.sprint-7onpvba-tvh56f275i3um.eu-2.platformsh.site/ If your project involves only a single apex domain with one app or multiple apps under subdomains, it’s generally best to use the {default} placeholder. If you are running multiple applications on different apex domains then you will need to use a static domain for all but one of them. Please note that when there are two routes sharing the same HTTP scheme, domain, and path, where the first route is using the {default} placeholder and the other is using the {all} placeholder, the route using {default} takes precedence. Route identifiers All routes defined for an environment are available to the application in its PLATFORM_ROUTES environment variable, which contains a base64-encoded JSON object. This object can be parsed by the language of your choice to give your application access to the generated routes. When routes are generated, all placeholders will be replaced with appropriate domains names, and depending on your configuration, additional route entries may be generated (e.g. automatic HTTP to HTTPS redirects). To make it easier to locate routes in a standardized fashion, you may specify an id key on each route which remains stable across environments. You may also specify a single route as primary, which will cause it to be highlighted in the web management console but will have no impact on the runtime environment. Consider this routes.yaml configuration example:  https://site1.{default}/ :type:upstreamupstream:'site1:http' https://site2.{default}/ :type:upstreamid:'the-second'upstream:'site2:http'This example defines two routes, on two separate subdomains, pointing at two separate app containers. (However, they could just as easily be pointing at the same container for our purposes). On a branch named test, the route array in PHP would look like this: Array ( [https://site1.test-t6dnbai-abcdef1234567.us-2.platformsh.site/] =\u003e Array ( [primary] =\u003e 1 [id] =\u003e [type] =\u003e upstream [upstream] =\u003e site1 [original_url] =\u003e https://site1.{default}/ // ... ) [https://site2.test-t6dnbai-abcdef1234567.us-2.platformsh.site/] =\u003e Array ( [primary] =\u003e [id] =\u003e the-second [type] =\u003e upstream [upstream] =\u003e site2 [original_url] =\u003e https://site2.{default}/ // ... ) [http://site1.test-t6dnbai-abcdef1234567.us-2.platformsh.site/] =\u003e Array ( [to] =\u003e https://site1.test-t6dnbai-abcdef1234567.us-2.platformsh.site/ [original_url] =\u003e http://site1.{default}/ [type] =\u003e redirect [primary] =\u003e [id] =\u003e ) [http://site2.test-t6dnbai-abcdef1234567.us-2.platformsh.site/] =\u003e Array ( [to] =\u003e https://site2.test-t6dnbai-abcdef1234567.us-2.platformsh.site/ [original_url] =\u003e http://site2.{default}/ [type] =\u003e redirect [primary] =\u003e [id] =\u003e ) ) (Some keys omitted for space.) Note that the site2 HTTPS route has an id specified as the-second while other routes have no ID. Futhermore, because we did not specify a primary route, the first non-redirect route defined is marked as the primary route by default. In each case, the original_url specified in the configuration file is accessible if desired. That makes it straightforward to look up the domain of a particular route, regardless of what branch it’s on, from within application code. For example, the following PHP function will retrieve the domain for a specific route id, regardless of the branch it’s on. function get_domain_for(string $id) { foreach ($routes as $domain =\u003e $route) { if ($route['id'] == $id) { return $domain; } } } That can be used, for example, for inbound request whitelisting, a feature of many PHP frameworks. Route attributes Route attributes are an arbitrary key/value pair attached to a route. This metadata does not have any impact on Platform.sh, but will be available in the route definition structure in $PLATFORM_ROUTES.  http://{default}/ :type:upstreamupstream: app:http attributes: foo :  bar Attributes will appear in the routes data for PHP like so: [https://site1.test-t6dnbai-abcdef1234567.us-2.platformsh.site/] =\u003e Array ( [primary] =\u003e 1 [id] =\u003e [type] =\u003e upstream [upstream] =\u003e site1 [original_url] =\u003e https://site1.{default}/ [attributes] =\u003e Array ( [foo] =\u003e bar ) // ... ) These extra attributes may be used to “tag” routes in more complex scenarios that can then be read by your application. Configuring routes on the management console Routes can also be configured using the management console in the routes section of the environment settings. If you have edited the routes via the management console, you will have to git pull the updated .platform/routes.yaml file from us. CLI Access You can get a list of the configured routes of an environment by running platform environment:routes. If you need to see more detailed info, such as cache and ssi, use platform route:get Wildcard routes Platform.sh supports wildcard routes, so you can map multiple subdomains to the same application. This works both for redirect and upstream routes. You can simply prefix the route with a star (*), for example *.example.com, and HTTP request to www.example.com, blog.example.com, us.example.com will all get routed to the same endpoint. For your master environment, this would function as a catch-all domain once you added the parent domain to the project settings. For development environments, we will also be able to handle this. Here is how: Let’s say we have a project on the EU cluster whose ID is “vmwklxcpbi6zq” and we created a branch called “add-theme”. It’s environment name will be similar to add-theme-def123. The generated apex domain of this environment will be add-theme-def123-vmwklxcpbi6zq.eu.platform.sh. If we have a http://*.{default}/ route defined, the generated route will be http://*.add-theme-def123-vmwklxcpbi6zq.eu.platform.sh/. This means you could put any subdomain before the left-most . to reach your application. HTTP request to both http://foo.add-theme-def123-vmwklxcpbi6zq.eu.platform.sh/ and http://bar.add-theme-def123-vmwklxcpbi6zq.eu.platform.sh/ URLs will be routed to your application properly. However, request to http://*.add-theme-def123-vmwklxcpbi6zq.eu.platform.sh/ will not be routed since it is not a legitimate domain name. Be aware, however, that we do not support Let’s Encrypt wildcard certificates (they would need DNS validation). That means if you want to use a wildcard route and protect it with HTTPS you will need to provide a custom TLS certificate . Note: In projects created before November 2017 the . in subdomains was replaced with a triple dash (---). It was switched to preserve . to simplify SSL handling and improve support for longer domains. If your project was created before November 2017 then it will still use --- to the left of the environment name. If you wish to switch to dotted-domains please file a support ticket and we can do that for you. Be aware that doing so may change the domain name that your production domain name should CNAME to. WebSocket routes To use WebSocket on a route, cache must be disabled because WebSocket is incompatible with buffering, which is a requirement of caching on our router. Here is an example to define a route that serves WebSocket:  https://{default}/ws :type:upstreamupstream: app:http cache:enabled:false",
        "section": "Configuration",
        "subsections": " Route templates Route configuration Route limits Routes examples Route placeholders Route identifiers Route attributes Configuring routes on the management console CLI Access Wildcard routes WebSocket routes  ",
        "image": "",
        "url": "/configuration/routes.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "6438412924c2bd66b579d62667929e9a",
        "title": "Connect to services",
        "description": "",
        "text": " Now that you have a local copy of your application code, you can make changes to the project without pushing to Platform.sh each time to test them. Instead you can locally build your application using the CLI, even when its functionality depends on a number of services. Note: If your application does not contain any services, you do not need to open a tunnel and can proceed to the next step. Open an SSH tunnel to connect to your services Open local SSH tunnels to your environment’s services. platform tunnel:open Export environment variables Platform.sh utilizes environment variables called Relationships within the application container. These store the credentials needed to connect to individual services. In order to connect with them remotely using the SSH tunnel you will need to mimic the same environment variables locally. export PLATFORM_RELATIONSHIPS= $(platform tunnel:info --encode)  In order to use these credentials to connect to your services, it may also be necessary to install the clients for those services are locally. Additionally, if your application also needs access to the PORT environment variable, you can mock the variable used in a Platform.sh environment with export PORT=8888 If you are using a Config Reader library with the application, it will also be necessary to mock two additional variables export PLATFORM_APPLICATION_NAME=\u0026lt;.platform.app.yaml name, i.e. app\u0026gt; export PLATFORM_BRANCH=\u0026lt;branch being built, i.e. dev\u0026gt; Verify You can visualize the open tunnels for your application with the command platform tunnel:list The tunnel will close itself after a timeout period of inactivity, but you can also do so with the command platform tunnel:close Now that you have created an SSH tunnel to your services, build your application locally. Back I\u0026#39;ve opened an SSH tunnel into my services",
        "section": "Getting started",
        "subsections": "",
        "image": "",
        "url": "/gettingstarted/developing/local-development/connect-services.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "5fca82741ee8914b75e2544f75403423",
        "title": "Create a new project",
        "description": "",
        "text": " In the previous step, you set up a free trial account with Platform.sh. Now you have access to the Platform.sh management console . From here you can create projects, adjust account settings, and a lot more that you will explore throughout these Getting Started guides. Since you do not yet have any projects on Platform.sh, the first thing you will see when you sign in is a workflow for creating a new project. Project type You will first be given the option of creating a New Project that is empty ( one that you can migrate your own code base to ), or to Use a Template. Select the Use a Template option, and then click Next. Details Give your project a name like My First Project. In the next field, you have the option to configure the project’s region, which corresponds with the data center where your project will live. Select the region that most closely matches where most of your traffic will come from and click Next. Template Now you will be able to see the large collection of Platform.sh’s supported templates. There are several types of templates available, including simple language-specific examples, ready-to-use frameworks you can build upon, and setapplications you can start using immediately after installation. If you are more comfortable with a particular language, click the dropdown labeled All languages. Select a language and the template list will update. Select a template and click Next. Note: You can find the source code for all Platform.sh templates in the GitHub Templates Organization. Plan \u0026amp; Pricing Under your free trial, your project will be created under a “Development” plan size. The management console will tell you how many users, development environments, and the price of that plan after your trial has completed. After you have read through the features, click Continue. With these few selections Platform.sh will create the project and build the template in less than two minutes. Back I\u0026#39;ve created a new project with a template",
        "section": "Getting started",
        "subsections": "",
        "image": "",
        "url": "/gettingstarted/gettingstarted/template/create-project.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "a47be1b5ba7b9e91da1de1ac98313fa5",
        "title": "Create environment",
        "description": "",
        "text": " Platform.sh supports the deployment of isolated development environments from your branches, complete with exact copies of all of your data. This is useful for testing changes in isolation before merging them. Create branch, make changes, push to Platform.sh Create and checkout a branch for your new feature. platform branch dev This command will create a new branch dev from master, as well as a local dev branch for you to work on. dev will be a clone of master, including an exact-copy of all of its data and files. Make changes to your code, commit them, and push to the Platform.sh remote. git add . git commit -m  Commit message.  git push platform dev Note: If you create a new branch with Git (i.e. git checkout -b new-feature), when you push its commits to Platform.sh that branch will not automatically build. new-feature is an inactive environment, because Platform.sh does not assume that every branch should be associated with an active environment, since your plan will limit the number of active environments you are allowed. If you want to activate the new-feature environment after it has been pushed, you can do so with the command platform environment:activate new-feature Verify After Platform.sh has built and deployed the environment, verify that it has been activated by visiting its new url: platform url The command will provide a list of the generated routes for the application according to how the routes were configured. The structure will be: Enter a number to open a URL [0] https://\u0026lt;branch name\u0026gt;-\u0026lt;hash of branch name\u0026gt;-\u0026lt;project ID\u0026gt;.\u0026lt;region\u0026gt;.platformsh.site/ [1] https://www.\u0026lt;branch name\u0026gt;-\u0026lt;hash of branch name\u0026gt;-\u0026lt;project ID\u0026gt;.\u0026lt;region\u0026gt;.platformsh.site/ The above URLs represent the upstream (0) and redirect (1) routes for the most common routes.yaml configuration. Back I\u0026#39;ve created a development environment",
        "section": "Getting started",
        "subsections": "",
        "image": "",
        "url": "/gettingstarted/developing/dev-environments/create-environment.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "7f3a1a50032fe827888ec42d287d9d28",
        "title": "Development environments",
        "description": "",
        "text": " While you’re developing your application, you will at some point create some new features for it. Typically you’re going to develop that feature on a separate branch in your Git repository, run some tests, and then merge that feature into your production application. This is where the stress comes in and where breaking your live site becomes a real worry. Platform.sh removes this stress considerably by providing live development environments for the features you’re working on. This guide assumes that you have already: Signed up for a free trial account with Platform.sh. Started either a template project or pushed your own code to Platform.sh. If you have not completed these steps by now, click the links and do so before you begin. Get started!",
        "section": "Introduction",
        "subsections": "",
        "image": "",
        "url": "/gettingstarted/developing/dev-environments.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "702e7331732912988d7075d3566d0496",
        "title": "Exporting data",
        "description": "",
        "text": " Platform.sh aims to be a great host, but we never want to lock you in to our service. Your code and your data belong to you, and you should always be able to download your site’s data for local development, backup, or to “take your data elsewhere”. Downloading code Your application’s code is maintained in Git. Because Git is a distributed system it is trivial to download your entire code history with a simple git clone or platform get command. Downloading files Your application runs on a read-only file system, so it cannot be edited. That means there’s nothing to download from most of it that isn’t already in your Git repository. The only files to download are from any writable file mounts you may have defined in your .platform.app.yaml file. The easiest way to download those is using the rsync tool. For instance, suppose you have a mounts section that defines one web-accessible directory and one non-web-accessible directory: mounts:\u0026#39;web/uploads\u0026#39;:source:localsource_path:uploads\u0026#39;private\u0026#39;:source:localsource_path:privateUsing the CLI The CLI provides a useful mount command for accessing mount data. platform mount:list Downloading a mount is then as simple as running the following: platform mount:download Using rsync To use rsync to download each directory, we can use the following commands. The platform ssh --pipe command will return the SSH URL for the current environment as an inline string that rsync can recognize. To use a non-default environment, use the -e switch after --pipe. Note that the trailing slash on the remote path means rsync will copy just the files inside the specified directory, not the directory itself. rsync -az `platform ssh --pipe`:/app/private/ ./private/ rsync -az `platform ssh --pipe`:/app/web/uploads ./uploads/ Note: If you’re running rsync on MacOS, you should add --iconv=utf-8,utf-8-mac to your rsync call. See the rsync documentation for more details on how to adjust the download process. Download data from services The mechanism for downloading from each service (such as your database) varies. For services designed to hold non-persistent information (such as Redis or Solr) it’s generally not necessary to download data as it can be rebuilt from the primary data store. To download data from persistent services ( MySQL , PostgreSQL , MongoDB , or InfluxDB ), see each service’s page for instructions.",
        "section": "Tutorials",
        "subsections": " Downloading code Downloading files  Using the CLI Using rsync   Download data from services  ",
        "image": "",
        "url": "/tutorials/exporting.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "13602a80bb29527d60e82bc0751512ac",
        "title": "Frameworks",
        "description": "",
        "text": " Full Drush support and Composer-based builds make handling dependencies and builds for PHP frameworks as simple as committing your composer.json to your project. Drupal Drupal is an open-source content management framework written in PHP. Since Composer comes pre-installed on Platform.sh, Drupal can be installed and updated completely using Composer. The default build flavor for PHP application runs composer install during build, handling all of your dependencies automatically. Drupal 7 Best Practices Drupal 8 Best Practices Community Support Drupal FAQs, how-to guides and other tutorials right on Platform.sh Community . Drupal on Platform.sh Community Templates Drupal 7 Drupal 7 (Vanilla) Drupal 8 Drupal 8 (Multisite variant) Opigno GovCMS8 eZ Platform eZ Platform is a CMS based on the Symfony full-stack framework. eZ Platform comes pre-configured for use with Platform.sh for versions 1.13 and later, all it takes is mapping a few environment variables to an existing project. Consult the caching, configuration, and local development best practices for eZ Platform and Fastly integration for more information. eZ Platform Best Practices Example Projects Note: Template projects (repositories in the platformsh-templates GitHub organization) are actively maintained by the Platform.sh team. Any other example projects come with less support, and remain in public repositories as proof-of-concepts. eZ Platform Symfony Symfony is a web application framework written in PHP. Like Drupal, Symfony projects can utilize native Composer to build applications and manage dependencies. Symfony Best Practices Community Support Symfony FAQs, how-to guides and other tutorials right on Platform.sh Community . Symfony on Platform.sh Community Templates Symfony 3 Symfony 4 TYPO3 TYPO3 is an open-source CMS written in PHP. Utilized Platform.sh native Composer to handle builds and maintain dependencies. TYPO3 Best Practices Example Projects Note: Template projects (repositories in the platformsh-templates GitHub organization) are actively maintained by the Platform.sh team. Any other example projects come with less support, and remain in public repositories as proof-of-concepts. TYPO3 Wordpress Wordpress is a PHP content management system. Platform.sh recommends using the composer-based installation method for Wordpress. Wordpress Best Practices Community Support All your Wordpress FAQs, plus how-to guides and tutorials right on Platform.sh Community . Wordpress on Platform.sh Community Templates Wordpress",
        "section": "PHP",
        "subsections": " Drupal  Community Support Templates   eZ Platform  Example Projects   Symfony  Community Support Templates   TYPO3  Example Projects   Wordpress  Community Support Templates    ",
        "image": "",
        "url": "/languages/php/frameworks.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "7ebe9ded6d61c1e6df9aca4b06c91069",
        "title": "Getting Started",
        "description": "",
        "text": " Drupal 8 and Composer The recommended way to deploy Drupal 8 on Platform.sh is to use Composer. Composer is the PHP package management suite, and is now supported by Drupal 8 (and Drupal 7 in a pinch). There is an unofficial but well-supported Composer flavor of Drupal 8 called Drupal Composer that we recommend. If you use the Drupal 8 Example Repository or select the Drupal 8 option when creating a new project from a template, that’s what you will be using. You can also create your own project directly from that repository and add the Platform.sh-specific configuration files. Note that you will also need to add the Drupal.org Composer repositories to composer.json if you are not working from our template. If you use Drupal Composer, note that any 3rd party modules, themes, or PHP libraries you install, as well as Drupal core itself, will not be checked into your repository. They are specifically excluded from Git by a .gitignore file, as they will be re-downloaded when you run composer install or composer update. Rather than downloading modules or themes using wget or FTP, you can add them using composer. For example, to add the devel module you would run this command: $ composer require drupal/devel And then commit just the changes to composer.json and composer.lock to your repository. That also means that to get a working copy of your site locally you will need to run composer install to download all of the necessary libraries and modules. We also strongly recommend installing the Platform.sh Config Reader library, which simplifies access to the Platform.sh environment. The rest of this documentation assumes you have it installed. $ composer require platformsh/config-reader Note: When using Composer, your docroot where most of Drupal lives will be called web, but the vendor directory will be outside of that directory in contrast to how a standard Drupal download .tar.gz file is organized. The config export directory will also be outside of the web root. This is normal, expected, and more secure. File organization Your repository should be laid out as follows: composer.json composer.lock config/ sync/ \u003cthis is where exported configuration will go\u003e drush/ .git/ .gitignore .platform/ routes.yaml services.yaml .platform.app.yaml scripts/ web index.php ... (other Drupal core files) modules/ contrib/ \u003cempty until composer runs\u003e custom/ \u003cyour custom modules here\u003e themes/ contrib/ \u003cempty until composer runs\u003e custom/ \u003cyour custom themes here\u003e sites/ default/ settings.php settings.platformsh.php Changes to settings.php Platform.sh exposes database configuration, as well as other configuration values such as a hash salt, to PHP as environment variables available either via $_ENV or getenv(). That means you’ll need to tell Drupal how to get that information. Additionally, Drupal needs to be told where the config export directory is, where the private files directory is (which is outside of the web root), and so on. The easiest way to access that information is via a small configuration add-on we provide. See our recommended settings.php file , which includes a file called settings.platformsh.php . The latter maps all Platform.sh-provided environment values to Drupal settings, either the Drupal database array or the global $settings object. If run on a non-Platform.sh server this file does nothing so it is safe to always include. If you need to add additional Platform.sh-specific configuration, such as to enable a Redis server for caching, we recommend also putting it into settings.platformsh.php. Vanilla Drupal 8 If you prefer, Drupal 8 can also be installed “vanilla” from Drupal.org, with the entire site checked into the repository. While not recommended it is fully supported. At the end of the day Platform.sh doesn’t care where your files come from, just that you tell the system where they are! You will still need to put the Drupal docroot in a subdirectory of your repository. We recommend web for consistency but any directory name will do. If using a vanilla Drupal install, your repository should look something like this: .git/ .gitignore config/ sync/ .platform/ routes.yaml services.yaml .platform.app.yaml web/ index.php ... (other Drupal core files) core/ modules/ sites/ sites/ default/ settings.php settings.platformsh.php Note the settings.php and settings.platformsh.php files. Both should be identical to the ones used for a Composer-based site. Also note that the config/sync directory is still outside the docroot. That is recommended for all Drupal installs generally, and is configured by the settings.php file. Configuring Platform.sh for Drupal The ideal .platform.app.yaml file will vary from project to project, and you are free to customize yours as needed. A recommended baseline Drupal 8 configuration is listed below, and can also be found in our Drupal 8 template project . # This file describes an application. You can have multiple applications# in the same project.## See https://docs.platform.sh/user_guide/reference/platform-app-yaml.html# The name of this app. Must be unique within a project.name:'app'# The runtime the application uses.type:'php:7.4'# The relationships of the application with services or other applications.## The left-hand side is the name of the relationship as it will be exposed# to the application in the PLATFORM_RELATIONSHIPS variable. The right-hand# side is in the form `\u003cservice name\u003e:\u003cendpoint name\u003e`.relationships:database:'db:mysql'redis:'cache:redis'# The size of the persistent disk of the application (in MB).disk:2048# The 'mounts' describe writable, persistent filesystem mounts in the application.mounts:'/web/sites/default/files':source:localsource_path:'files''/tmp':source:localsource_path:'tmp''/private':source:localsource_path:'private''/.drush':source:localsource_path:'drush''/drush-backups':source:localsource_path:'drush-backups''/.console':source:localsource_path:'console'# Configuration of the build of this application.build:flavor:composer# The hooks executed at various points in the lifecycle of the application.hooks:# The build hook runs after Composer to finish preparing up your code.build:| set -ebashinstall-redis.sh4.3.0# The deploy hook runs after your application has been deployed and started.deploy:| set -ephp./drush/platformsh_generate_drush_yml.phpcdwebdrush-ycache-rebuilddrush-yupdatedbdrush-yconfig-import# The configuration of app when it is exposed to the web.web:# Specific parameters for different URL prefixes.locations:'/':# The folder from which to serve static assets, for this location.## This is a filesystem path, relative to the application root.root:'web'# How long to allow static assets from this location to be cached.## Can be a time in seconds, or -1 for no caching. Times can be# suffixed with  s  (seconds),  m  (minutes),  h  (hours),  d # (days),  w  (weeks),  M  (months, as 30 days) or  y  (years, as# 365 days).expires:5m# Whether to forward disallowed and missing resources from this# location to the application.## Can be true, false or a URI path string.passthru:'/index.php'# Deny access to static files in this location.allow:false# Rules for specific URI patterns.rules:# Allow access to common static Deny direct access to configuration Allow access to all files in the public files directory.allow:trueexpires:5mpassthru:'/index.php'root:'web/sites/default/files'# Do not execute PHP scripts.scripts:falserules:# Provide a longer TTL (2 weeks) for aggregated CSS and JS files.'^/sites/default/files/(css|js)':expires:2w# The configuration of scheduled execution.crons:drupal:spec:'*/20 * * * *'cmd:'cd web ; drush core-cron'",
        "section": "Featured frameworks",
        "subsections": " Drupal 8 and Composer  File organization Changes to settings.php   Vanilla Drupal 8 Configuring Platform.sh for Drupal  ",
        "image": "",
        "url": "/frameworks/drupal8.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "9dde4f32b7dc0685d7cca3d1af46ca85",
        "title": "Going Live - Steps",
        "description": "",
        "text": " Going live on Platform.sh is a simple two or three step process. You can either use the Platform.sh management console or the CLI to configure your project for production. Once you have gone through it once the whole process usually takes a couple of minutes. Note: The order of operations is not really important, but if you are migrating a site from an existing provider, you should first configure the domain on the Platform.sh side, and only then switch DNS over. 1. Change your plan to a production plan If you are on a Development plan, you cannot add a domain. You will need to upgrade your subscription to a production plan. Go to your account , click on the small wheel next to you project’s name and click on edit. You can also access information about the project’s plan under “Billing”, and then by selecting the project from your list of projects. You can make changes to the project by clicking ‘Upgrade Plan’. You can make changes to the type of plan, the number of environments, amount of storage and number of users here. When you make changes, it will update the monthly price you will be paying. Click Upgrade plan to save the new settings. You can find more information on pricing on the pricing page . 2. (CDN version) Configure your DNS provider If you are serving the site through a CDN, configure your DNS provider to point at your CDN account. The address or CNAME to set for that will vary with the CDN provider. Refer to their documentation or to the CDN guide . 2. (Non-CDN version) Configure your DNS provider Configure your DNS provider to point your domain to your Platform.sh Master environment domain name. The way to do so will vary somewhat depending on your registrar, but nearly all registrars should allow you to set a CNAME. Some will call it an Alias or similar alternate name, but either way the intent is to say “this domain should always resolve to… this other domain”. You can access the CNAME target by running platform environment:info edge_hostname. That is the host name by which Platform.sh knows your environment. Add a CNAME record from your desired domain (www.example.com) to the value of the edge_hostname. If you have multiple domains you want to be served by the same application you will need to add a CNAME record for each of them. Note that depending on your registrar and the TTL you set, it could take anywhere from 15 minutes to 72 hours for the DNS change to fully propagate across the Internet. If you are using an apex domain (example.com), see the additional information about Apex domains and CNAME records . 3. (Non-CDN version) Set your domain in Platform.sh Note: If using a CDN, skip this step. The CDN should already have been configured in advance to point to Platform.sh as its upstream. This step will tell the Platform.sh edge layer where to route requests for your web site. You can do this through the CLI with platform domain:add example.com or using the managment console . You can add multiple domains to point to your project. Each domain can have its own custom SSL certificate, or use the default one provided. If you require access to the site before the domain name becomes active you can create a hosts file entry on your computer and point it to the IP address that resolves when you access your master project branch. To get the IP address, first run platform environment:info edge_hostname. That will print the “internal” domain name for your project. Run ping \u003cthat domain name\u003e to get its IP address. In OS X and Linux you can add that IP to your /etc/hosts file. In Windows the file is named You will need to be a admin user to be able to change that file. So in OS X you will usually run something like sudo vi /etc/hosts. After adding the line the file will look something like: Alternatively there is also an add-on for Firefox and Google Chrome that allow you to dynamically switch DNS IP addresses without modifying your hosts file. Firefox LiveHosts add-on Google Chrome LiveHosts add-on Note: Do not put the IP address you see here, but the one you got from the ping command. Also, remember to remove this entry after you have configured DNS! Sometimes it can take Let’s Encrypt a couple of minutes to provision the certificate the first time. This is normal, and only means the first deploy after enabling a domain may take longer than usual. Setting the CNAME record with your DNS provider first helps to minimize that disruption. 4. Bonus steps (Optional) Configure health notifications While not required, it’s strongly recommended that you set up health notifications to advise you if your site is experiencing issues such as running low on disk space. Notifications can be sent via email, Slack, or PagerDuty. Configure production cron tasks It’s strongly recommended that you set up automatic backups and automatic certificate renewal cron tasks. You will first need to set up an API token and install the CLI as part of the build hook. Then you can easily configure the appropriate cron tasks. The following snippet is generally sufficient but see the the links above for more details, and please modify the cron schedules listed to match your use case. crons:backup:# Take a backup automatically every night at 3 am (UTC).spec:'0 3 * * *'cmd:| if [  $PLATFORM_BRANCH  = master ]; thenplatformbackup:create--yes--no-waitfirenewcert:# Force a redeploy at 8 am (UTC) on the 14th and 28th of every month.spec:'0 8 14,28 * *'cmd:| if [  $PLATFORM_BRANCH  = master ]; thenplatformredeploy--yes--no-waitfi",
        "section": "Going live",
        "subsections": " 1. Change your plan to a production plan 2. (CDN version) Configure your DNS provider 2. (Non-CDN version) Configure your DNS provider 3. (Non-CDN version) Set your domain in Platform.sh 4. Bonus steps (Optional)  Configure health notifications Configure production cron tasks    ",
        "image": "",
        "url": "/golive/steps.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "37db2162f2d40f81c95d41bdee7fb3fa",
        "title": "Headless Chrome",
        "description": "",
        "text": " Headless Chrome is a headless browser that can be configured on projects like any other service on Platform.sh. You can interact with the headless-chrome service container using Puppeteer, a Node.js library that provides an API to control Chrome over the DevTools Protocol. Puppeteer can be used to generate PDFs and screenshots of web pages, automate form submission, and test your project’s UI. You can find out more information about using Puppeteer on GitHub or in their documentation . Supported versions Grid Dedicated 73 None available Relationship The format exposed in the $PLATFORM_RELATIONSHIPS environment variable : { service :  headless , ip :  169.254.73.96 , hostname :  3rxha4e2w4yv36lqlypy7qlkza.headless.service._.eu-3.platformsh.site , cluster :  moqwtrvgc63mo-master-7rqtwti , host :  headless.internal , rel :  http , scheme :  http , type :  chrome-headless:73 , port : 9222}Requirements Puppeteer requires at least Node.js version 6.4.0, while using the async and await examples below requires Node 7.6.0 or greater. Using the Platform.sh Config Reader library requires Node.js 10 or later. Other languages It will be necessary to upgrade the version of Node.js in other language containers before using Puppeteer. You can use Node Version Manager or NVM to change or update the version available in your application container by following the instructions in the Alternate Node.js install documentation. Usage example In your .platform/services.yaml: headlessbrowser:type:chrome-headless:73 In your .platform.app.yaml: relationships:chromeheadlessbrowser: headlessbrowser:http  Note: You will need to use the chrome-headless type when defining the service # .platform/services.yamlservice_name:type:chrome-headless:version and the endpoint http when defining the relationship # .platform.app.yamlrelationships:relationship_name:“service_name:http” Your service_name and relationship_name are defined by you, but we recommend making them distinct from each other. After configuration, include Puppeteer as a dependency in your package.json: {  dependencies : {  puppeteer :  ^1.14.0  } } Using the Node.js Config Reader library, you can retrieve formatted credentials for connecting to headless Chrome with Puppeteer: const platformsh = require(\u0026#39;platformsh-config\u0026#39;); let config = platformsh.config(); const credentials = config.credentials(\u0026#39;chromeheadlessbrowser\u0026#39;); and use them to define the browserURL parameter of puppeteer.connect() within an async function: exports.takeScreenshot = async function (url) { try { // Connect to chrome-headless using pre-formatted puppeteer credentials const formattedURL = config.formattedCredentials(\u0026#39;chromeheadlessbrowser\u0026#39;, \u0026#39;puppeteer\u0026#39;); const browser = await puppeteer.connect({browserURL: formattedURL}); ... return browser } catch (e) { return Promise.reject(e); } }; Puppeteer allows your application to create screenshots , emulate a mobile device , generate PDFs , and much more. You can find some useful examples of using headless Chrome and Puppeteer on Platform.sh on the Community Portal: How to take screenshots using Puppeteer and Headless Chrome How to generate PDFs using Puppeteer and Headless Chrome",
        "section": "Configure services",
        "subsections": " Supported versions Relationship Requirements  Other languages   Usage example  ",
        "image": "",
        "url": "/configuration/services/headless-chrome.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "44c7c836fce4755546271a721b6f6849",
        "title": "HTTP cache",
        "description": "",
        "text": " Platform.sh supports HTTP caching at the server level. Caching is enabled by default, but is only applied to GET and HEAD requests. The cache can be controlled using the cache key in your .platform/routes.yaml file. If a request is cacheable, Platform.sh builds a cache key from several request properties and stores the response associated with this key. When a request comes with the same cache key, the cached response is reused. When caching is on… you can configure cache behaviour for different location blocks in your .platform.app.yaml; the router will respect whatever cache headers are sent by the application; cookies will bypass the cache; responses with the Cache-Control header set to Private, No-Cache, or No-Store are not cached. Basic usage The HTTP cache is enabled by default, however you may wish to override this behaviour. To configure the HTTP cache, add a cache key to your route in .platform/routes.yaml. You may like to start with the defaults: https://{default}/:type:upstreamupstream:app:httpcache:enabled:truedefault_ttl:0cookies:[\u0026#39;*\u0026#39;]headers:[\u0026#39;Accept\u0026#39;,\u0026#39;Accept-Language\u0026#39;]Example In this example, requests will be cached based on the URI, the Accept header, Accept-Language header, and X-Language-Locale header; Any response that lacks a Cache-Control header will be cached for 60 seconds; and the presence of any cookie in the request will disable caching of that response. https://{default}/:type:upstreamupstream:app:httpcache:enabled:trueheaders:[\u0026#39;Accept\u0026#39;,\u0026#39;Accept-Language\u0026#39;,\u0026#39;X-Language-Locale\u0026#39;]cookies:[\u0026#39;*\u0026#39;]default_ttl:60How it works The cache key If a request is cacheable, Platform.sh builds a cache key from several request properties and stores the response associated with this key. When a request comes with the same cache key, the cached response is reused. There are two parameters that let you control this key: headers and cookies. The default value for these keys are the following: cache:enabled:truecookies:[\u0026#39;*\u0026#39;]headers:[\u0026#39;Accept\u0026#39;,\u0026#39;Accept-Language\u0026#39;]Duration The cache duration is decided based on the Cache-Control response header value. If no Cache-Control header is in the response, then the value of default_ttl key is used. Conditional requests Conditional requests using If-Modified-Since and If-None-Match are both supported. Our web server does not honor the Pragma request header. Cache revalidation When the cache is expired (indicated by Last-Modified header in the response) the web server will send a request to your application with If-Modified-Since header. If the If-None-Match header is sent in the conditional request when Etag header is set in the cached response, your application can extend the validity of the cache by replying HTTP 304 Not Modified. Flushing The HTTP cache does not support a complete cache flush, however, you can invalidate the cache by setting cache:false. Cache configuration properties enabled Turns the cache on or off for a route. Type: Boolean Required: Yes Values true: enable the cache for this route [default, but only if the cache key is not actually specified] false: disable the cache for this route headers Adds specific header fields to the cache key, enabling caching of separate responses for those headers. For example, if the headers key is the following, Platform.sh will cache a different response for each value of the Accept HTTP request header only: cache:enabled:trueheaders:[ Accept ] Type: List Values: [\u0026#39;Accept\u0026#39;, \u0026#39;Accept-Language\u0026#39;]: Cache on Accept \u0026amp; Accept-Language [default] Header behaviors The cache is only applied to GET and HEAD requests. Some headers trigger specific behaviours in the cache. Header field Cache behavior Cache-Control Responses with the Cache-Control header set to Private, No-Cache, or No-Store are not cached. All other values override default_ttl. Vary A list of header fields to be taken into account when constructing the cache key. Multiple header fields can be listed, separted by commas. The Cache key is the union of the values of the Header fields listed in Vary header, and whatever’s listed in the routes.yaml file. Set-Cookie Not cached Accept-Encoding, Connection, Proxy-Authorization, TE, Upgrade Not allowed, and will throw an error Cookie Not allowed, and will throw an error. Use the cookies value, instead. Pragma Ignored A full list of HTTP headers is available on Wikipedia . cookies A whitelist of cookie names to include values for in the cache key. All cookies will bypass the cache when using the default ([\u0026#39;*\u0026#39;]) or if the Set-Cookie header is present. For example, for the cache key to depend on the value of the foo cookie in the request. Other cookies will be ignored. cache:enabled:truecookies:[ foo ] Type: List Values: [\u0026#39;*\u0026#39;]: any request with a cookie will bypass the cache [default] []: Ignore all cookies [\u0026#39;cookie_1\u0026#39;,\u0026#39;cookie_2\u0026#39;]: A whitelist of cookies to include in the cache key. All other cookies are ignored. A cookie value may also be a regular expression. An entry that begins and ends with a / will be interpreted as a PCRE regular expression to match the cookie name. For example: cache:enabled:truecookies:[\u0026#39;/^SS?ESS/\u0026#39;]Will cause all cookies beginning with SESS or SSESS to be part of the cache key, as a single value. Other cookies will be ignored for caching. If your site uses a session cookie as well as 3rd party cookies, say from an analytics service, this is the recommended approach. default_ttl Defines the default time-to-live for the cache, in seconds, for non-static responses, when the response does not specify one. The cache duration is decided based on the Cache-Control response header value. If no Cache-Control header is in the response, then the value of default_ttl is used. If the application code returns a Cache-Control header or if your .platform.app.yaml file is configured to set a cache lifetime, then this value is ignored in favor of the application headers. The default_ttl only applies to non-static responses, that is, those generated your application. To set a cache lifetime for static resources configure that in your .platform.app.yaml file. All static assets will have a Cache-Control header with a max age defaulting to 0 (which is the default for expires in the .platform.app.yaml). Type: integer Values: 0: Do not cache [default]. This prevents caching, unless the response specifies a Cache-Control header value. Debugging Platform.sh adds an X-Platform-Cache header to each request which show whether your request is a cache HIT, MISS or BYPASS. This can be useful when trying to determine whether it is your application, the HTTP cache, or another proxy or CDN which is not behaving as expected. If in doubt, disable the cache using cache:false. Advanced caching strategies Cache per route If you need fine-grained caching, you can set up caching rules for several routes separately: https://{default}/:type:upstreamupstream:app:httpcache:enabled:truehttps://{default}/foo/:type:upstreamupstream:app:httpcache:enabled:falsehttps://{default}/foo/bar/:type:upstreamupstream:app:httpcache:enabled:trueWith this configuration, the following routes are cached: https://{default}/ https://{default}/foo/bar/ https://{default}/foo/bar/baz/ And the following routes are not cached: https://{default}/foo/ https://{default}/foo/baz/ Note: Regular expressions in routes are not supported. Allowing only specific cookies Some applications use cookies to invalidate cache responses, but expect other cookies to be ignored. This is a simple case of allowing only a subset of cookies to invalidate the cache. cache:enabled:truecookies:[ MYCOOKIE ]Cache HTTP and HTTPS separately using the Vary header Set the Vary header to X-Forwarded-Proto custom request header to render content based on the request protocol (i.e. HTTP or HTTPS). By adding Vary: X-Forwarded-Proto to the response header, HTTP and HTTPS content would be cached separately. Cache zipped content separately Use Vary: Accept-Encoding to serve different content depending on the encoding. Useful for ensuring that gzipped content is not served to clients that can’t read it.",
        "section": "Configure routes",
        "subsections": " Basic usage Example How it works  The cache key Duration Conditional requests Cache revalidation Flushing   Cache configuration properties  enabled headers cookies default_ttl   Debugging Advanced caching strategies  Cache per route Allowing only specific cookies Cache HTTP and HTTPS separately using the Vary header Cache zipped content separately    ",
        "image": "",
        "url": "/configuration/routes/cache.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "0a9e34befd7f9cc9e291e8a1690857c9",
        "title": "Import your own code",
        "description": "",
        "text": " Welcome to Platform.sh! Importing your own code to Platform.sh is as easy as installing the CLI and configuring your application with a few YAML files. When code examples are provided in the guide, click the language of your application. If you consult those examples and a few templates as you go along, your code will be up and running on Platform.sh in no time. Get started!",
        "section": "Introduction",
        "subsections": "",
        "image": "",
        "url": "/gettingstarted/gettingstarted/own-code.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "dbb0e89777d8d38b2e967257e06d456b",
        "title": "One site or many",
        "description": "",
        "text": " Platform.sh supports running multiple “application containers” in a single project. That can be extremely powerful in some cases, but if misused can lead to unnecessary maintenance difficulty and excessive costs. The way to determine what setup is appropriate for your use case is to think of your project as a collection of services, some of which you’ve written yourself. That is, put “your code” and “the database” on the same level. (That is essentially true from the Platform.sh perspective.) Does your project consist of multiple “your code” pieces, but they all are parts of the same project? Or are they discrete applications that conceptually exist independently of each other? Discrete projects If your applications are discrete systems that are only incidentally related (such as because you wrote both of them), make them separate projects. That will provide the most flexible development workflow. It will also be cheaper, as running multiple applications in a single project requires at least a Medium plan, which costs more than two Standard plans. Discrete projects are appropriate if: You want to deploy new releases of each application independently of the others. The projects are for different customers/clients. The projects do not need deep internal knowledge of each other’s data. Different teams will be working on different applications. You want to develop true-microservices, where each microservice is fully stand-alone process with its own data. If you are uncertain how your needs map to projects, it probably means they should be separate, discrete projects. Clustered applications A clustered application is one where your project requires multiple “app services”, written by you, but are all part of the same conceptual project. That is, removing one of the app services would render the others broken. In a clustered application, you either have multiple .platform.app.yaml files in different directories with separate code bases that deploy separately or you have a single application that spawns one or more worker instances that run background processes. (See the link for details on how to set those up.) A Clustered application requires at least a Medium plan. With a clustered application, you often will not need multiple service instances. The MySQL, MariaDB , and Solr services support defining multiple databases on a single service, which is significantly more efficient than defining multiple services. Redis , Memcached , Elasticsearch , and RabbitMQ natively support multiple bins, or queues, or indexes (the terminology varies) defined by the client application as part of the request so they need no additional configuration on Platform.sh, although they may need application configuration. Clustered applications are appropriate if: You want one user-facing application and an entirely separate admin-facing application that are both operating on the same data. You want to have a user-facing application and a separate worker process (either the same code or separate) that handles background tasks. You want a single conceptual application written in multiple programming languages, such as a PHP frontend with Node.js background worker. Multi-site applications Some Content Management Systems or other applications support running multiple logical “sites” off of a single code base. Those will usually work on Platform.sh depending on the configuration details of the application but are generally not recommended. Often their multi-site logic is dependant on the domain name of the incoming request, which on Platform.sh will vary by branch. They also often recommend running multiple databases, which while supported just fine on Platform.sh makes the setup process for each site more difficult. Leveraging multi-site capabilities of an application are appropriate if, and only if: There is only a single team working on all of the “sites” involved. All “sites” should be updated simultaneously as a single unit. Each individual site is relatively low traffic, such that the aggregate traffic is appropriate for your plan size. All sites really do use the same codebase with no variation, just different data. If any of those is not the case, discrete projects will be a better long term plan.",
        "section": "Best practices",
        "subsections": " Discrete projects Clustered applications Multi-site applications  ",
        "image": "",
        "url": "/bestpractices/oneormany.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "613a2e6671a11c75801a0de3a535e6d6",
        "title": "Performance tuning Java",
        "description": "",
        "text": " There are a number of settings that can be adjusted for each application to optimize its performance on Platform.sh. Memory limits The JVM generally requires specifying a maximum memory size it is allowed to use, using the Xmx parameter. That should be set based on the available memory on the application container, which will vary with its size. To extract the container-scaled value on the command line, use $(jq .info.limits.memory /run/config.json). You should also set the ExitOnOutOfMemoryError. When you enable this option, the JVM exits on the first occurrence of an out-of-memory error. Platform.sh will restart the application automatically. These are the recommended parameters for running a Java application. Thus, the command to use to start a Java application is: java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:\u0026#43;ExitOnOutOfMemoryError //The rest of the arguments and the jar file. Garbage collection When migrating the application to a cloud environment, it is often essential to analyze the Garbage Collector’s log and behavior. For this, there are two options: Placing the log into the Platform.sh /var/log/app.log file (which captures STDOUT). Creating a log file specifically for the GC. To use the STDOUT log, you can add the parameter -XX: \u0026#43; PrintGCDetails, E.g.: java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:\u0026#43;ExitOnOutOfMemoryError -XX:\u0026#43;PrintGCDetails //The rest of the arguments and the jar file. Java supports a number of different garbage collection strategies. Which one is optimal for your application will vary depending on your available memory, Java version, and application profile. Determining which is best for your application is out of scope, but the main options and how to enable them are: Name Command Flag Description Serial Garbage Collector -XX:\u0026#43;UseSerialGC This is the simplest GC implementation, as it basically works with a single thread. Parallel Garbage Collector -XX:\u0026#43;UseParallelGC Unlike Serial Garbage Collector, this uses multiple threads for managing heap space. But it also freezes other application threads while performing GC. CMS Garbage Collector -XX:\u0026#43;USeParNewGC The Concurrent Mark Sweep (CMS) implementation uses multiple garbage collector threads for garbage collection. It’s for applications that prefer shorter garbage collection pauses, and that can afford to share processor resources with the garbage collector while the application is running. G1 Garbage Collector -XX:\u0026#43;UseG1GC Garbage First, G1, is for applications running on multiprocessor machines with large memory space. The default strategy on Java 9 and later is G1. The GC strategy to use can be set in the start line with: Serial java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:\u0026#43;ExitOnOutOfMemoryError -XX:\u0026#43;PrintGCDetails -XX:\u0026#43;UseSerialGC //The rest of the arguments and the jar file. Parallel Garbage Collector java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:\u0026#43;ExitOnOutOfMemoryError -XX:\u0026#43;PrintGCDetails -XX:\u0026#43;UseParallelGC //The rest of the arguments and the jar file. CMS Garbage Collector java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:\u0026#43;ExitOnOutOfMemoryError -XX:\u0026#43;PrintGCDetails -XX:\u0026#43;USeParNewGC //The rest of the arguments and the jar file. G1 java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:\u0026#43;ExitOnOutOfMemoryError -XX:\u0026#43;PrintGCDetails -XX:\u0026#43;UseG1GC //The rest of the arguments and the jar file. Java 8 Optimization Ideally, all applications should run the latest LTS release of the JVM at least. That is currently Java 11. Java 11 has a number of performance improvements, particularly on container-based environments such as Platform.sh. However, in many cases, this is not possible. If you are still running on Java 8 there are two additional considerations. The default garbage collector for Java 8 is Parallel GC. In most cases G1 will offer better performance. We recommend enabling it, as above. Furthermore, there is the UseStringDeduplication flag which works to eliminate duplicate Strings within the GC process. That flag can save between 13% to 30% of memory, depending on application. However, this can impact on the pause time of your app. java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:\u0026#43;UseG1GC -XX:\u0026#43;UseStringDeduplication -XX:\u0026#43;ExitOnOutOfMemoryError -XX:\u0026#43;PrintGCDetails References Java Memory Commands How to Migrate my Java application to Platform.sh Garbage Collector Log Introduction to Garbage Collection Tuning",
        "section": "Java",
        "subsections": " Memory limits Garbage collection  Serial Parallel Garbage Collector CMS Garbage Collector G1   Java 8 Optimization References  ",
        "image": "",
        "url": "/languages/java/tuning.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "e88f1bf9c1df6ed5f5b9c384ec4e281d",
        "title": "Platform.sh Dedicated cluster specifications",
        "description": "",
        "text": " Platform.sh Dedicated clusters are launched into a Triple Redundant configuration consisting of 3 virtual machines (VMs). This is an N+1 configuration that is sized to withstand the total loss of any one of the 3 members of the cluster without incurring any downtime. Each instance hosts the entire application stack, allowing this architecture superior fault tolerance to traditional N-Tier installations. Moreover, the Cores assigned to production are solely for production. Storage Each Dedicated cluster comes with 50GB of storage per environment by default. This storage is intended for customer data - databases, search indexes, user uploaded files, etc. - and can be subdivided in any way that the customer wishes. 50GB is only the default amount; more storage can be added easily as a line item in the contract and can be added at anytime that the project requires: at contract renewal or at any point in the term. Default storage is based on the default SSD block-storage offering for each cloud. Extra provisioned IOPS can be discussed with your sales representative. Accessing services Your application will be able to connect to each service by referencing the exact same environment variables as a Grid environment. While the configuration of the service will be performed by our team, the application configuration is the same and your code should be the same. See the services documentation for service-specific details. Note that not all services and languages are available in a Dedicated environment.",
        "section": "Dedicated",
        "subsections": " Storage Accessing services  ",
        "image": "",
        "url": "/dedicated/architecture.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "e0ef22ff09aeff7b1299d16840505e38",
        "title": "Platform.sh environments",
        "description": "",
        "text": " Platform.sh helps a coder with the development workflow by making it easy to manage multiple environments, including the Master environment which runs the production website. It’s precisely like a “development” or a “staging” server, except they are created on the fly, and they are absolutely identical copies of their parent environments. An environment is tied to a Git branch, plus all the services that are serving that branch. You can see that as a complete working website. With Bitbucket and GitHub integrations you can even get a “development server” for each and every pull request. You can have branches that are not tied to a running instance of your application; these are what we call “inactive environments”. Master environment Every Platform.sh project starts with a Master environment which corresponds to the Master branch in Git. If you subscribed to a production plan, this environment is your live site and can be mapped to a domain name and a custom SSL certificate. Note: Your project must have a master branch: it will not function properly without one. Hierarchy Platform.sh brings the concept of a hierarchy between your environments. Each new environment you create is considered a child of the parent environment from which it was branched. Each child environment can sync code and/or data down from its parent, and merge code up to its parent. These are used for development, staging, and testing. When you create a branch or child environment through the Platform.sh management console the branch it was made from will be treated as the parent. If you create a branch through your local Git checkout and push it to Platform.sh, or synchronize a branch from a 3rd party such as GitHub or Bitbucket, its parent will default to the master branch. Any environment’s parent can be changed using the Platform.sh CLI with the following command: platform environment:info parent NEW_PARENT In this case, the current environment (the branch you’re on) will be set to have NEW_PARENT as its parent environment. The environment to reparent can be set explicitly with the -e option: platform environment:info -e feature-x parent NEW_PARENT Workflows Since you can organize your environments as you want, you have complete flexibility to create your own workflows. There are no rules you must follow when branching the master environment. You simply need a structure that best fits your workflow: Agile: a child environment per sprint. Each story in the sprint can have its own environment as a child of the sprint environment. Developer-centric: one QA environment and a few development environments (per developer, per task…). Testing: an operational test environment, a user test environment and a few unit test environments. Hotfix: one environment for every bug, security, or hotfix that needs deployment. Here is an example of a possible Agile workflow. The administrator creates a Sprint environment and gives each of the developers permission to create new feature environments. Another approach is that the administrator could create an environment for each developer. As a feature is completed, the administrator can review the work by accessing the website of the feature environment. The new feature is then merged back into the Sprint environment. The remaining features will sync with the Sprint environment to ensure their working environment is up-to-date with the latest code. When the objectives of the sprint are complete, the administrator can then make a backup of the live site, then merge the Sprint environment into the live (Master) environment. The administrator can then synchronize the next sprint’s environment with data from the live (Master) environment to repeat and continue the development process. Naming conventions Platform.sh provides great flexibility on the way you can organize and work with your development environments. To improve readability and productivity, it’s important to think carefully about how to name and structure those environments. The name should represent the purpose of the environment. Is it a Staging site to show to your client? Is it an implementation of a new feature? Is it a hotfix? If you use Agile, for example, you could create hierarchical environments and name them like this: Sprint1 Feature1 Feature2 Feature3 Sprint2 Feature1 Feature2 ... If you prefer splitting your environments per developer and having a specific environment per task or per ticket, you could use something like this: Staging Developer1 Ticket-526 Ticket-593 Developer2 Ticket-395 ...",
        "section": "Management console",
        "subsections": " Master environment Hierarchy Workflows Naming conventions  ",
        "image": "",
        "url": "/administration/web/environments.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "af754b130a377bea960ed8e2f6ec4e9e",
        "title": "Requirements for the CLI",
        "description": "",
        "text": " Now that you have created your free trial account, you are able to push your application to Platform.sh once you have installed the CLI, but there are a few requirements that must be met first. Git Git is the open source version control system that is utilized by Platform.sh. Any change you make to your Platform.sh project will need to be committed via Git. You can see all the Git commit messages of an environment in the Environment Activity feed of the management console for each project you create. Before getting started, make sure you have Git installed on your computer. SSH key pair Once your account has been set up and the CLI is installed, Platform.sh needs one additional piece of information about your computer so that you can access your projects from the command line. If you are unfamiliar with how to generate an RSA public and private key, there are excellent instructions in the documentation about how to do so . Add your SSH key to your account Now that you have the requirements out of the way, place your SSH key onto Platform.sh so that you can communicate with your projects from your computer using the management console. Log in to your account Access SSH key settings in the management console From the management console, move to the top right hand corner of the screen and click the dropdown menu to the left of the settings gear box icon. In the menu, click on Account. The next page will normally list all of your projects, which at this point will be empty if you’re just starting out. Click on the Account Settings link at the top of the page, then click the SSH keys tab to the left of your account information. Add your SSH key to your account At this point you won’t see anything listed in the body of the page, because you don’t have SSH configured with Platform.sh yet. Click the \u0026#43; Add public key button in the top right hand corner of the screen. This will open up another window with two fields. Name the key with something memorable, like home-computer, and in the field below that, paste the content of the public key you created in the previous step. When you have finished, click Save to save the key. That’s it! Now that you have met the requirements and configured an SSH key, Platform.sh can authenticate your computer and you can interact with your project from the command line. Next, you will need to install the Platform.sh CLI so that you can import your code to a project. Back I\u0026#39;ve added my public SSH key",
        "section": "Getting started",
        "subsections": "   Git SSH key pair Add your SSH key to your account    ",
        "image": "",
        "url": "/gettingstarted/gettingstarted/own-code/cli-requirements.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "4636e25c794d2ede787bcaeaf6199672",
        "title": "Resource and incident monitoring",
        "description": "",
        "text": " All of our Dedicated clusters are monitored 24/7 to ensure uptime and to measure server metrics such as available disk space, memory and disk usage, and several dozen other metrics that give us a complete picture of the health of your application’s infrastructure. Alerting is set up on these metrics, so if any of them goes outside of normal bounds an operations engineer can react accordingly to maintain the uptime and performance of your cluster. These alerts are sent to our support and operations teams, and are not directly accessible to the customer. Monitoring systems Platform.sh uses well-known open source tooling to collect metrics and to alert our staff if any of these metrics goes out of bounds. That includes the use of Munin for collecting time-series data on server metrics, and dashboarding of these metrics so that our team can monitor trends over time. It also includes use Nagios as a point in time alerting system for our operations staff. These tools are internal Platform.sh tools only. A third-party availability monitoring system is configured for every Dedicated project. The customer can be subscribed to email alerts upon request. Application performance monitoring Platform.sh does not provide application-level performance monitoring. However, we strongly recommend that customers leverage application monitoring themselves. Platform.sh is a Blackfire.io reseller. You can contact your sales representative to get a quote for whatever size cluster is running your application. Platform.sh also supports New Relic APM . After you have signed up with New Relic and gotten your license key, open a support ticket so that it can be installed on your project. New Relic infrastructure monitoring is not supported. Availability incident handling procedure Automated monitoring may trigger alerts that will page the on-call engineer, or the end-user may file an urgent priority ticket. PagerDuty will page the on-call using several methods. The on-call engineer responds to the alerts and begins to triage the issue. Cloud infrastructure issues are handled by the customer success team. Application problems are escalated to an application support specialist if an agreement is part of the customer subscription. Otherwise, they are returned to the user and may be downgraded. When a Urgent/High issue is escalated it will page the on-call application support specialist. Application support may also escalate infrastructure issues back as Urgent/High.",
        "section": "Platform.sh Dedicated",
        "subsections": " Monitoring systems Application performance monitoring Availability incident handling procedure  ",
        "image": "",
        "url": "/dedicated/overview/monitoring.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "b0be6b730e93a77709ec53ebca895b12",
        "title": "SCA",
        "description": "",
        "text": " In accordance with Article 14(1) of the Commission Delegated Regulation (EU) 2018/389, Platform.sh has made changes in order to comply with and implement Strong Customer Authentication (SCA) for customers using payment methods from the EU. That article states: Payment service providers shall apply strong customer authentication when a payer creates, amends, or initiates for the first time, a series of recurring transactions with the same amount and with the same payee. SCA is part of the second Payment Services Directive (PSD2) , acting as a regulatory requirement to reduce fraud and to make online transactions more secure. The law went into affect September 14, 2019, and European card holders will be required to go through an additional re-authentication step within the Platform.sh management console starting October 1, 2019 in order to authenticate recurring payments with your payment institution. Prior to October 1, 2019, Platform.sh projects associated with an EU credit card will see a banner at the top of the management console. That banner will direct you to authenticate your payment information settings. The process after you click “Authenticate” will vary according to your payment institution. In most cases, having your phone number registered with that institution will be sufficient to enable 2FA with them from here. After October 1, 2019, the SCA banner in the management console will become a warning for you to update your payment settings. There will be a grace period in the first few weeks following that change, but when that period has ended projects that have not set up payment authentication through the console will be suspended, so it is important for you to do so as soon as possible.",
        "section": "Security and compliance",
        "subsections": "",
        "image": "",
        "url": "/security/sca.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "5d645502da3c569d07ef810d7bf6bde4",
        "title": "Security \u0026 data privacy",
        "description": "",
        "text": " Updates \u0026amp; upgrades Platform.sh updates the core software of the Dedicated Cluster (operating system, web server, PHP, MySQL, etc.) periodically, and after any significant security vulnerability is disclosed. These updates are deployed automatically with no additional work required by the user. We attempt to maintain parity with your development environment, but we do not guarantee absolute parity of point versions of your Dedicated environments with their corresponding development environments. I.e, your development environment may have a PHP container running 5.6.30, but your production environment may lag behind at 5.6.22. We can upgrade point releases on request and will always upgrade the underlying software in the event of security release. Updates to application software (PHP code, Javascript, etc.) are the responsibility of the customer. Project isolation All Dedicated Clusters are single-tenant. The three VMs are exclusively used by a single customer and each Dedicated cluster is launched into its own isolated network (VPC on AWS, equivalent on other providers). The network is firewalled to incoming connections; only ports 22 (SSH), 80 (HTTP), 443 (HTTPS), 2221 (SFTP) are opened to incoming traffic. There are no exceptions for this rule, so any incoming web service requests, ETL jobs, or otherwise will need to transact over one of these protocols. Outgoing TCP traffic is not firewalled. Outgoing UDP traffic is disallowed. The Development Environment deploys each branch as a series of containers hosted on a shared underlying VM. Many customers will generally share the same VM. However, all containers are whitelisted to connect only to other containers in their same environment, and even then only if an explicit “relationship” has been defined by the user via configuration file. Security incident handling procedure Should Platform.sh become aware of a security incident — such as an active or past hacking attempt, virus or worm, or data breach — senior personnel including the CTO will be promptly notified. Our security incident procedures include isolating the affected systems, collecting forensic evidence for later analysis including a byte-for-byte copy of the affected system, and finally restoring normal operations. Once normal service is restored we perform a root cause analysis to determine exactly what happened. A Reason for Outage report may be provided to the customer upon request that summarizes the incident, cause, and steps taken. Platform.sh will cooperate with relevant law enforcement, and inform law enforcement in the event of an attempted malicious intrusion. Depending on the type of incident the root cause analysis may be conducted by law enforcement rather than Platform.sh personnel. Platform.sh will endeavor to notify affected customers within 24 hours in case of a personal data breach and 72 hours in case of a project data breach. Under the European General Data Protection Regulation (GPDR), Platform.sh is required to notify our supervising authority within 72 hours of a discovered breach that may result in risk to the rights and freedoms of individuals. Our supervising authority is the French Commission Nationale de l’Informatique et des Libertés . Audit trail As part of the security incident process we record a log of all steps taken to identify, isolate, and respond to the incident. This log may include: A byte-for-byte copy of the affected systems How the intrusion was detected The steps taken to contain the intrusion Any contact with 3rd parties, including law enforcement Any conclusions reached regarding the root cause Encryption AWS AWS EBS Volumes are encrypted on Platform.sh Dedicated sites are fully encrypted. Keys are managed by AWS’s KMS (Key Management Service). AWS automatically rotates these keys every three years. In some cases, temporary storage (eg swap) is stored on unencrypted local storage volumes. Azure By default, data is encrypted using Microsoft Managed Keys for Azure Blobs, Tables, Files and Queues.",
        "section": "Platform.sh Dedicated",
        "subsections": " Updates \u0026amp; upgrades Project isolation Security incident handling procedure Audit trail Encryption  AWS Azure    ",
        "image": "",
        "url": "/dedicated/overview/security.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "a18c123f65c496ecad5d8efb610128e1",
        "title": "Technical Requirements: Git and SSH",
        "description": "",
        "text": " Git Git is the open source version control system that is utilized by Platform.sh. Any change you make to your Platform.sh project will need to be committed via Git. You can see all the Git commit messages of an environment in the activity feed of the management console. Before getting started, make sure you have it installed on your computer to be able to interact with Platform.sh. See also: Install Git Learn more about Git SSH You connect to your Platform.sh Git repository and to your applications and services using SSH. SSH requires two RSA keys : A private key kept secret by the user A public key stored within the Platform.sh account These keys are called the public-private keypair and usually look like random lines of characters, like this: A private key: -----BEGIN RSA PRIVATE KEY----- MIIEowIBAAKCAQEAtpw0S4DwDVj2q04mhiIMkhvrYU7Z6hRiNbTFsqg3X7x/uYS/ dcNrSvT82j/jSeYQP3Dsod9GERW\u0026#43;dmOuLaFNeiqOStZi6jRSWo41hCOWOFbpBum3 ra1n6nUO1wa/7O5wbgzhUOfnim77oOK0UgkqPArBCNXiNFTUJAvRyVmCtvJOyrqz ...(20 more lines of this garbage)... cPjJ/wKBgGd3eZIBK6Ak92u65HYXgY9EcX3vBNP4NsF087uxV4YfrM18KlGf5I87 QGerp3VKaGe0St3ot57GlwCAQUJAf1mit8qDTi0I8MhBe7q2lstXkBvde7GY1gKx Kng4ohG6xHZ/OvC9tq7/THwAvleaxgLZN5GyXfAqNylDdZ0LtSjl -----END RSA PRIVATE KEY----- A public key (one very long line): ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC2nDRLgPANWParTiaGIgySG\u0026#43;thTtnqFGI1tMWyqDdfvH\u0026#43;5hL91w2tK9PzaP\u0026#43;NJ5hA/cOyh30YRFb52Y64toU16Ko5K1mLqNFJajjWEI5Y4VukG6betrWfqdQ7XBr/s7nBuDOFQ5\u0026#43;eKbvug4rRSCSo8CsEI1eI0VNQkC9HJWYK28k7KurMdTN7X/Z/4vknM4/Rm2bnMk2idoORQgomeZS1p3GkG8dQs/c0j/b4H7azxnqdcCaR4ahbytX3d49BN0WwE84C\u0026#43;ItsnkCt1g5tVADPrab\u0026#43;Ywsm/FTnGY3cJKKdOAHt7Ls5lfpyyug2hNAFeiZF0MoCekjDZ2GH2xdFc7AX/ your_email_address@example.com You will need a SSH public/private keypair in order to interact with Platform.sh. Your public key is uploaded to your Platform.sh user account, and it then governs authentication for Git, SSH sessions (shell access), and other tools that connect to your Platform.sh project. GitHub has a good walk-through of creating an SSH keypair on various operating systems.",
        "section": "Development",
        "subsections": " Git SSH  ",
        "image": "",
        "url": "/development/tools.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "09f532a2ca00d9080bf73a5fa9b588fd",
        "title": "Type",
        "description": "",
        "text": " The type key defines the base container image that will be used to run the application. There is a separate base container image for each primary language for the application, often in multiple versions. Supported types Available languages and their supported versions include: Language runtime Supported version C#/.Net Core dotnet 2.0, 2.1, 2.2, 3.1 Elixir elixir 1.9 Go golang 1.11, 1.12, 1.13, 1.14 Java java 11, 12, 8, 13 Lisp lisp 1.5 Node.js nodejs 6, 8, 10, 12 PHP php 7.2, 7.3, 7.4 Python python 2.7, 3.5, 3.6, 3.7, 3.8 Ruby ruby 2.3, 2.4, 2.5, 2.6, 2.7 Example configuration type:\u0026#39;php:7.4\u0026#39; Runtime The .platform.app.yaml file also supports a runtime key that allows selected customizations to the language runtime. As those possibilities vary by language, please see the appropriate language documentation. PHP",
        "section": "Configure your application",
        "subsections": " Supported types Example configuration Runtime  ",
        "image": "",
        "url": "/configuration/app/type.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "f5e039061ce2e2b11cf4c777ad7c0fba",
        "title": "Untethered local",
        "description": "",
        "text": " It’s possible to run your entire site locally on your computer. That is more performant as there’s no extra latency to connect to a remote database and doesn’t require an active Internet connection to work. However, it does require running all necessary services (databases, search servers, etc.) locally. These can be set up however you prefer, although Platform.sh recommends using a virtual machine to make it easier to share configuration between developers. If you already have a development workflow in place that works for you, you can keep using it with virtually no changes. To synchronize data from an environment on Platform.sh, consult the documentation for each service . Each service type has its own native data import/export process and Platform.sh does not get in the way of that. It’s also straightforward to download user files from your application using rsync.",
        "section": "Set up your local development environment",
        "subsections": "",
        "image": "",
        "url": "/development/local/untethered.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "589897e6fc3b1bd4b80bb95e4dde918c",
        "title": "Upgrade plan",
        "description": "",
        "text": " “Development” plan projects cannot be assigned a domain name, so you will not be able to go live until you upgrade to at least a Standard plan. This can be done using the management console. Development plans come with four environments: three development and one “future” production environment, which is the master branch. For example, “Small” plan sizes provide a production environment, but restrict your application to the use of a single service (i.e. a database). On your project, click the “Go live” button in the top right hand corner of your project preview image. This will allow you to edit the project’s plan, and it can also be reached from your “Account” page by clicking “Edit” from the vertical dot dropdown for your project. Select the plan size that is appropriate for the needs of your application. This is also the page where you can increase the number of development environments, and the amount of storage. Make your changes and then click “Update plan” at the bottom of the page. Your application will redeploy. Back I\u0026#39;ve upgraded my plan size",
        "section": "Getting started",
        "subsections": "",
        "image": "",
        "url": "/gettingstarted/next-steps/going-live/upgrade-plan.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "c9f731512314f55f0583ad2e06d08a5c",
        "title": "Working with Drush in Drupal 8",
        "description": "",
        "text": " Drush is a command-line shell and scripting interface for Drupal, a veritable Swiss Army knife designed to make life easier for those who spend their working hours hacking away at the command prompt. Drush commands can, for example, be used to clear the Drupal cache, run module and database updates, revert features, perform database imports and dumps, and a whole lot more. You can reference the full set of Drush commands at Drush.org If you have never used Drush before, you can learn more about it on the Drush GitHub Repository Platform.sh’s Drupal templates have Drush installed. Drush commands can be run in build, deploy, and post_deploy hooks, although remember that as the database is not available at build time many Drush commands will not work at build time. In addition, you can use the Platform.sh CLI to set up Drush aliases easily for all of your project’s environments. See the section below on use drush aliases . Note: Platform’s CLI requires Drush 6 or higher. Installing Drush Use the Platform.sh-provided Drupal examples If you started your project from one of Platform.sh’s Drupal templates then Drush is already installed and configured. There is nothing else for you to do. Install Drush in custom projects using Composer This is the recommended approach. Run this command in the project’s repository root folder: $ composer require drush/drush Commit the composer.json and composer.lock files and push. Drush will then be available at vendor/bin/drush, in the exact same version on your local system and on Platform.sh. Install Drush in custom projects using Build Dependencies Platform.sh supports installing some as system-level tools as build dependencies . To install Drush using this method, use the following in .platform.app.yaml dependencies:php: drush/drush :  ^8.0 Accessing Drush within the project For Drush to be available on the command line, it must be added to the project’s $PATH. Add a new file named .environment to the root of your your project’s git repository with this code: # Statements in this file will be executed (sourced) by the shell in SSH # sessions, in deploy hooks, in cron jobs, and in the application\u0026#39;s runtime # environment. This file must be placed in the root of the application, not # necessarily the git repository\u0026#39;s root. In case of multiple applications, # each application can have its own .environment file. # Allow executable app dependencies from Composer to be run from the path. if [ -n  $PLATFORM_APP_DIR  -a -f  $PLATFORM_APP_DIR /composer.json ] ; then bin=$(composer config bin-dir --working-dir= $PLATFORM_APP_DIR  --no-interaction 2\u0026gt;/dev/null) export PATH= ${PLATFORM_APP_DIR}/${bin:-vendor/bin}:${PATH}  fi Install Drush locally You can install drush globally with Composer. This does not add it to your project. $ composer global require drush/drush At the end of the installation, you should be able to run: $ drush Use drush aliases Create Drush aliases Drush aliases make it easy to manage your development websites. Here’s an example of a Drush alias file . The Platform.sh CLI generates Drush aliases for you automatically when you run platform get [project_id]. To see the aliases that are created, run platform drush-aliases and you should get output similar to that below: $ platform drush-aliases Aliases for My Site (tqmd2kvitnoly): @my-site._local @my-site.master @my-site.staging @my-site.sprint1 Recreating Drush aliases To recreate existing aliases, or after pushing a new branch via git to create the new alias, run: platform drush-aliases -r",
        "section": "Getting Started",
        "subsections": " Installing Drush  Use the Platform.sh-provided Drupal examples Install Drush in custom projects using Composer Install Drush in custom projects using Build Dependencies   Accessing Drush within the project Install Drush locally Use drush aliases  Create Drush aliases Recreating Drush aliases    ",
        "image": "",
        "url": "/frameworks/drupal8/drush.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "910b8a2be805fd45e86c829cc1486b84",
        "title": "Build site locally",
        "description": "",
        "text": " Now that you’ve opened tunnels into your services, you’ll have access to all of your data in your environment. All that’s left now is to actually build the site. Build the site From the repository root, run the command platform build The Platform CLI will first ask you for the source directory and the build destination, then it will use your .platform.app.yaml file to execute the build process locally . This will create a _www directory in the project root that is a symlink to the currently active build, which is now located in .platform/local/builds. Verify Move to the build destination (i.e. cd _www) and then run a local web server to verify the build. PHP Python Ruby php -d variables_order=EGPCS -S localhost:8001 python3 -m http.server 8000 ruby -run -e httpd . -p 8000 Applications written in Node.js, Go and Java can be configured to listen on a port locally, so it will only be necessary to execute the program directly. Cleanup That’s it! Now you can easily spin up a local build of your application and test new features with full access to all of the data in your services. When you are finished, shut down the web server and then close the tunnel to your services: platform tunnel:close Now you know how to connect to your services on Platform.sh and perform a local build during development. Back I\u0026#39;ve built my application locally",
        "section": "Getting started",
        "subsections": "",
        "image": "",
        "url": "/gettingstarted/developing/local-development/build-locally.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "1a6db47c5770be902d43a43ac3444955",
        "title": "Build, Deploy, Done!",
        "description": "",
        "text": " Once you have configured the template application in the previous step, Platform.sh will build your project for you. If you created a blank project, be sure to set up your SSH keys before trying to upload your files. Explore the management console When the build screen has cleared, Platform.sh will return you to the management console. Since you now have a project on your account, a version of this page will be what you see each time you visit the console. You will start on the main page for your new project, My First Project. From here, you can control the settings of this project and monitor its status. In the Environments box, click on Master. Check the build status Take a minute to notice some the information available on this page. Overview In this box the Master environment, which is a live environment built from the master branch of your application code, will have a status of Building. Once that status has updated to Active, the build is complete and the application has deployed. Environment Activity In this block, you can see what you have done so far has two initial entries: My First Project was created, and the template profile you chose was initialized on the environment. Done! That’s it! Once the build status has changed to Active, your application has been deployed on Platform.sh. You can view the template by clicking on the link that is now visible for the Master environment under the Overview box. It will open another tab in your browser to your new live site! In these few steps you created a free trial account, configured a template application on a project and deployed it using the management console entirely from your browser. Using the Platform.sh CLI , however, you get even more control over your project configurations, including the ability to migrate your own applications to Platform.sh. Move onto the next step to install it. Back I\u0026#39;ve deployed a template application",
        "section": "Getting started",
        "subsections": "",
        "image": "",
        "url": "/gettingstarted/gettingstarted/template/check-status.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "a77c46cb0716f05cb976ca30e4105f96",
        "title": "CLI (Command line interface)",
        "description": "",
        "text": " The CLI is the official tool to use and manage your Platform.sh projects directly from your terminal. Anything you can do within the management console can be done with the CLI. Behinds the scenes it uses both the Git interface and our REST API. The source code of the CLI is hosted on GitHub . Find detailed information on setting up a local development environment . Installation You can install the CLI easily using this command: curl -sS https://platform.sh/cli/installer | php You can find the system requirements and more information in the installation instructions on GitHub . Authentication The Platform.sh CLI will authenticate you with Platform.sh and show your projects. Just type this command to start: platform You will be asked to log in via a browser. When you are logged in, a list of your projects appears, along with some tips for getting started. Your command-line tools are now ready to use with Platform.sh. Note: Please consult the full documentation on CLI Authentication on the public CLI Github repository for further details. Usage The CLI uses Platform.sh API to trigger commands (Branch, Merge…) on your projects. It’s also very useful when you work locally since it can simulate a local build of your codebase as if you were pushing a change to Platform.sh. Once you have the CLI installed, run platform list to see all of the available commands. You can preface any command with help to see more information on how to use that command. $ platform help domain:add Command: domain:add Description: Add a new domain to the project Usage: domain:add [--project[= ... ]] [--cert= ... ] [--key= ... ] [--chain= ... ] [name] Arguments: name The name of the domain Options: --project The project ID --cert The path to the certificate file for this domain. --key The path to the private key file for the provided certificate. --chain The path to the certificate chain file or files for the provided certificate. (multiple values allowed) --help (-h) Display this help message --quiet (-q) Do not output any message --verbose (-v|vv|vvv) Increase the verbosity of messages --version (-V) Display this application version --yes (-y) Answer  yes  to all prompts --no (-n) Answer  no  to all prompts --shell (-s) Launch the shell CLI features Additional settings to control the operation of the Platform.sh CLI can be managed in the configuration file (.platform/local/project.yaml) or environment variables. See the README for the CLI for details . Auto-selecting your project When your shell’s working directory is inside a local checkout of your project repository, the CLI will autodetect your project ID and environment so you don’t need to list them as parameters each time. In your home directory, for example, you need to provide the project ID as an argument each time: $ platform project:info --project=acdefghijkl --environment=staging You can instead get the same result with just: $ cd myproject $ platform project:info You can also set a preferred project ID with the environment variables PLATFORM_PROJECT, PLATFORM_BRANCH and PLATFORM_APPLICATION_NAME. export PLATFORM_PROJECT=acdefghijkl; export PLATFORM_BRANCH=staging; platform project:info Autocomplete on the command line Once installed, the platform CLI tool provides tab auto-completion for commands, options, and even some values (your projects, valid regions). Note: Your system must include the bash-completion package or equivalent. This is not available by default on OSX, but can be installed via brew. Check your home directory and ensure that the file ~/.platformsh/autocompletion.sh is being included by your shell. platform self:install will attempt a reinstall of this utility if it’s needed. Installing the CLI on Windows 10 There are multiple ways to install the CLI on Windows 10. Platform.sh recommends using Bash for Windows (Windows Subsystem for Linux). Installing Bash for Windows You can install Bash to use the CLI on a Windows 10, 64-bit machine. The Windows 10 Anniversary Update is needed to support Git. To install Bash on Windows 10 Anniversary Edition you need to: Activate the Developer Mode in “Update \u0026 Security” in Windows Settings. This will prompt you to restart your computer. Activate the “Windows Subsystem for Linux (Beta) , under “Turn Windows features on or off” in the Programs and Features section of the Control Panel. Once again, you will need to restart your computer. In the Start Menu, search for the program “bash.exe”, which will prompt you to install it from the Windows Store. Bash is now installed. You can read more on WindowsCentral . Upon starting Bash, you will be asked to choose a username. According to the article, it doesn’t have to be the same as your current username. However, if the username don’t exist, the Linux system might not be able to create the Linux directory (depending on your permissions level). It is therefore recommended you use the same username for Linux as your Windows machine (provided your Windows user name isn’t “Admin”, as that will not be allowed). Once Bash for Windows is installed, you can install the Platform.sh CLI with the same command as above: curl -sS https://platform.sh/cli/installer | php",
        "section": "Development",
        "subsections": " Installation Authentication Usage CLI features Auto-selecting your project Autocomplete on the command line Installing the CLI on Windows 10  Installing Bash for Windows    ",
        "image": "",
        "url": "/development/cli.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "34d6bd5038a2e3e0dda87a632e86e933",
        "title": "Configure DNS",
        "description": "",
        "text": " The next step is to configure your DNS provider to point to the domain of your master environment on Platform.sh. You can access the CNAME target from your terminal by using the CLI and the command platform environment:info edge_hostname Add a CNAME record from your desired domain to the value of the edge_hostname. Depending on your registrar, this value may be called an “Alias” or something similar. If your application is going to serve multiple domains, you will need to add a CNAME record for each of them. You can find out more information about using an apex domain and CNAME records in the Going Live documentation . Depending on your registrar and the TTL you set for the domain, it may take up to 72 hours for the DNS change to fully propagate across the Internet. Back I have configured my DNS provider",
        "section": "Getting started",
        "subsections": "",
        "image": "",
        "url": "/gettingstarted/next-steps/going-live/configure-dns.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "726b3bcbc3ca6d46f1c0e79916d7d938",
        "title": "Configure services",
        "description": "",
        "text": " Platform.sh allows you to completely define and configure the topology and services you want to use on your project. Unlike other PaaS services, Platform.sh is batteries included which means that you don’t need to subscribe to an external service to get a cache or a search engine. And that those services are managed. When you back up your project, all of the services are backed-up. Services are configured through the .platform/services.yaml file you will need to commit to your Git repository. This section describes specifics you might want to know about for each service.” If you don’t have a .platform folder, you need to create one: mkdir .platform touch .platform/services.yaml Here is an example of a services.yaml file: database1:type:mysql:10.1disk:2048database2:type:postgresql:9.6disk:1024Configuration Name The name you want to give to your service. You are free to name each service as you wish (lowercase alphanumeric only). Note: Because we support multiple services of the same type (you can have 3 different MySQL instances), changing the name of the service in services.yaml will be interpreted as destroying the existing service and creating a new one. This will make all the data in that service disappear forever. Remember to always back up your environment in which you have important data before modifying this file. Type The type of your service. It’s using the format type:version. If you specify a version number which is not available, you’ll see this error when pushing your changes: Validating configuration files. E: Error parsing configuration files: - services.mysql.type: 'mysql:5.6' is not a valid service type. Service types and their supported versions include: Service type Supported version Headless Chrome chrome-headless 73 Elasticsearch elasticsearch 6.5, 7.2 InfluxDB influxdb 1.2, 1.3, 1.7 Kafka kafka 2.1, 2.2, 2.3, 2.4 MariaDB mariadb 10.0, 10.1, 10.2, 10.3, 10.4 Memcached memcached 1.4, 1.5, 1.6 MongoDB mongodb 3.0, 3.2, 3.4, 3.6 Network Storage network-storage 1.0 Oracle MySQL oracle-mysql 5.7, 8.0 PostgreSQL postgresql 9.6, 10, 11, 12 RabbitMQ rabbitmq 3.5, 3.6, 3.7, 3.8 Redis redis 3.2, 4.0, 5.0 Solr solr 3.6, 4.1, 6.3, 6.6, 7.6, 7.7, 8.0, 8.4 Varnish varnish 5.6, 6.0 Disk The disk attribute is the size of the persistent disk (in MB) allocated to the service. For example, the current default storage amount per project is 5GB (meaning 5120MB) which you can distribute between your application (as defined in .platform.app.yaml) and each of its services. For memory-resident-only services such as memcache or redis, the disk key is not available and will generate an error if present. Note: Currently we do not support downsizing the persistent disk of a service. Size By default, Platform.sh will allocate CPU and memory resources to each container automatically. Some services are optimized for high CPU load, some for high memory load. By default, Platform.sh will try to allocate the largest “fair” size possible to all services, given the available resources on the plan. That is not always optimal, however, and you can customize that behavior on any service or on any application container. See the application sizing page for more details. Service timezones All services have their system timezone set to UTC by default. In most cases that is the best option. For some applications it’s possible to change the application timezone, which will affect only the running application itself. MySQL - You can change the per-connection timezone by running SQL SET time_zone = \u003ctimezone\u003e;. PostgreSQL - You can change the timezone of current session by running SQL SET TIME ZONE \u003ctimezone\u003e;. Using the services In order for a service to be available to an application in your project (Platform.sh supports not only multiple backends but also multiple applications in each project) you will need to refer to it in the .platform.app.yaml file which configures the relationships between applications and services. Endpoints All services offer one or more endpoints. An endpoint is simply a named set of credentials that can be used to gives access to other applications and services in your project to that service. Only some services support multiple user-defined endpoints. If you do not specify one then one will be created with a standard defined name, generally the name of the service type (e.g., mysql or solr). An application container, defined by a .platform.app.yaml file, always exposes and endpoint named http to allow the router to forward requests to it. When defining relationships in a configuration file you will always address a service as \u003cservicename\u003e:\u003cendpoint\u003e. See the appropriate service page for details on how to configure multiple endpoints for each service that supports it. Connecting to a service Once a service is running and exposed as a relationship, its appropriate credentials (host name, username if appropriate, etc.) will be exposed through the PLATFORM_RELATIONSHIPS environment variable. The structure of each is documented on the appropriate service’s page, along with sample code for how to connect to it from your application. Note that different applications manage configuration differently so the exact code will vary from one application to another. Be aware that the keys in the PLATFORM_RELATIONSHIPS structure are fixed but the values they hold may change on any deployment or restart. Never hard-code connection credentials for a service into your application. You should re-check the environment variable every time your script or application starts. Access to the database or other services is only available from within the cluster. For security reasons they cannot be accessed directly. However, they can be accessed over an SSH tunnel. There are two ways to do so. (The example here uses MariaDB but the process is largely identical for any service.) Obtaining service credentials In either case, you will also need the service credentials. For that, run platform relationships. That will give output similar to the following: redis:- service: rediscacheip:246.0.82.19cluster:jyu7waly36ncj-master-7rqtwtihost:redis.internalrel:redisscheme:redisport:6379database:- username: userscheme:mysqlservice:mysqldbip:246.0.80.37cluster:jyu7waly36ncj-master-7rqtwtihost:database.internalrel:mysqlpath:mainquery:is_master:truepassword:''port:3306That indicates that the database relationship can be accessed at host database.internal, user user, and an empty password. The path key contains the database name, main. The other values can be ignored. Note: When using the default endpoint on MySQL/MariaDB, the password is usually empty. It will be filled in if you define any custom endpoints. As there is only the one user and port access is tightly restricted anyway the lack of a password does not create a security risk. Open an SSH tunnel directly The first option is to open an SSH tunnel for all of your services. You can do so with the Platform.sh CLI, like so: $ platform tunnel:open SSH tunnel opened on port 30000 to relationship: redis SSH tunnel opened on port 30001 to relationship: database Logs are written to: ~/.platformsh/tunnels.log List tunnels with: platform tunnels View tunnel details with: platform tunnel:info Close tunnels with: platform tunnel:close The tunnel:open command will connect all relationships defined in the .platform.app.yaml file to local ports, starting at 30000. You can then connect to those ports on localhost using the program of your choice. In this example, we would connect to localhost:30001, database name main, with username user and an empty password. The platform tunnels command will list all open tunnels: +-------+---------------+-------------+-----------+--------------+ | Port | Project | Environment | App | Relationship | +-------+---------------+-------------+-----------+--------------+ | 30000 | a43m75zns6k4c | master | [default] | redis | | 30001 | a43m75zns6k4c | master | [default] | database | +-------+---------------+-------------+-----------+--------------+ Using an application tunnel Alternatively, many database applications (such as MySQL Workbench and similar tools) support establishing their own SSH tunnel. Consult the documentation for your application for how to enter SSH credentials, including telling it where your SSH private key is. (Platform.sh does not support password-based SSH authentication.) To get the values to use, the easiest way is to run platform ssh --pipe. That will return a command line that can be used to connect over SSH, from which you can pull the appropriate information. For example: jyu7waly36ncj-master-7rqtwti--app@ssh.us.platform.sh In this case, the username is jyu7waly36ncj-master-7rqtwti--app and the host is ssh.us.platform.sh. Note that the host will vary per region, and the username will vary per-environment. In this example, we would configure our database application to setup a tunnel to ssh.us.platform.sh as user jyu7waly36ncj-master-7rqtwti--app, and then connect to the database on host database.internal, username user, empty password, and database name main.",
        "section": "Configuration",
        "subsections": " Configuration  Name Type Disk Size   Service timezones Using the services Endpoints Connecting to a service  Obtaining service credentials Open an SSH tunnel directly Using an application tunnel    ",
        "image": "",
        "url": "/configuration/services.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "7f0dfb316b5ab9f92ea424cf1598742a",
        "title": "Content Delivery Networks",
        "description": "",
        "text": " Platform.sh Dedicated plans include a Fastly CDN account by default, which will be managed by Platform.sh. Our experience has shown that effective caching can mean a huge difference in the perceived performance of an application by its users, and that placing the caches closer to your users (wherever they may be) is the best solution currently available. Self-Service Grid plans do not include a CDN by default, but you are welcome to configure one yourself. See our guidelines for when and if to use a CDN for HTTP caching. We have partnerships with a variety of CDN vendors depending on your application’s needs. Our recommended CDN provider is Fastly . DNS management The distributed nature of most CDNs means that for proper functioning, any domains that you intend to make use of the CDN will be required to use CNAME records for pointing the DNS entries. Pointing the root domain (example.com) at a CNAME record is not possible for all DNS hosts, so you will need to confirm this functionality or migrate to a new DNS host. CloudFlare has a more detailed writeup of the challenges of root CNAMEs. In the event that you and your team choose a pure Fastly solution, this is negated by their providing a set of Anycast IP addresses for you. This allows you to create A records for your root domain that will point to Fastly’s CDN. Initial setup For Enterprise-Dedicated plans, CDN setup is handled by Platform.sh as part of your onboarding. After the application is stood up on its Dedicated VMs we can begin the collaborative process of provisioning the CDN and configuring DNS and caching setup. We provide CDN services for both staging and production. For self-service Grid plans, the setup can be done at any time by the customer. Cache configuration Depending on which CDN is decided as part of the pre-sales analysis, there may be varying levels of flexibility with regard to caching and ongoing cache invalidation. This should be discussed between your sales representative and senior technical members of your team if there are concerns with CDN configuration and functionality. If using Fastly as a CDN, it is possible to provide either custom VCL snippets or a full custom VCL file. Platform.sh will grant customers access to do so upon request. However, be aware that downtime caused by custom VCL configuration will not be covered by the SLA, just as application code in your repository is not covered by the SLA. TLS encryption Security and the related topic of encryption of data are fundamental principles here at Platform.sh, and as such we provide TLS certificates in the default Enterprise-Dedicated package. This allows for encryption of all traffic between your users and your application. By default we will provision a shared certificate with the chosen CDN vendor. If you opt for the Global Application Cache, we will provision certificates for both the site subdomain (www) and the asset/CDN subdomain. We use wildcard certificates to secure production, staging, and any other subdomains simultaneously. If you need Extended Validation TLS certificates you will need to provide your own from an issuer of your choice that we can install for you. If you need to provide your own TLS certificate, place the certificate, the unencrypted private key, and the necessary certificate chain supplied by your TLS provider in your application’s private directory (not web accessible), and then open a ticket to let our team know to install it. Platform.sh Enterprise-Dedicated supports a single TLS certificate on the origin. Support for multiple certificates is offered only through a CDN such as CloudFront or Fastly. Self-signed certificates can optionally be used on the origin for development purposes or for enabling TLS between the CDN and origin. All TLS certificates used with CloudFront MUST be 2048 bit certificates. Larger sizes will not work. Web Application Firewall \u0026 Anti-DDoS All Platform.sh-hosted sites, either Grid or Dedicated, live on infrastructure provided by major cloud vendors. These vendors include their own Level 3 DDoS protection that is sufficient for the vast majority of cases. Customers are welcome to put their own WAF in front of a Dedicated cluster or add other security measures not included in the offering. The router cache When using a CDN the Platform.sh router’s HTTP cache becomes redundant. In most cases it’s best to disable it outright. Modify your route in .platform/routes.yaml like so to disable the cache:  https://{default}/ :type:upstreamupstream: app:http cache:# Disable the HTTP cache on this route. It will be handled by the CDN instead.enabled:falsePreventing direct access When using a CDN, you might not want users to access your Platform.sh origin directly. There are three ways to secure your origin. Password protected HTTP Authentication You can password protect your project using HTTP access control . Make sure that you generate a password of sufficient strength. You can then share the password with your CDN provider. Make sure the CDN adds a header to authenticate correctly to your origin. Add a custom header to the origin request with the base64 encoded username:password. For example: Aladdin:OpenSesame would become Authorization: Basic QWxhZGRpbjpPcGVuU2VzYW1l. Be aware that this approach will apply the same user and password to all development environments, too. You can have developers enter credentials through their browser, or override the access control setting for each child environment. Note: This is the recommended approach for CloudFlare. IP whitelisting If your CDN does not support adding headers to the request to origin, you can allow the IP addresses of your CDN. Note: You WILL have to update your configuration when your CDN updates their IP addresses. List of IP ranges for: CloudFlare Fastly Be aware that this approach will apply the same IP restrictions to all development environments, too. To remove it from development environments, you will need to disable it on each environment or else create a single child of master where it is disabled, and them make all development branches off of that environment. Client authenticated TLS If your CDN offers this option, an alternative way of securing the connection is client authenticated TLS . note: Please remember to permit your developers to access the origin by creating your own certificate or else they won’t be able to access the project url directly. (see below) CloudFlare has a very good article on what client authenticated TLS is, and how to set this up. To activate authenticated TLS follow the following steps: Download the correct certificate from your CDN provider. CloudFlare Caveat! an attacker could make a Cloudflare account to bypass your origin restriction. For CloudFlare, using the HTTP access control described above is the recommended way of securing your origin. Fastly Make sure you have a .crt file. If you have have .pem file, simply rename it to cdn.crt Add the cdn.crt to your git repository Add the relevant configuration to your .platform.app.yaml file tls: client_authentication:  require  client_certificate_authorities: - !include type: string path: cdn.crt Note: The steps above are generally similar but can vary for different CDN providers. Contact your CDN provider’s support department for specific assistance.",
        "section": "Going live",
        "subsections": " DNS management Initial setup Cache configuration TLS encryption Web Application Firewall \u0026amp; Anti-DDoS The router cache Preventing direct access  Password protected HTTP Authentication IP whitelisting Client authenticated TLS    ",
        "image": "",
        "url": "/golive/cdn.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "d7452b54e01892fffca3eae8ff36cf7b",
        "title": "Custom sizing",
        "description": "",
        "text": " By default, Platform.sh will automatically select appropriate resource sizes (CPU and memory) for a container when it’s deployed, based on the plan size and the number of other containers in the cluster. The more containers in a project the fewer resources each one gets, and vice versa, with similar containers getting similar resources. Note: These are advanced settings and should only be used by experienced Platform.sh users. 99.9% of the time our default container sizes are the correct choice for best performance. Usually that’s fine, but sometimes it’s undesirable. You may, for instance, want to have a queue worker container that you know has low memory and CPU needs, so it’s helpful to give that one fewer resources and another container more. Or a given service may be very heavily used in your architecture so it needs all the resources it can take. In those cases you can provide sizing hints to the system on a per-service basis. Every application container as well as every service in .platform/services.yaml supports a size key, which instructs the system how many resources to allocate to it. The exact CPU and memory allocated will depend on the application or service type, and we may adjust these values over time to better optimize resource usage. Legal values for the size key are AUTO (the default), S, M, L, XL, 2XL, 4XL. Note that in a development environment this value is ignored and always set to S. It will only take effect in a production deployment (a master branch with an associated domain). If the total resources requested by all apps and services is larger than what the plan size allows then a production deployment will fail with an error. How do I make a background processing container smaller to save resources? Simply set the size key to S to ensure that the container gets fewer resources, leaving more to be allocated to other containers. name:processingtype:nodejs:6.11size:S...",
        "section": "Configure your application",
        "subsections": " How do I make a background processing container smaller to save resources?  ",
        "image": "",
        "url": "/configuration/app/size.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "a9015effbb9bd28b57ead22d637175fb",
        "title": "Data collection",
        "description": "",
        "text": " As part of our normal business operations we do collect various pieces of data. In GDPR terms: Article 4: Our accounts system contains some (routine) Article 4 items (name, address, phone, etc.) in order to allow us to bill your account appropriately. This information can be verified, changed, and deleted by logging into your account . Article 9: We don’t capture and store any Article 9 special identifiers (such as race, religion, sexual orientation, or other attributes that are irrelevant to our business). Article 30: The only Article 30 items we keep are IP address and Log files. These reside on AWS/Azure/Orange (depending on your hosting), and may be sent to Sentry.io when there are crashes. Application logs Application logs are those generated by the host application or application server (such as PHP-FPM). They are immutable to Customers to prevent tampering. These logs are secured behind key-based SSH so that only the Customer and our relevant teams have access. System logs Platform.sh records routine system logs. We do not access Customer-specific system logs or the customer environment unless requested to do so to help solve a problem. In the future, we will be rolling out better log segregation to allow a Customer to get easier access to their own logs for diagnostic purposes. Access logs There are two main types of access logs: web and SSH. Web access logs Application access logs are immutable to Customers to prevent tampering. These logs are secured behind key-based SSH so that only the Customer and our relevant teams have access. SSH access logs SSH access logs are securely stored in our infrastructure and not accessible to customers. They can be accessed by Platform.sh support personnel as part of an audit if requested. Access by customers and Platform.sh support personnel to customer environments is logged. However, we only log the connection itself, not what was done during the session, as that would be a violation of customer privacy. Vendor data sharing We have identified and mapped all data we collect and share with vendors (such as AWS, Azure, and Orange). We know what we capture and where it goes. All of our vendors have been vetted for security and GDPR compliance. We have enacted contract amendments and Data Processing Agreements (DPAs) where applicable.",
        "section": "Security and compliance",
        "subsections": " Application logs System logs Access logs  Web access logs SSH access logs   Vendor data sharing  ",
        "image": "",
        "url": "/security/data-collection.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "1cb46400325c3c9c33bb934b162ffed2",
        "title": "Deployment",
        "description": "",
        "text": " Deploying to Production and Staging The production branch of your Git repository is designated for production, and a staging branch is designated for staging. Any code merged to those branches will automatically trigger a rebuild of the production and staging environments, respectively, in the Dedicated Cluster. Any defined users or environment variables will also be propagated to the Dedicated Cluster as well. Note that there is no automatic cloning of data from the Dedicated Cluster to the Development Environment the way there is between branches in the Development Environment. Production data may still be replicated to the Development Environment manually. The master branch is still available but will have no impact on either the production or staging environments. Deploys of the master branch will not trigger a rebuild of the Dedicated Cluster environments. A common model is to use the master branch as a pre-integration branch before merging code to staging, such as at the end of a sprint. Deployment process When deploying to the Dedicated Cluster the process is slightly different than when working with Platform.sh on the Grid. The new application image is built in the exact same fashion as for Platform.sh Professional. Any active background tasks on the cluster, including cron tasks, are terminated. The cluster (production or staging) is closed, meaning it does not accept new requests. Incoming requests will receive an HTTP 500 error. The application image on all three servers is replaced with the new image. The deploy hook is run on one, and only one, of the three servers. The cluster is opened to allow new requests. The deploy usually takes approximately 30-90 seconds, although that is highly dependent on how long the deploy hook takes to run. During the deploy process the cluster is unavailable. However, nearly all Platform.sh Dedicated instances are fronted by a Content Delivery Network (CDN). Most CDNs can be configured to allow a “grace period”, that is, requests to the origin that fail will be served from the existing cache, even if that cache item is stale. We strongly recommend configuring the CDN with a grace period longer than a typical deployment. That means anonymous users should see no interruption in service at all. Authenticated traffic that cannot be served by the CDN will still see a brief interruption. Deployment philosophy Platform.sh values consistency over availability, acknowledging that it is nearly impossible to have both. Because the deploy hook may make database changes that are incompatible with the previous code version it is unsafe to have both old and new code running in parallel (on different servers), as that could result in data loss. We believe that a minute of planned downtime for authenticated users is preferable to a risk of race conditions resulting in data corruption, especially with a CDN continuing to serve anonymous traffic uninterrupted. That brief downtime applies only to changes pushed to the production branch. Deployments to staging or to a development branch have no impact on the production environment and will cause no downtime.",
        "section": "Platform.sh Dedicated cluster specifications",
        "subsections": " Deploying to Production and Staging Deployment process Deployment philosophy  ",
        "image": "",
        "url": "/dedicated/architecture/deploying.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "6d163cc59d4928838cda44688d3ffed4",
        "title": "Developing on Platform.sh",
        "description": "",
        "text": "Once an application has been migrated to Platform.sh, there's plenty more features that will help improve your development life cycle. You can build your site locally, remotely connect to your services, and test new features on a live site all with Platform.sh.",
        "section": "Introduction",
        "subsections": "",
        "image": "",
        "url": "/gettingstarted/developing.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "97f66866006367ccf3beeca603b04376",
        "title": "Environment configuration",
        "description": "",
        "text": " You can access an environment’s settings by selecting that environment from the Select Environments pull-down menu at the top of the page or by clicking that environment within the Environments graphic on the right side. Click the Settings tab at the top of the screen. General The General screen allows you to extend the behavior of a specific environment. Environment name The first setting allows you to modify the name of the environment and view its parent environment. Status From the Status tab, you can activate or deactivate an environment. The Deactivate \u0026amp; Delete Data action will Deactivate the environment. Unless is is re-activated, it will no longer deploy and it will not be accessible from the web or via SSH. Destroy all services running on this environment. Delete all data specific to the environment. If the environment is reactivated, it will sync data from its parent environment. Once the environment is deactivated, the Git branch will remain on Platform.sh in the inactive environment. To delete the branch as well, you need to execute the following: git push origin :BRANCH-NAME Note: Deleting the Master environment is forbidden. Outgoing emails From this tab, you can allow your application to send emails via a SendGrid SMTP proxy. Changing this setting will temporarily list the environment’s status as “Building”, as the project re-builds with the new setting. Once it has re-deployed, it will appear once again as “Active” in your settings. Search engine visibility From this tab, you can tell search engines to ignore the site entirely, even if it is publicly visible. X-Robots-Tag By default, Platform.sh includes an additional X-Robots-Tag header on all non-production environments: X-Robots-Tag: noindex, nofollow That tells search engines to not index sites on non-production environments entirely nor traverse links from those sites, even if they are publicly visible. That keeps non-production sites out of search engine indexes that would dilute the SEO of the production site. To disable that feature for a non-production environment, use the Platform.sh CLI command below: platform environment:info restrict_robots false Or to disable it for a specific environment other than the one that is currently checked out, execute the following: platform environment:info -e ENVNAME restrict_robots false where ENVNAME is the name of the environment. On a production instance (the master branch, after a domain has been assigned) the search-blocker is disabled and your application can serve a robots.txt file as normal. However, you must ensure that the file is in your project’s web root (the directory where the / location maps to) and your application is configured to serve it. See the location section in .platform.app.yaml . HTTP access control You should not expose your development environments to the whole wide world. Platform.sh allows you to simply implement access control, either by login/password (the equivalent to .htaccess) or by filtering IP addresses or a network using the CIDR format . That is, 4.5.6.7 and 4.5.6.0/8 are both legal formats. Note: Changing access control will trigger a new deploy of the current environment. However, the changes will not propagate to child environments until they are manually redeployed. These settings get inherited by branches below the one you are on. That means if you create a staging environment, and you create branches from this one, they will all inherit the same authentication information and you only have to set-it up once. You can also setup authentication with the CLI using the following command platform environment:http-access which also allows you to read the current setup. This eases the integration of CI jobs with Platform.sh as you will not need to hardcode the values in the CI. You can allow or deny access to specific IPs or IP ranges. First switch the access control section to ON. Then add one or more IPs or CIDR IP masks, followed by allow or deny. See the example below. Note that allow entries should come before deny entries in case both of them would match. For example, the following configuration will only allow the 1.2.3.4 IP to access your website. 1.2.3.4/32 allow 0.0.0.0/0 deny Access The Access screen allows you to manage the users’ access on your project. You can invite new users to a specific environment by clicking the Add button and entering their email address, or modify permissions of existing users by clicking the Edit link when hovering the user. Note: Currently, permission changes that grant or revoke SSH access to an environment take effect only after the next time that environment is deployed. Selecting a user will allow you to either edit or remove access to that environment. You can also manage access to users on multiple environments using the project configuration screen. Variables The Variables screen allows you to define the variables that will be available on a specific environment. Routes The Routes screen describes the configuration features that define the routes of your application. Routes cannot be edited here, but it provides a simple routes configuration example for your project’s .platform/routes.yaml file. Consult the documentation for more information about properly configuring Routes for your project.",
        "section": "Management console",
        "subsections": " General  Environment name Status Outgoing emails Search engine visibility X-Robots-Tag HTTP access control   Access Variables Routes  ",
        "image": "",
        "url": "/administration/web/configure-environment.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "93f4343df884eff652b0ab0924905c16",
        "title": "eZ Platform",
        "description": "",
        "text": " eZ Platform is a Composer-based PHP CMS, and as such fits well with the Platform.sh model. As a Symfony-based application its setup is very similar to Symfony. eZ Platform comes pre-configured for use with Platform.sh in version 1.13 and later. Version 2.5 and later is recommended. Those are the only versions that are supported. Appropriate Platform.sh configuration files are included in the eZ Platform application itself, but of course may be modified to suit your particular site if needed. Cache and sessions By default, eZ Platform is configured to use a single Redis instance for both the application cache and session storage. You may optionally choose to use a separate Redis instance for session storage in case you have a lot of authenticated traffic (and thus there would be many session records). To do so, uncomment the redissession entry in the .platform/services.yaml file and the corresponding relationship in the .platform.app.yaml file. The bridge code that is provided with eZ Platform 1.13 and later will automatically detect the additional Redis service and use it for session storage. On a Dedicated instance, we strongly recommend using two separate Redis instances for Cache and Sessions. The service and relationship names that ship with the default Platform.sh configuration in eZ Platform should be used as-is. To ensure the development environment works like Production, uncomment the redissession entry in the .platform/services.yaml file and the corresponding relationship in the .platform.app.yaml file. The bridge code that is provided with eZ Platform 1.13 and later will automatically detect the additional Redis service and use it for session storage. By default, on Dedicated instances we will configure both Cache and Session storage in “persistent” mode, so that data is not lost in case of a system or process restart. That reduces the potential for cache stampede issues or inadvertently logging people out. Modifying an existing eZ Platform project If you have an existing eZ Platform project that was upgraded from a previous version, or want to resynchronize with the latest recommended configuration, please see the eZ Platform official repository . In particular, see: The .platform.app.yaml file, which automatically builds eZ Platform in dev mode or production mode depending on your defined project-level variables. The .platform directory The platformsh.php configuration file, which does the work of mapping Platform.sh environment variables into eZ Platform. It also will automatically enable Redis-based cache and session support if detected. Local Development with eZ Platform 2.x and later eZ Systems provide a tool called eZ Launchpad for local development on top of a Docker stack. It improves Developer eXperience and reduces complexity for common actions by simplifying your interactions with Docker containers. eZ Launchpad is ready to work with Platform.sh. It serves as a wrapper that allows you to run console commands from within the container without logging into it explicitly. For example to run bin/console cache:clear inside the PHP container do: ~/ez sfrun cache:clear eZ Launchpad installation eZ Launchpad’s approach is to stay as decoupled as possible from your development machine and your remote hosting whether you are Linux or Mac OSX. To install run: curl -LSs https://ezsystems.github.io/launchpad/install_curl.bash | bash Then you can start to use it to initialize your eZ Platform project on top Docker. ~/ez init or create the Docker stack based on an existing project git clone yourproject.git application cd application ~/ez create You will find more details on the eZ Launchpad documentation . At this time you will have a working eZ Platform application with many services including Varnish, Solr, Redis etc. Platform.sh integration To generate the key files for Platform.sh (.platform.app.yaml and .platform) run: ~/ez platformsh:setup eZ Launchpad will generate the files for you and you are then totally free to fine tune them. Solr specificity Solr is fully functional with eZ Launchpad but it is not enabled by default on Platform.sh. You will have to set it up manually following the current documenation here: https://github.com/ezsystems/ezplatform/blob/master/.platform/services.yaml#L37. Actions needed are: Generate the Solr configuration thanks to the script provided by eZ Systems. Put the result in the .platform at the root of your project. Add the service in the .platform/services.yaml. Add the relationship in the .platform.app..yaml. Environment variables (optional) eZ Launchpad allows you to define environment variables in the provisioning/dev/docker-compose.yml file. You may use that to set Platform.sh variables to match Platform.sh environments so that you can keep your environment behavior in sync. Such variables have to be set in the engine container. # provisioning/dev/docker-compose.ymlengine:environment:- ASIMPLEVARIABLE=avalue- PLATFORM_RELATIONSHIPS=A_BASE64_ENCODED_VALUELocal development with Platform.sh Thanks to eZ Launchpad you are able to be work 100% locally: untethered . We have the whole project working offline on our local machine. Note: Platform.sh also provides a smooth SSH tunnels integration described in the tethered page. Local services are provided by the Docker stack but there are minimum day-to-day tasks that you might need with Platform.sh. The main ones are: Downstream database synchronization: Getting it from the remote to the local. Downstream file storage synchronization: Getting it from the remote to the local. To help you with that, Platform.sh provides a CLI that you probably already have. If you don’t, see the install guide . Combined together, eZ Launcphad and Platform.sh CLI make those actions straight forward and simple. Database and storage synchronization platform db:dump --gzip -f ezplatform.sql.gz -d data/ -y platform mount:download -m ezplatform/web/var --target=ezplatform/web/var/ -y ~/ez/importdata The two first lines get the remote database and storage from the remote environment and stores it locally in data/. The third tells to eZ Launchpad to import those data in the Docker stack. Note: The storage (images and files) synchronization is optional. eZ Platform provides a placeholder generator mechanism which allows you to forget about the real images for your local.",
        "section": "Featured frameworks",
        "subsections": " Cache and sessions Modifying an existing eZ Platform project Local Development with eZ Platform 2.x and later  eZ Launchpad installation Platform.sh integration Local development with Platform.sh    ",
        "image": "",
        "url": "/frameworks/ez.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "8c18f1de7b0d457621f4d862c950b32a",
        "title": "InfluxDB (Database service)",
        "description": "",
        "text": " InfluxDB is a time series database optimized for high-write-volume use cases such as logs, sensor data, and real-time analytics. It exposes an HTTP API for client interaction. See the InfluxDB documentation for more information. Supported versions Grid Dedicated 1.2 1.3 1.7 None available Relationship The format exposed in the $PLATFORM_RELATIONSHIPS environment variable : {  cluster :  rjify4yjcwxaa-master-7rqtwti ,  host :  influxdb.internal ,  hostname :  mycuti5glfqyt322jjhcfahrpi.influxdb.service._.eu-3.platformsh.site ,  ip :  169.254.180.153 ,  port : 8086,  rel :  influxdb ,  scheme :  http ,  service :  influxdb ,  type :  influxdb:1.7  } Usage example In your .platform/services.yaml: timedb:type:influxdb:1.7disk:256 In your .platform.app.yaml: relationships:influxtimedb: timedb:influxdb  Note: You will need to use the influxdb type when defining the service # .platform/services.yamlservice_name:type:influxdb:versiondisk:256 and the endpoint influxdb when defining the relationship # .platform.app.yamlrelationships:relationship_name:“service_name:influxdb” Your service_name and relationship_name are defined by you, but we recommend making them distinct from each other. You can then use the service in a configuration file of your application with something like: \u0026lt;?php // This assumes a fictional application with an array named $settings. if (getenv(\u0026#39;PLATFORM_RELATIONSHIPS\u0026#39;)) { $relationships = json_decode(base64_decode($relationships), TRUE); // For a relationship named \u0026#39;influxtimedb\u0026#39; referring to one endpoint. if (!empty($relationships[\u0026#39;influxtimedb\u0026#39;])) { foreach ($relationships[\u0026#39;influxtimedb\u0026#39;] as $endpoint) { $settings[\u0026#39;influxdb_host\u0026#39;] = $endpoint[\u0026#39;host\u0026#39;]; $settings[\u0026#39;influxdb_port\u0026#39;] = $endpoint[\u0026#39;port\u0026#39;]; break; } } } Exporting data InfluxDB includes its own export mechanism . To gain access to the server from your local machine open an SSH tunnel with the Platform.sh CLI: platform tunnel:open That will open an SSH tunnel to all services on your current environment, and produce output something like the following: SSH tunnel opened on port 30000 to relationship: influxtimedb The port may vary in your case. Then, simply run InfluxDB’s export commands as desired. influx_inspect export -compress",
        "section": "Configure services",
        "subsections": " Supported versions Relationship Usage example Exporting data  ",
        "image": "",
        "url": "/configuration/services/influxdb.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "a1b09f1f5b13f42ffa8e096570485ece",
        "title": "Install the CLI",
        "description": "",
        "text": " In the previous steps you checked that the requirements on your computer were met and configured an SSH key on your Platform.sh account. Now all we have to do is install the CLI and you can access your projects from the command line. Install the CLI In your terminal run the following command depending on your operating system: Installing on OSX or Linux curl -sS https://platform.sh/cli/installer | php Installing on Windows curl https://platform.sh/cli/installer -o cli-installer.php php cli-installer.php Authenticate and Verify Once the installation has completed, you can run the CLI in your terminal with the command platform Take a moment to view some of the available commands with the command platform list Now that you have installed the CLI and it is communicating with Platform.sh, you can configure and push your project to Platform.sh. Back I\u0026#39;ve installed the CLI",
        "section": "Getting started",
        "subsections": "",
        "image": "",
        "url": "/gettingstarted/gettingstarted/own-code/cli-install.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "9cbb9988fbeeb9cf6f649e71650b799b",
        "title": "Lando",
        "description": "",
        "text": " Using Lando for local development Lando is a container-based local development toolchain that plays nicely with Platform.sh. It is maintained by Tandem , a 3rd party agency, but is a viable option for most Platform.sh projects. See the Lando documentation for installing and setting up Lando on your system. Lando will ask you to create a .lando.yml file in your application root, which functions similarly to the .platform.app.yaml file. (Note the different file extension.) It is safe to check this file into your Git repository as Platform.sh will simply ignore it. If your application is one of those with a specific “recipe” available from Lando, you can use that directly in your .lando.yml file. It can be customized further as needed for your application, and some customizations are specific to certain applications. .lando.yml configuration In particular, we recommend: # Name the application the same as in your .platform.app.yaml.name:app# Use the recipe appropriate for your application.recipe:drupal8config:# Lando defaults to Apache. Switch to nginx to match Platform.sh.via:nginx# Set the webroot to match your .platform.app.yaml.webroot:web# Lando defaults to the latest MySQL release, but Platform.sh uses MariaDB.# Specify the version to match what\u0026#39;s in services.yaml.database:mariadb:10.1Downloading data from Platform.sh into Lando In most cases downloading data from Platform.sh and loading it into Lando is straightforward. If you have a single MySQL database then the following two commands, run from your application root, will download a compressed database backup and load it into the local Lando database container. platform db:dump --gzip -f database.sql.gz lando db-import database.sql.gz Rsync can download user files easily and efficiently. See the exporting tutorial for information on how to use rsync. Then you need to update your sites/default/settings.local.php to configure your codebase to connect to the local database that you just imported: /* Working in local with Lando */ if (getenv(\u0026#39;LANDO\u0026#39;) === \u0026#39;ON\u0026#39;) { $lando_info = json_decode(getenv(\u0026#39;LANDO_INFO\u0026#39;), TRUE); $settings[\u0026#39;trusted_host_patterns\u0026#39;] = [\u0026#39;.*\u0026#39;]; $settings[\u0026#39;hash_salt\u0026#39;] = \u0026#39;CHANGE THIS TO SOME RANDOMLY GENERATED STRING\u0026#39;; $databases[\u0026#39;default\u0026#39;][\u0026#39;default\u0026#39;] = [ \u0026#39;driver\u0026#39; =\u0026gt; \u0026#39;mysql\u0026#39;, \u0026#39;database\u0026#39; =\u0026gt; $lando_info[\u0026#39;database\u0026#39;][\u0026#39;creds\u0026#39;][\u0026#39;database\u0026#39;], \u0026#39;username\u0026#39; =\u0026gt; $lando_info[\u0026#39;database\u0026#39;][\u0026#39;creds\u0026#39;][\u0026#39;user\u0026#39;], \u0026#39;password\u0026#39; =\u0026gt; $lando_info[\u0026#39;database\u0026#39;][\u0026#39;creds\u0026#39;][\u0026#39;password\u0026#39;], \u0026#39;host\u0026#39; =\u0026gt; $lando_info[\u0026#39;database\u0026#39;][\u0026#39;internal_connection\u0026#39;][\u0026#39;host\u0026#39;], \u0026#39;port\u0026#39; =\u0026gt; $lando_info[\u0026#39;database\u0026#39;][\u0026#39;internal_connection\u0026#39;][\u0026#39;port\u0026#39;], ]; }",
        "section": "Set up your local development environment",
        "subsections": " .lando.yml configuration Downloading data from Platform.sh into Lando  ",
        "image": "",
        "url": "/development/local/lando.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "ac44f2f6e4f3015808a97000be48b7d7",
        "title": "Merge into production",
        "description": "",
        "text": " Now that you’ve had the chance to verify that your application built and deployed correctly on your development environment, you’re ready to merge it into your production site. Platform.sh provides backup features that protect against any unforeseen consequences of your merges, keeping a historical copy of all of your code and data. Note: The --project flag is not needed if you are running the platform command from within your local repository. Create a backup Before you merge the dev feature into master, create a backup of the master environment. The backup will preserve both the code and all of its data. platform backup --project \u0026lt;project id\u0026gt; Select master as the environment you want to back up. Merge feature into production git checkout master git merge dev git push When the build process completes, verify that your changes have been merged. platform url Restore a backup If you would like to restore the code and data to the time of your backup, use the command platform backup:restore --project \u0026lt;project id\u0026gt; Back I\u0026#39;ve merged the new feature",
        "section": "Getting started",
        "subsections": "",
        "image": "",
        "url": "/gettingstarted/developing/dev-environments/merge.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "f81e5b46acd926264d0a4e1ccfd225a8",
        "title": "Migrating an existing Drupal 7 site to Platform.sh",
        "description": "",
        "text": " Once you’ve setup the code for your site as a Platform.sh project, you will need to upload your existing database and files directories as well to complete the site. Import database With Drush (preferred) You can use drush aliases to import your existing local database into Platform. The aliases here are examples. Use the CLI’s platform drush-aliases command to find your own aliases. drush @platform._local sql-dump \u0026gt; backup_database.sql You can also sanitize your database prior to import it into Platform.sh by running: drush @platform._local sql-sanitize When you’re ready, export your local database and then import it into your remote Platform.sh environment. drush @platform._local sql-dump \u0026gt; local_database.sql drush @platform.master sql-cli \u0026lt; local_database.sql When the process completes, you can visit the URL of your development environment and test that the database has been properly imported. Without Drush Export your database in an SQL file or in a compressed file. Copy it via SSH to the remote environment on Platform into the /app/tmp folder which is writable: scp database.sql [SSH-URL]:/app/tmp Log in to the environment via SSH and import the database: ssh [SSH-URL] web@[PROJECT-ID]-master--php:~$ mysql -h database.internal main \u0026lt; tmp/database.sql Import files With Drush You can use Drush site aliases to import your existing local files. $ drush rsync @platform._local:%files @platform.master:%files You will destroy data from [SSH-URL]:././sites/default/files and replace with data from ~/Sites/platform/sites/default/files/ Do you really want to continue? (y/n): y Note: Drush will verify that you are copying and overwriting the proper “files” folders, so double-check that information before you type y to continue. This step may take some time, but when the process completes, you can visit the URL of your master environment and test that the files have properly been imported. Without Drush Go to your Drupal root on your local machine and synchronize the files folder to your remote Platform environment: $ rsync -r sites/default/files/. [SSH-URL]:public/sites/default/files/ Note: The local files path may depend of your installation. The path in URL may vary depending on what your .platform.app.yaml file specifies as the root path and files mount. Directly from server to platform.sh If the files folder is too large to fit on your computer, you can transfer them directly from server to server. If you have a firewall between the origin server and platform.sh, you can use agent-forwarding to enable a direct connection: $ ssh -A -t [USER]@[ORIGIN-SERVER] ssh -A -t [SSH-URL] $ rsync -a --delete [USER]@[ORIGIN-SERVER]:/var/www/drupal/sites/default/files/ public/sites/default/files Note: If you are using a Mac OS computer, you might experience issues where files with non-ascii characters in them don’t work after transfer because Mac OS X uses decomposed form (like “a \u0026#43; ¨ = ä”, a form known as NFD), not the usual composed form (“ä”, a form known as NFC used everywhere else). One workaround is to use the direct server-to-server transfer method mentioned above.",
        "section": "Getting Started",
        "subsections": " Import database  With Drush (preferred) Without Drush   Import files  With Drush Without Drush    ",
        "image": "",
        "url": "/frameworks/drupal7/migrating.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "3c1cc86d71e1be82729b7561be5e8dfc",
        "title": "Migrating to Platform.sh",
        "description": "",
        "text": " Moving an already-built site to Platform.sh is generally straightforward. For the most part, the only part that will vary from one framework to another is the details of the Platform.sh configuration files. See the Featured Frameworks section or our Project Templates for more project-specific documentation. Preparation First, assemble your Git repository as appropriate, on your master branch. Be sure to include the Platform.sh configuration files, as you will not be able to push the repository to Platform.sh otherwise! For some applications, such as Drupal you will need to dump configuration to files before proceeding. You will also need to provide appropriate configuration to read the credentials for your services at runtime and integrate them into your application’s configuration. The details of that integration will vary between systems. Be sure to see the appropriate project templates for our recommended configuration. Go Templates Java Templates Node.js Templates PHP Templates Python Templates In the management console, click \u0026#43; Add project to create a new Platform.sh project. When asked to select a template pick “Create a blank project”. Push your code When creating a new project, the management console will provide two commands to copy and paste similar to the following: git remote add platform nodzrdripcyh6@git.us.platform.sh:nodzrdripcyh6.git git push -u platform master The first will add a Git remote for the Platform.sh repository named platform. The name is significant as the Platform.sh CLI will look for either platform or origin to be the Platform.sh repository, and some commands may not function correctly otherwise. The second will push your repository’s master branch to the Platform.sh master branch. Note that a project must always start with a master branch, or deploys to any other environment will fail. When you push, a new environment will be created using your code and the provided configuration files. The system will flag any errors with the configuration if it can. If so, correct the error and try again. Import your database You will need to have a dump or backup of the database you wish to start from. The process is essentially the same for each type of persistent data service. See the MySQL , PostgreSQL , or MongoDB documentation as appropriate. Import your files Content files (that is, files that are not intended as part of your code base so are not in Git) can be uploaded to your mounts using the Platform.sh CLI or by using rsync. You will need to upload each directory’s files separately. Suppose for instance you have the following file mounts defined: mounts:\u0026#39;web/uploads\u0026#39;:source:localsource_path:uploads\u0026#39;private\u0026#39;:source:localsource_path:privateWhile using the CLI and rsync are the most common solutions for uploading files to mounts, you can also use SCP . Platform.sh CLI The easiest way to import files to your project mounts is by using the Platform.sh CLI mount:upload command. To upload to each of directories above, we can use the following commands. platform mount:upload --mount web/uploads --source ./uploads platform mount:upload --mount private --source ./private rsync You can also use rsync to upload each directory. The platform ssh --pipe command will return the SSH URL for the current environment as an inline string that rsync can recognize. To use a non-default environment, use the -e switch after --pipe. Note that the trailing slash on the remote path means rsync will copy just the files inside the specified directory, not the directory itself. rsync -az ./private `platform ssh --pipe`:/app/private/ rsync -az ./web/uploads `platform ssh --pipe`:/app/web/uploads Note: If you’re running rsync on MacOS, you should add --iconv=utf-8-mac,utf-8 to your rsync call. See the rsync documentation for more details on how to adjust the upload process.",
        "section": "Tutorials",
        "subsections": " Preparation Push your code Import your database Import your files  Platform.sh CLI rsync    ",
        "image": "",
        "url": "/tutorials/migrating.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "621e172e4ec322e85de660d2cc247d72",
        "title": "Onboarding process",
        "description": "",
        "text": " On-Boarding a new Dedicated client is a three phase process that begins the moment your contract is closed with your sales representative. As part of the onboarding service, you will continue to work with the Solutions Engineer who was present during the technical discovery and analysis during the pre-sales phase. During the entire on-boarding process, your Solutions Engineer is available to assist with questions and prioritize any tickets that may be submitted through the help desk. Phase Meetings Description Setup Introduction Hand-off The Solutions Engineer briefly reviews the development workflow that was discussed during the sales process. A dedicated Slack channel for your team’s onboarding process will be set up and members of your team invited. This will allow for a quicker feedback loop during the onboarding process. The Solutions Engineer will introduce you to the Platform Dedicated workflow process, provision resources, and hand them off to the client. Development Developer Workflow Consultation The customer has access to all the resources necessary to develop, migrate, and test the project on the Platform.sh infrastructure - development, staging, and production. If necessary, developers may request an additional training session (held by the Solutions Engineer) to discuss the development workflow and best practices in detail. Once the application is live on your staging environment, a staging CDN distribution will be created so that proper configuration of your CDN can begin. Go-Live Pre-launch Debrief Customer notifies their Solutions Engineer through a support ticket of the intention to go live. The Solutions Engineer may also reach out to the customer a few days before the approximate cut-over date if one was provided. The Solutions Engineer reviews the infrastructure, notes any risks, and discusses the go live process. The production configuration of your CDN will be created with the configuration mirroring that of your staging distribution, and will be configured to pull from your production environment.",
        "section": "Platform.sh Dedicated",
        "subsections": "",
        "image": "",
        "url": "/dedicated/overview/onboarding.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "009340ee0a2d47e658a82896690cc72b",
        "title": "Performance tuning",
        "description": "",
        "text": " Once your application is up and running it still needs to be kept fast. Platform.sh offers a wide degree of flexibility in how PHP behaves, but that does mean you may need to take a few steps to ensure your site is running optimally. The following recommendations are guidelines only. They’re also listed in approximately the order we recommend investigating them, although your mileage may vary. Upgrade to PHP 7.2\u0026#43; There is very little purpose to trying to optimize a PHP application on PHP 5. PHP 7 is generally twice as fast and uses half as much memory as PHP 5, making it unquestionably the first step to take when trying to make a PHP-based site run faster. To change your PHP version, simply change the type key in your .platform.app.yaml to the desired PHP version. As always, test it on a branch first before merging to master. Ensure that the router cache is properly configured Although not PHP-specific, a common source of performance issues is a misconfigured cache. The most common issue is not whitelisting session cookies, which results in a site with any cookies at all, including from analytics tools, never being cached. See the router cache documentation, and the cookie entry specifically. You will also need to ensure that your application is sending the correct cache-control header. The router cache will obey whatever cache headers your application sends, so send it good ones. Static assets cache headers are set using the expires key in .platform.app.yaml. See the web.locations documentation for more details. Optimize the FPM worker count PHP-FPM reserves a fixed number of simultaneous worker processes to handle incoming requests. If more simultaneous requests are received than the number of workers then some requests will wait. The default worker count is deliberately set rather conservative but can be improved in many cases. See the PHP-FPM sizing page for how to determine and set a more optimal value. Enable preloading PHP 7.4 and later supports preloading code files into shared memory once at server startup, bypassing the need to include or autoload them later. Depending on your application doing so can result in significant improvements to both CPU and memory usage. If using PHP 7.4, see the PHP Preload instructions for how to configure it on Platform.sh and consult your application’s documentation to see if they have any recommendations for an optimal preload configuration. If you are not using PHP 7.4, this is a good reason to upgrade. Configure opcache PHP 5.5 and later include an opcache that is enabled at all times, as it should be. It may still need to be tuned, however. The opcache can be configured using php.ini values, which in this case are best set using the variables block in .platform.app.yaml. Note: If using opcache preloading on PHP 7.4 or later, configure that first and let the application run for a while before tuning the opcache itself as the preload script may change the necessary configuration here. The most important values to set are: opcache.max_accelerated_files: The max number of files that the opcache may cache at once. If this is lower than the number of files in the application it will begin thrashing and become less effective. opcache.memory_consumption: The total memory that the opcache may use. If the application is larger than this the cache will start thrashing and become less effective. To determine how many files you have, run this command from the root of your application: find . -type f -name \u0026#39;*.php\u0026#39; | wc -l That will report the number of files in your file tree that end in .php. That may not be perfectly accurate (some applications have PHP code in files that don’t end in .php, it may not catch generated files that haven’t been generated yet, etc.) but it’s a reasonable approximation. Set the opcache.max_accelerated_files option to a value slightly higher than this. Note that PHP will automatically round the value you specify up to the next highest prime number, for reasons long lost to the sands of time. Determining an optimal opcache.memory_consumption is a bit harder, unfortunately, as it requires executing code via a web request to get adequate statistics. Fortunately there is a command line tool that will handle most of that. Change to the /tmp directory (or any other non-web-accessible writable directory) and install CacheTool . It has a large number of commands and options but we’re only interested in the opcache status for FastCGI command. The really short version of downloading and using it would be: cd /tmp curl -sO http://gordalina.github.io/cachetool/downloads/cachetool.phar php cachetool.phar opcache:status --fcgi=$SOCKET The --fcgi=$SOCKET option tells the command how to connect to the PHP-FPM process on the server through the Platform.sh-defined socket. That command will output something similar to the following: \u0026#43;----------------------\u0026#43;---------------------------------\u0026#43; | Name | Value | \u0026#43;----------------------\u0026#43;---------------------------------\u0026#43; | Enabled | Yes | | Cache full | No | | Restart pending | No | | Restart in progress | No | | Memory used | 29.65 MiB | | Memory free | 34.35 MiB | | Memory wasted (%) | 0 b (0%) | \u0026#43;----------------------\u0026#43;---------------------------------\u0026#43; | Cached scripts | 1528 | | Cached keys | 2609 | | Max cached keys | 32531 | | Start time | Mon, 18 Jun 2018 18:19:32 \u0026#43;0000 | | Last restart time | Never | | Oom restarts | 0 | | Hash restarts | 0 | | Manual restarts | 0 | | Hits | 8554 | | Misses | 1594 | | Blacklist misses (%) | 0 (0%) | | Opcache hit rate | 84.29247142294 | \u0026#43;----------------------\u0026#43;---------------------------------\u0026#43; The most important values for now are the Memory used, Memory free, and Oom restarts (Out Of Memory Restarts). If the Oom restarts number is high (meaning more than a handful) it means you don’t have enough memory allocated to the opcache. In this example the opcache is using about half of the 64 MB given to it by default, which is fine. If Memory free is too low or Oom Restarts too high, set a higher value for the memory consumption. Remember to remove the cachetools.phar file once you’re done with it. Your .platform.app.yaml file will end up including a block similar to: variables:php:\u0026#39;opcache.max_accelerated_files\u0026#39;:22000\u0026#39;opcache.memory_consumption\u0026#39;:96(Memory consumption is set in megabytes.) Optimize your code It’s also possible that your own code is doing more work than it needs to. Profiling and optimizing a PHP application is a much larger topic than will fit here, but Platform.sh recommends enabling Blackfire.io on your project to determine what slow spots can be found and addressed. The web agency Pixelant has also published a log analyzer tool for Platform.sh . It works only for PHP scripts, but offers good visualizations and insights into the operation of your site that can suggest places to further optimize your configuration and provide guidance on when it’s time to increase your plan size. (Please note that this tool is maintained by a 3rd party, not by Platform.sh.)",
        "section": "PHP",
        "subsections": " Upgrade to PHP 7.2+ Ensure that the router cache is properly configured Optimize the FPM worker count Enable preloading Configure opcache Optimize your code  ",
        "image": "",
        "url": "/languages/php/tuning.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "2e1393762fdce12bafc0b2c1b2c269a5",
        "title": "Platform.sh development environments",
        "description": "",
        "text": " Architecture (Development Environments) Default limits The Development Environment for a Dedicated project provides a production and staging branch linked to the Dedicated Cluster, a master branch, and ten (10) additional active environments. This number can be increased if needed for an additional fee. The default storage for Dedicated contracts is 50GB per environment (production, staging, and each development environment) - this comprises total storage for your project and is inclusive of any databases, uploaded files, writable application logging directories, search index cores, and so on. The storage amount for your development environment will reflect the amount in your Enterprise contract. A project may have up to six (6) users associated with it at no additional charge. Additional users may be added for an additional fee. These users will have access to both the Development Environment and the Dedicated Cluster. Larger developments environments By default, all containers in development environments are “Small” sized, as they have limited traffic needs. For more resource-intensive applications this size can be increased for an additional fee.",
        "section": "Platform.sh Dedicated cluster specifications",
        "subsections": " Architecture (Development Environments) Default limits Larger developments environments  ",
        "image": "",
        "url": "/dedicated/architecture/development.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "ae98ba09f46790676990d7923db8b500",
        "title": "Pricing",
        "description": "",
        "text": " Platform.sh is the Idea-to-Cloud Application Platform—the end to end solution to develop and deploy web apps and sites. We offer a free trial period so you can test the service and see how great it is. You can see our full pricing information at: https://platform.sh/pricing/ All Platform.sh plans include: four Environments (3 for staging/development, 1 for the live site). one Developer license 5GB of Storage per environment multiple Backend services (MySQL, PostgreSQL, Elasticsearch, Redis, Solr..) support You can switch between plans (downgrade or upgrade) freely, but note that reducing storage is currently not supported for technical reasons. If you need to reduce storage, please create a support ticket. You will always be billed the prorated rate of your plan over the period it was used. You may cancel your plan at any time and you will only be billed for the actual period used. For Elite, Enterprise and Agency Plans you can pay by purchase order. For all other plans you need to add a credit card to your account. We offer a free trial period so you can test the service and see how great it is. If ever you need more time to evaluate Platform.sh, please contact our sales representatives. They can issue you an extra voucher to prolong your test. Prices below are listed in US Dollars. You will be billed in US Dollars, Euros, or British Pounds depending on where your billing address is. For a list of current prices please refer to https://platform.sh/pricing Euro Prices are presented excluding VAT. In your bill, as appropriate we will include the correct VAT rate. Extras All extra consumption is prorated to the time it was actually used. For example, if you added an extra developer for 10 days you would be billed around $3 extra at the end of the month (based on the then-current price of an extra developer seat). Extra developers Adding a developer to your project will add a monthly per project per user fee unless you have an agency or an enterprise account. Extra environments You can add extra staging/development environments to any plan by multiples of 3. For example, if you want to have 12 staging environments you would pay additional $63 per month on top of your basic plan price. Extra storage You can add additional storage at $2.50 per 5GB per staging/development environment. For example, if you have the default plan (with 3 staging environments) and you add 10GB (for a total of 15GB per environment), you would pay an extra $15 a month. If you added 3 extra environments (for a total of 6 staging environments) and you added 10GB (for a total of 15GB per environment), you would pay an extra $30 a month. Development The basic plan (Development) starts at $10 per month, and includes 4 environments: 3 staging/development and 1 future production). You can not map a custom domain name to a development plan Development environments have less resources than production environments. Production The live environment (master) of a production plan has more resources than the development environments of the project. https://platform.sh/pricing lists the resources available per plan (these are always only the production environment resources) the development environment have their own resources, and are not counted towards the limit. You can map domain names to your master environment. SSL support is always included. Multiple Applications in a single project All Platform.sh plans support multiple applications in a single cluster, but they share the global resources of the cluster. The resources of a Standard plan are not sufficient to run more than one application in the same cluster if there is also a MySQL database as a Service. Useful multi-apps start at Medium. A Medium plan, for example, can support 3 Apps with a MySQL instance and a Redis instance. If you wonder if a specific setup would fit in a plan, don’t hesitate to contact our support. Dedicated Instances For a price lower than traditional managed hosting, you get included development and staging environments, as well as triple redundancy on every element of the stack with: 99.99% Uptime Guaranteed 24/7 White Glove On-boarding and Support Please contact our sales department to discuss how we can help you. Agencies We offer three tiers for agencies with many perks. Free user licenses A free site for your own agency Up to 10% customer lifetime referral fees and 15% discounts Access to an agency speciﬁc “Small” price plan Free Medium or Large plan website for your own agency site Free Small plan for every Enterprise project sold …and more! Learn more and join today… German Cloud Pricing The prices for Germany are currently set at 10% above the EU and US plan prices. Thus, a “Production Standard” environment on the Sovereign German Cloud will be $55 instead of $50. Our estimation page (which you can reach by clicking on your account dashboard on the edit link for a project) does reflect these new options. If you have any questions don’t hesitate to contact our sales department .",
        "section": "The big picture",
        "subsections": " Extras  Extra developers Extra environments Extra storage   Development Production  Multiple Applications in a single project   Dedicated Instances Agencies German Cloud Pricing  ",
        "image": "",
        "url": "/overview/pricing.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "0c8b91d0dcd51e175732e47489963a0e",
        "title": "Redirects",
        "description": "",
        "text": " Managing redirection rules is a common requirement for web applications, especially in cases where you do not want to lose incoming links that have changed or been removed over time. You can manage redirection rules on your Platform.sh projects in two different ways, which we describe here. If neither of these options satisfy your redirection needs, you can still implement redirects directly from within your application, which if implemented with the appropriate caching headers would be almost as efficient as using the configuration options provided by Platform.sh. Whole-route redirects Using whole-route redirects, you can define very basic routes in your .platform/routes.yaml file whose sole purpose is to redirect. A typical use case for this type of route is adding or removing a www. prefix to your domain, as the following example shows: https://{default}/:type:redirectto:https://www.{default}/Partial redirects In the .platform/routes.yaml file you can also add partial redirect rules to existing routes: https://{default}/:# [...]redirects:expires:1dpaths:\u0026#39;/from\u0026#39;:to:\u0026#39;https://example.com/\u0026#39;\u0026#39;^/foo/(.*)/bar\u0026#39;:to:\u0026#39;https://example.com/$1\u0026#39;regexp:trueThis format is more rich and works with any type of route, including routes served directly by the application. Two keys are available under redirects: expires: optional, the duration the redirect will be cached. Examples of valid values include 3600s, 1d, 2w, 3m. paths: the paths to apply redirections to. Each rule under paths is defined by its key describing the expression to match against the request path and a value object describing both the destination to redirect to with detail on how to handle the redirection. The value object is defined with the following keys: to: required, a relative URL - \u0026#39;/destination\u0026#39;, or absolute URL - \u0026#39;https://example.com/\u0026#39;. regexp: optional, defaults to false. Specifies whether the path key should be interpreted as a PCRE regular expression. In the following example, a request to https://example.com/foo/a/b/c/bar would redirect to https://example.com/a/b/c: https://{default}/:type:upstreamredirects:paths:\u0026#39;^/foo/(.*)/bar\u0026#39;:to:\u0026#39;https://example.com/$1\u0026#39;regexp:trueNote that special arguments in the to statement are also valid when regexp is set to true: $is_args will evaluate to ? or empty string $args will evaluate to the full query string if any $arg_foo will evaluate to the value of the query parameter foo $uri will evaluate to the full URI of the request. prefix: optional, specifies whether we should redirect both the path and all its children or just the path itself. Defaults to true, but not supported if regexp is true. For example, https://{default}/:type:upstreamredirects:paths:\u0026#39;/from\u0026#39;:to:\u0026#39;https://{default}/to\u0026#39;prefix:truewith prefix set to true, /from will redirect to /to and /from/another/path will redirect to /to/another/path. If prefix is set to false then /from will trigger a redirect, but /from/another/path will not. append_suffix: optional, determines if the suffix is carried over with the redirect. Defaults to true, but not supported if regexp is true or if prefix is false. If we redirect with append_suffix set to false, for example, then the following https://{default}/:type:upstreamredirects:paths:\u0026#39;/from\u0026#39;:to:\u0026#39;https://{default}/to\u0026#39;append_suffix:falsewould result in /from/path/suffix redirecting to just /to. If append_suffix was left on its default value of true, then /from/path/suffix would have redirected to /to/path/suffix. code: optional, HTTP status code. Valid status codes are 301, 302, 307, and 308. Defaults to 302. https://{default}/:type:upstreamredirects:paths:\u0026#39;/from\u0026#39;:to:\u0026#39;https://example.com/\u0026#39;code:308\u0026#39;/here\u0026#39;:to:\u0026#39;https://example.com/there\u0026#39;In this example, redirects from /from would use a 308 HTTP status code, but redirects from /here would default to 302. expires: optional, the duration the redirect will be cached for. Defaults to the expires value defined directly under the redirects key, but at this level we can fine-tune the expiration of individual partial redirects: https://{default}/:type:upstreamredirects:expires:1dpaths:\u0026#39;/from\u0026#39;:to:\u0026#39;https://example.com/\u0026#39;\u0026#39;/here\u0026#39;:to:\u0026#39;https://example.com/there\u0026#39;expires:2wIn this example, redirects from /from would be set to expire in one day, but redirects from /here would expire in two weeks. Application-driven redirects If neither of the above options satisfy your redirection needs, you can still implement redirects directly in your application. If sent with the appropriate caching headers, this is nearly as efficient as implementing the redirect through one of the two configurations described above. Implementing application-driven redirects depends on your own code or framework and is beyond the scope of this documentation.",
        "section": "Configure routes",
        "subsections": " Whole-route redirects Partial redirects Application-driven redirects  ",
        "image": "",
        "url": "/configuration/routes/redirects.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "08c931477e4c4a3a1814ad94169096ba",
        "title": "Using Redis with Drupal 8.x",
        "description": "",
        "text": " If you are using the Platform.sh-provided Drupal 8 template, most of this work is already done for you. All you need to do is uncomment a the Redis relationship in .platform.app.yaml after your site is installed and Redis-based caching should “just work”. If you are working from an older repository or migrating a pre-built site to Platform.sh, see the instructions below. Requirements Add a Redis service First you need to create a Redis service. In your .platform/services.yaml file, add or uncomment the following: rediscache:type:redis:5.0That will create a service named rediscache, of type redis, specifically version 5.0. Expose the Redis service to your application In your .platform.app.yaml file, we now need to open a connection to the new Redis service. Under the relationships section, add the following: relationships:redis: rediscache:redis The key (left side) is the name that will be exposed to the application in the PLATFORM_RELATIONSHIPS variable . The right hand side is the name of the service we specified above (rediscache) and the endpoint (redis). If you named the service something different above, change rediscache to that. Add the Redis PHP extension Because the Redis extension for PHP has been known to have BC breaks at times, we do not bundle a specific verison by default. Instead, we provide a script to allow you to build your desired version in the build hook. See the PHP-Redis page for a simple-to-install script and instructions. Add the Drupal module You will need to add the Redis module to your project. If you are using Composer to manage your Drupal 8 site (which we recommend), simply run: composer require drupal/redis Then commit the resulting changes to your composer.json and composer.lock files. Note that the Redis module does not need to be enabled in Drupal except for diagnostic purposes. The configuration below is sufficient to leverage its functionality. Configuration To make use of the Redis cache you will need to set some Drupal variables. The configuration is a bit more complex than can be easily represented in Platform.sh’s environment variables configuration, so using settings.php directly is the recommended approach. Place the following at the end of settings.platformsh.php. Note the inline comments, as you may wish to customize it further. Also review the README.txt file that comes with the redis module, as it has a great deal more information on possible configuration options. For instance, you may wish to not use Redis for the persistent lock if you have a custom module that needs locks to persist for more than a few seconds. The example below is intended as a “most common case”. (Note: This example assumes Drupal 8.2 or later.) Note: If you do not already have the Platform.sh Config Reader library installed and referenced at the top of the file, you will need to install it with composer require platformsh/config-reader and then add the following code before the block below: \u0026lt;?php $platformsh = new if (!$platformsh-\u0026gt;inRuntime()) { return; } \u0026lt;?php // Set redis configuration. if ($platformsh-\u0026gt;hasRelationship(\u0026#39;redis\u0026#39;) \u0026amp;\u0026amp; !drupal_installation_attempted() \u0026amp;\u0026amp; extension_loaded(\u0026#39;redis\u0026#39;)) { $redis = $platformsh-\u0026gt;credentials(\u0026#39;redis\u0026#39;); // Set Redis as the default backend for any cache bin not otherwise specified. $settings[\u0026#39;cache\u0026#39;][\u0026#39;default\u0026#39;] = \u0026#39;cache.backend.redis\u0026#39;; $settings[\u0026#39;redis.connection\u0026#39;][\u0026#39;host\u0026#39;] = $redis[\u0026#39;host\u0026#39;]; $settings[\u0026#39;redis.connection\u0026#39;][\u0026#39;port\u0026#39;] = $redis[\u0026#39;port\u0026#39;]; // Apply changes to the container configuration to better leverage Redis. // This includes using Redis for the lock and flood control systems, as well // as the cache tag checksum. Alternatively, copy the contents of that file // to your project-specific services.yml file, modify as appropriate, and // remove this line. $settings[\u0026#39;container_yamls\u0026#39;][] = \u0026#39;modules/contrib/redis/example.services.yml\u0026#39;; // Allow the services to work before the Redis module itself is enabled. $settings[\u0026#39;container_yamls\u0026#39;][] = \u0026#39;modules/contrib/redis/redis.services.yml\u0026#39;; // Manually add the classloader path, this is required for the container cache bin definition below // and allows to use it without the redis module being enabled. \u0026#39;modules/contrib/redis/src\u0026#39;); // Use redis for container cache. // The container cache is used to load the container definition itself, and // thus any configuration stored in the container itself is not available // yet. These lines force the container cache to use Redis rather than the // default SQL cache. $settings[\u0026#39;bootstrap_container_definition\u0026#39;] = [ \u0026#39;parameters\u0026#39; =\u0026gt; [], \u0026#39;services\u0026#39; =\u0026gt; [ \u0026#39;redis.factory\u0026#39; =\u0026gt; [ \u0026#39;class\u0026#39; =\u0026gt; ], \u0026#39;cache.backend.redis\u0026#39; =\u0026gt; [ \u0026#39;class\u0026#39; =\u0026gt; \u0026#39;arguments\u0026#39; =\u0026gt; [\u0026#39;@redis.factory\u0026#39;, \u0026#39;@cache_tags_provider.container\u0026#39;, \u0026#39;@serialization.phpserialize\u0026#39;], ], \u0026#39;cache.container\u0026#39; =\u0026gt; [ \u0026#39;class\u0026#39; =\u0026gt; \u0026#39;factory\u0026#39; =\u0026gt; [\u0026#39;@cache.backend.redis\u0026#39;, \u0026#39;get\u0026#39;], \u0026#39;arguments\u0026#39; =\u0026gt; [\u0026#39;container\u0026#39;], ], \u0026#39;cache_tags_provider.container\u0026#39; =\u0026gt; [ \u0026#39;class\u0026#39; =\u0026gt; \u0026#39;arguments\u0026#39; =\u0026gt; [\u0026#39;@redis.factory\u0026#39;], ], \u0026#39;serialization.phpserialize\u0026#39; =\u0026gt; [ \u0026#39;class\u0026#39; =\u0026gt; ], ], ]; } The example.services.yml file noted above will also use Redis for the lock and flood control systems. The redis module is able to use Redis as a queue backend, however, that should not be done on an ephemeral Redis instance as that could result in lost items when the Redis service instance is restarted or fills up. If you wish to use Redis for the queue we recommend using a separate persistent Redis instance. See the Redis documentation page for more information. Verifying Redis is running Run this command in a SSH session in your environment redis-cli -h redis.internal info. You should run it before you push all this new code to your repository. This should give you a baseline of activity on your Redis installation. There should be very little memory allocated to the Redis cache. After you push this code, you should run the command and notice that allocated memory will start jumping. Clear SQL cache tables Once you’ve confirmed that your site is using Redis for caching, you can and should purge any remaining cache data in the MySQL database as it is now just taking up space. TRUNCATE any table that begins with cache except for cache_form. Despite its name cache_form is not part of the cache system proper and thus should not be moved out of SQL.",
        "section": "Getting Started",
        "subsections": " Requirements  Add a Redis service Expose the Redis service to your application Add the Redis PHP extension Add the Drupal module   Configuration  Verifying Redis is running Clear SQL cache tables    ",
        "image": "",
        "url": "/frameworks/drupal8/redis.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "02c57db0ac1a6df1b24d9946022cdcd9",
        "title": "Backups",
        "description": "",
        "text": " Platform.sh takes a byte-for-byte snapshot of Dedicated production environments every six (6) hours. They are retained on a sliding scale, so more recent time frames have more frequent backups. Time frame Backup retention Days 1-3 Every backup Days 4-6 One backup per day Weeks 2-6 One backup per week Weeks 8-12 One bi-weekly backup Weeks 12-22 One backup per month Platform.sh Dedicated creates the backup using snapshots to encrypted elastic block storage (EBS) volumes. An EBS snapshot is immediate, but the time it takes to write to the simple storage service (S3) depends on the volume of changes. Recovery Point Objective (RPO) is 6 hours (maximum time to last backup). Recovery Time Objective (RTO) depends on the size of the storage. Large EBS volumes take more time to restore. These backups are only used in cases of catastrophic failure and can only be restored by Platform.sh. A ticket must be opened by the customer to request a restoration. The restoration process may take a few hours, depending on the infrastructure provider in use. In the ticket, specify if you want backups of files, MySQL, or both. Uploaded files will be placed in an SSH-accessible directory on the Dedicated Cluster. MySQL will be provided as a MySQL dump file on the server. You may restore these to your site at your leisure. (We will not proactively overwrite your production site with a backup; you are responsible for determining a “safe” time to restore the backup, or for selectively restoring individual files if desired.) Customers are welcome to make their own backups using standard tools (mysqldump, rsync, etc.) at their own leisure.",
        "section": "Platform.sh Dedicated",
        "subsections": "",
        "image": "",
        "url": "/dedicated/overview/backups.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "689d8aa9f60b16a6ddb7716c5f61e120",
        "title": "Configure your application",
        "description": "",
        "text": " You control your application and the way it will be built and deployed on Platform.sh via a single configuration file, .platform.app.yaml, located at the root of your application folder inside your Git repository. The .platform.app.yaml file is extremely flexible. Depending on your needs it could be less than 10 lines long or over 100. The only required keys are name, type, disk, and at least one “instance definition”, either a web or worker block. All others are optional. Your application code can generate one or more application instances. Web instances can be accessed from the outside world, while workers cannot and just run a persistent background process. Otherwise they are very similar. Different configuration properties can be applied to individual web and worker instances, or globally to all of them. In the most typical case, with one web instance and no workers, it’s common to just list each of the configuration directives below as a top-level property. However, they can also be specified within the web or worker blocks to apply to just those instances. The following properties apply only at the global level, and cannot be replicated inside an instance definition. name (required) - Sets the unique name of the application container. type (required) - Sets the container base image to use, including application language. timezone - Sets the timezone of cron tasks in application container. build, dependencies, and hooks - Control how the application gets compiled. Note that this compilation happens before the application is copied into different instances, so any steps here will apply to all web and worker instances. cron - Defines scheduled tasks for the application. Cron tasks will, technically, run as part of the web instance regardless of how many workers are defined. source.root - This nested value specifies the path where all code for the application lives. It defaults to the directory where the .platform.app.yaml file is defined. It is rarely needed except in advanced configurations. The following properties can be set at the top level of the .platform.app.yaml file and apply to all application instances, or set within a given instance definition and apply just to that one. If set in both places then the instance-specific one will take precedence, and completely replace the global one. That is, if you want to make a change to just one sub-property of one of the following keys you need to replicate the entire block. size - Sets an explicit sizing hint for the application. relationships - Defines connections to other services and applications. access - Restricts SSH access with more granularity than the management console. disk and mounts (required) - Defines writable file directories for the application. variables - Sets environment variables that control application behavior. firewall - Defines outbound firewall rules for the application. The .platform.app.yaml file needs at least one of the following to define an instance, but may define both. web - Controls how the web application is served. worker - Defines alternate copies of the application to run as background processes. Available resources Each web or worker instance is its own running container, which takes its own resources. The size key allows some control over how many resources each container gets and if omitted the system will select one of a few fixed sizes for each container automatically. All application and service containers are given resources out of a common pool defined by your plan size. That means the more containers you define, the fewer resources each one will get and you may need to increase your plan size. Compression Platform.sh does not compress any dynamic responses generated by your application due to a well known security issue . While your application can compress its own response, doing so when the response includes any user-specific information, including a session cookie, opens up an attack vector over SSL/TLS connections. For that reason we recommend against compressing any generated responses. Requests for static files that are served directly by Platform.sh are compressed automatically using either gzip or brotli compression if: The request headers for the file support gzip or brotli. The file is served directly from disk by Platform.sh, not passed through your application. The file would be served with a cache expiration time in the future. The file type is one of: html, javascript, json, pdf, postscript, svg, css, csv, plain text, or XML. Additionally, if a file with a “.gz” or “.br” extension exists that will be served instead for the appropriate compression type regardless of the file type. That is, a request for styles.css that accepts a gzipped file (according to the request headers) will automatically return the contents of styles.css.gz if it exists. This approach supports any file type and offers some CPU optimization, especially if the cache lifetime is short. Example configuration An example of a minimalist .platform.app.yaml file for PHP, heavily commented, is below: # .platform.app.yaml# The name of this application, which must be unique within a project.name:'app'# The type key specifies the language and version for your application.type:'php:7.0'# On PHP, there are multiple build flavors available. Pretty much everyone# except Drupal 7 users will want the composer flavor.build:flavor:composer# The relationships of the application with services or other applications.# The left-hand side is the name of the relationship as it will be exposed# to the application in the PLATFORM_RELATIONSHIPS variable. The right-hand# side is in the form `\u003cservice name\u003e:\u003cendpoint name\u003e`.relationships:database:'mysqldb:mysql'# The hooks that will be triggered when the package is deployed.hooks:# Build hooks can modify the application files on disk but not access any services like databases.build:| rm web/app_dev.php# Deploy hooks can access services but the file system is now read-only.deploy:| app/console --env=prod cache:clear# The size of the persistent disk of the application (in MB).disk:2048# The 'mounts' describe writable, persistent filesystem mounts in the application.# The keys are directory paths relative to the application root. The values are a# mount definition. In this case, `web-files` is just a unique name for the mount.mounts:'web/files':source:localsource_path:'web-files'# The configuration of the application when it is exposed to the web.web:locations:'/':# The public directory of the application relative to its root.root:'web'# The front-controller script which determines where to send# non-static requests.passthru:'/app.php'# Allow uploaded files to be served, but do not run scripts.# Missing files get mapped to the front controller above.'/files':root:'web/files'scripts:falseallow:truepassthru:'/app.php' Note: This configuration file is specific to one application. If you have multiple applications inside your Git repository (such as a RESTful web service and a front-end, or a main web site and a blog), you need .platform.app.yaml at the root of each application. See the Multi-app documentation. Upgrading from previous versions of the configuration file. Although we make an effort to always maintain backward compatibility in the .platform.app.yaml format, we do from time to time upgrade the file and encourage you to upgrade as well.",
        "section": "Configuration",
        "subsections": " Available resources Compression Example configuration Upgrading from previous versions of the configuration file.  ",
        "image": "",
        "url": "/configuration/app.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "24a0f103d7c4174594f3d60822bddb4f",
        "title": "Create a new project",
        "description": "",
        "text": " With the Platform.sh CLI now installed and configured to communicate with your account, you can create a new project from the command line and connect it to your application. Create an empty project Type the command platform create in your terminal. The CLI will then ask you to set up some initial project configurations: Project title: You need a unique name for each project, so title this one My CLI Project. Region: In general you will choose the region that is closest to where most of your site’s traffic is coming from. Here, go ahead and begin typing us-2.platform.sh and the CLI will auto-complete the rest for you. Plan: Select the development plan for your trial project. Environments: The master branch will become the Master environment, the live production environment for your application. Additionally, other branches may be activated as fully running environments for developing new features. More on that later . This value selects the maximum number of development environments the project will allow. You can change this value later at any time. For now, press Enter to select the default number of environments. Storage: You can modify the amount of storage your application can use from the CLI and from the management console, as well as upgrade that storage later once your project starts growing. For now, press Enter to select the default amount of storage. When the CLI has finished creating a project, it will output your project ID. This is the primary identifier for making changes to your projects, and you will need to use it to set Platform.sh as the remote for your repository in the next step. You can also retrieve the project ID with the command platform project:list, which lists all of your projects and their IDs in a table. Set Platform.sh as remote for your application Next you will need to connect to the remote project in order to push your code to Platform.sh. If you have not already initialized your project directory as a Git repository, you will first need to do so git init Then you can set Platform.sh as a remote with the command platform project:set-remote \u0026lt;project ID\u0026gt; That’s it! You have now created an empty project and connected your repository to that project using the CLI. Move on now to the next step to start configuring your repository to deploy on Platform.sh. Back I\u0026#39;ve created a project",
        "section": "Getting started",
        "subsections": "",
        "image": "",
        "url": "/gettingstarted/gettingstarted/own-code/create-project.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "48d3eea65894ade75808c6e3159467c7",
        "title": "Cron timezone",
        "description": "",
        "text": " All Platform.sh containers default to running in UTC time. Applications and application runtimes may elect to use a different timezone but the container itself runs in UTC. That includes the spec parameter for cron tasks that are defined by the application. That is generally fine but sometimes it’s necessary to run cron tasks in a different timezone. Setting the system timezone for cron tasks The timezone property sets the timezone for which the spec property of any cron tasks defined by the application will be interpreted. Its value is one of the tz database region codes such as Europe/Paris or America/New_York. This key will apply to all cron tasks defined in that file. This entry is only meaningful on cron specs that specify a particular time of day, rather than a “time past each hour”. For example, 25 1 * * * would run every day at 1:25 am in the timezone specified. Setting an application runtime timezone The application runtime timezone can also be set, although the mechanism varies a bit by the runtime. PHP runtime - You can change the timezone by providing a custom php.ini . Node.js runtime - You can change the timezone by starting the server with env TZ=\u0026#39;\u0026lt;timezone\u0026gt;\u0026#39; node server.js. Python runtime - You can change the timezone by starting the server with env TZ=\u0026#39;\u0026lt;timezone\u0026gt;\u0026#39; python server.py. Java runtime - You can change the timezone by starting the server with env TZ=\u0026#39;\u0026lt;timezone\u0026gt;\u0026#39; java -jar .... An alternative to setting an environment variable is setting the JVM argument user.timezone. This JVM argument takes precedence over the environment variable TZ. For example, you can use the flag -D when running the application: java -jar -Duser.timezone=GMT or java -jar -Duser.timezone= Asia/Kolkata  Setting the application timezone will only affect the application itself, not system operations such as log files. Note: In the vast majority of cases it’s best to leave all timezones in UTC and store user data with an associated timezone instead.",
        "section": "Configure your application",
        "subsections": " Setting the system timezone for cron tasks Setting an application runtime timezone  ",
        "image": "",
        "url": "/configuration/app/timezone.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "4d05bae40d9b853cc53632f9ea3a06b4",
        "title": "Customize settings.php",
        "description": "",
        "text": " For applications using the drupal build flavor (those based on our Drupal 7 example ), Platform.sh automatically generates a settings.php file if not present and will always generate a settings.local.php file. This allows the Drupal site to be connected to MySQL without any additional configuration. If you wish to customize either file, we recommend instead using the example files provided in our Drupal 7 project template. There are two: settings.php and settings.platformsh.php . The former will automatically include the latter, and all Platform.sh-specific configuration is found in the settings.platformsh.php file. It will also automatically include a settings.local.php file if found so it will not conflict with your local development workflow. Note: You should never commit a settings.local.php file to your repository. If you need to add additional configuration that is specific to Platform.sh, such as connecting to additional services like Redis or Solr , those changes should go in the settings.platformsh.php file.",
        "section": "Getting Started",
        "subsections": "",
        "image": "",
        "url": "/frameworks/drupal7/customizing-settings-php.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "69f9345e04cb90015861b6a12ec23a63",
        "title": "Data retention",
        "description": "",
        "text": " Platform.sh logs and stores all sorts of data as a normal part of its business. This information is retained as needed for business purposes and old data is purged. The retention time varies depending on the type of data stored. Account information Information relating to customer accounts (login information, billing information, etc.) is retained for as long as the account is active with Platform.sh. Customers may request that their account be deleted and all related data be purged by filing an issue ticket. System logs System level access and security logs are maintained by Platform.sh for diagnostic purposes. These logs are not customer-accessible. These logs are retained for at least 6 months and at most 1 year. General system level logs are retained for at least 30 days and at most 1 year. Payment processing logs Logs related to payment processing are retained for at least 3 months and at most 1 year. This is consistent with PCI recommendations. Application logs Application logs on each customer environment are retained with the environment. Individual log files are truncated at 100 MB, regardless of their age. See the accessing logs page for instructions on how to access them. When an environment is deleted its application logs are deleted as well. Grid Backups Application backups running on the Grid (e.g. If you subscribe to a Platform.sh Professional plan) are retained for at least 7 days. They will be purged between 7 days and 6 months, at Platform.sh’s discretion. Dedicated backups Backups for applications running on a Dedicated instance will follow the schedule documented on our dedicated backups page. Tombstone backups When a project is deleted Platform.sh takes a final backup of active environments as well as the Git repository holding user code. This final backup is to allow Platform.sh to recover a recently-deleted project in case of accident. These “tombstone” backups are retained for between 7 days and 6 months. Analytics Platform.sh uses Google Analytics on various web pages, and therefore Google Analytics will store collected data for a period of time. We have configured our Google Analytics account to store data for 14 months from the time you last accessed our site, which is the minimum Google allows.",
        "section": "Security and compliance",
        "subsections": " Account information System logs Payment processing logs Application logs Grid Backups Dedicated backups Tombstone backups Analytics  ",
        "image": "",
        "url": "/security/data-retention.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "1807729f93a685bf2af4ae9b6011922b",
        "title": "Docksal",
        "description": "",
        "text": " Using Docksal for local development Docksal is a docker based local development tool that plays nicely with Platform.sh. It is maintained by a community of developers and is a viable option for most Platform.sh projects. See the Docksal documentation for installing and setting up Docksal on your system. Docksal will ask you to create a .docksal directory in your application root, which functions similarly to the .platform.app.yaml file. It is safe to check this directory into your Git repository as Platform.sh will simply ignore it. Docksal Fin (fin) is a command line tool for controlling Docksal and used for interacting with a Docksal project. For more information on the use of the fin command, type the help subcommand to get all available commands and options. fin help Using Platform.sh CLI within Docksal The SECRET_PLATFORMSH_CLI_TOKEN must be set to use the Platform.sh CLI within your Docksal project. This is an API Token found with your Platform.sh account and can be generated by going to the API Tokens page and clicking the Create API Token link. This will allow for you to interact with your Platform.sh account from within the CLI container. fin config set --global SECRET_PLATFORMSH_CLI_TOKEN=XXX Pulling a Platform.sh project The Docksal CLI ships with the Platform.sh CLI tool. To use the tool and pull a project locally, make sure you have uploaded your SSH key to your Platform.sh account. Once that is done and a SECRET_PLATFORMSH_CLI_TOKEN has been added using the above step, you can set up your project with the following instructions. Note: Replace PROJECT_ID with your project’s ID, which can found within the Platform.sh dashboard. Replace PROJECT_DIRECTORY with the name of the local directory you’d like the project cloned into. If you do not already have Platform.sh CLI installed locally, you can use the one in the CLI image. The advantage of this would mean that the tool would never have to be installed locally and therefore is one less dependency. fin run-cli \u0026#39;platform get PROJECT_ID -e master PROJECT_DIRECTORY\u0026#39; If you already have Platform.sh CLI installed locally, you can use that instead. platform get PROJECT_ID -e master PROJECT_DIRECTORY Initializing a Platform.sh project To start a new Docksal project, initialize the configuration with the fin config generate command and specify the docroot flag. fin config generate --docroot=web fin project start The web directory is one of the many different items that can be set for the document. If this is different or changes over time running the following will fix this. fin config set docroot=XXX # Replacing XXX with the new document root. Customizing a Platform.sh project By default, Docksal comes configured with a PHP 7.1 container, an Apache 2.4 web container, and a MySQL 5.6 database container. Additional versions are available in the images and you can set the desired versions by setting following variables within your .docksal/docksal.env file. # Apache Versions 2.2 / 2.4 #WEB_IMAGE=\u0026#39;docksal/web:2.1-apache-2.2\u0026#39; WEB_IMAGE=\u0026#39;docksal/web:2.1-apache-2.4\u0026#39; # MySQL Version: 5.6 / 5.7 / 8.0 #DB_IMAGE=\u0026#39;docksal/db:1.2-mysql-5.6\u0026#39; DB_IMAGE=\u0026#39;docksal/db:1.2-mysql-5.7\u0026#39; #DB_IMAGE=\u0026#39;docksal/db:1.2-mysql-8.0\u0026#39; # PHP Versions Available 5.6 / 7.0 / 7.1 / 7.2 #CLI_IMAGE=\u0026#39;docksal/cli:2.5-php5.6\u0026#39; #CLI_IMAGE=\u0026#39;docksal/cli:2.5-php7.0\u0026#39; CLI_IMAGE=\u0026#39;docksal/cli:2.5-php7.1\u0026#39; #CLI_IMAGE=\u0026#39;docksal/cli:2.5-php7.2\u0026#39; You can further create and customize a .docksal/docksal.yml file within your project. This is a docker-compose file and can be customized as needed for your application, as some customizations are specific to certain applications. See Docksal documentation on extending stock images . Downloading MySQL data from Platform.sh into Docksal In most cases, downloading data from Platform.sh and loading it into your project is straightforward. The following commands, run from your application root, will download a compressed database backup and load it into the local Docksal database container. fin platform db:dump --gzip -f /tmp/database.sql.gz fin exec \u0026#39;zcat \u0026lt; /tmp/database.sql.gz | mysql -u user -puser -h db default\u0026#39; Connecting Projects to the Database After importing your database into the project the next step is connecting to the database server. The following information can be used for setting up a connection for your application. Key Value DB Name default Username user Password user Host db Port 3306 See the exporting tutorial for information on how to use rsync.",
        "section": "Set up your local development environment",
        "subsections": " Using Platform.sh CLI within Docksal Pulling a Platform.sh project Initializing a Platform.sh project Customizing a Platform.sh project Downloading MySQL data from Platform.sh into Docksal  Connecting Projects to the Database    ",
        "image": "",
        "url": "/development/local/docksal.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "fd096e5b720aedc538db711f4e5f85da",
        "title": "Getting help",
        "description": "",
        "text": " If you’re facing any issue with Platform.sh, you can submit a support ticket from the Platform.sh management console. You will be redirected to a list of your support tickets, and you can click to open a ‘New ticket’ at the top of the page. File your issues in the fields for the new ticket and Submit. You can also open a support ticket directly from your user account . You are more than welcome to hop-on to our public Slack chat channel . And you are always invited to drop us a line at our contact form .",
        "section": "The big picture",
        "subsections": "",
        "image": "",
        "url": "/overview/getting-help.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "179af012a85ecc13da4e7e9137b2e002",
        "title": "Going Live - Troubleshooting",
        "description": "",
        "text": " If all steps above have been followed and the site still does not resolve (after waiting for the DNS update to propagate), here are a few simple self-help steps to take before contacting support. Verify DNS On the command line with OS X or Linux (or using the Linux subsystem for Windows) type host www.example.com: The response should be something like: www.example.com is an alias for master-t2xxqeifuhpzg.eu.platform.sh. master-t2xxqeifuhpzg.eu.platform.sh has address 54.76.136.188 If it is not either you have not configured correctly your DNS server, or the DNS configuration did not propagate yet. As a first step you can try and remove your local DNS cache. You can also try to set your DNS server to the Google public DNS server (8.8.8.8/8.8.4.4) to see if the issue is with the DNS server you are using. Try to run ping www.example.com (with you own domain name) if the result is different from what you got form the host www.example.com you might want to verify your /etc/hosts file (or its windows equivalent), you might have left there an entry from testing. Verify SSL On the command line with OS X or Linux (or using the Linux subsystem for Windows) type curl -I -v https://example.com (again using your own domain): The response should be long. Look for error messages. They are usually explicit enough. Often the problem will be with a mismatch between the certificate and the domain name. Verify your application On the command line type platform logs app and see there are no clear anomalies there. Do the same with platform logs error Something still wrong ? Contact support We are here to help. Please include as much detail as possible (we will be able to provide quicker help).",
        "section": "Going live",
        "subsections": " Verify DNS Verify SSL Verify your application Something still wrong ? Contact support  ",
        "image": "",
        "url": "/golive/troubleshoot.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "00e511700dd45ace2bc6e6e2bf0794b7",
        "title": "Hibernate",
        "description": "",
        "text": " Hibernate ORM is an object-relational mapping tool for the Java programming language. It provides a framework for mapping an object-oriented domain model to a relational database. Hibernate handles object-relational impedance mismatch problems by replacing direct, persistent database accesses with high-level object handling functions. Services The configuration reader library for Java is used in these examples, so be sure to check out the documentation for installation instructions and the latest version. MySQL MySQL is an open-source relational database technology. Define the driver for MySQL , and the Java dependencies. Then determine the SessionFactory client programmatically: import org.hibernate.Session; import org.hibernate.SessionFactory; import org.hibernate.Transaction; import org.hibernate.cfg.Configuration; import sh.platform.config.Config; import sh.platform.config.Hibernate; public class HibernateApp { public static void main(String[] args) { Config config = new Config(); Configuration configuration = new Configuration(); configuration.addAnnotatedClass(Address.class); final Hibernate credential = config.getCredential( database , Hibernate::new); final SessionFactory sessionFactory = credential.getMySQL(configuration); try (Session session = sessionFactory.openSession()) { Transaction transaction = session.beginTransaction(); //... transaction.commit(); } } } Note: You can use the same MySQL driver for MariaDB as well if you wish to do so. MariaDB MariaDB is an open-source relational database technology. Define the driver for MariaDB , and the Java dependencies. Then determine the SessionFactory client programmatically: import org.hibernate.Session; import org.hibernate.SessionFactory; import org.hibernate.Transaction; import org.hibernate.cfg.Configuration; import sh.platform.config.Config; import sh.platform.config.Hibernate; public class HibernateApp { public static void main(String[] args) { Config config = new Config(); Configuration configuration = new Configuration(); configuration.addAnnotatedClass(Address.class); final Hibernate credential = config.getCredential( database , Hibernate::new); final SessionFactory sessionFactory = credential.getMariaDB(configuration); try (Session session = sessionFactory.openSession()) { Transaction transaction = session.beginTransaction(); //... transaction.commit(); } } } PostgreSQL PostgreSQL is an open-source relational database technology. Define the driver for PostgreSQL , and the Java dependencies. Then determine the SessionFactory client programmatically: import org.hibernate.Session; import org.hibernate.SessionFactory; import org.hibernate.Transaction; import org.hibernate.cfg.Configuration; import sh.platform.config.Config; import sh.platform.config.Hibernate; public class HibernateApp { public static void main(String[] args) { Config config = new Config(); Configuration configuration = new Configuration(); configuration.addAnnotatedClass(Address.class); final Hibernate credential = config.getCredential( database , Hibernate::new); final SessionFactory sessionFactory = credential.getPostgreSQL(configuration); try (Session session = sessionFactory.openSession()) { Transaction transaction = session.beginTransaction(); //... transaction.commit(); } } }",
        "section": "Featured frameworks",
        "subsections": " Services  MySQL MariaDB PostgreSQL    ",
        "image": "",
        "url": "/frameworks/hibernate.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "837460cf59fb1618fa1dfb808c2df277",
        "title": "Kafka (Message queue service)",
        "description": "",
        "text": " Apache Kafka is an open-source stream-processing software platform. It is a framework for storing, reading and analyzing streaming data. See the Kafka documentation for more information. Supported versions Grid Dedicated 2.1 2.2 2.3 2.4 None available Relationship The format exposed in the $PLATFORM_RELATIONSHIPS environment variable : {  cluster :  rjify4yjcwxaa-master-7rqtwti ,  host :  kafka.internal ,  hostname :  wsxz7kjwfkb3j6eh2sdzfubrry.kafka.service._.eu-3.platformsh.site ,  ip :  169.254.252.225 ,  port : 9092,  rel :  kafka ,  scheme :  kafka ,  service :  kafka ,  type :  kafka:2.2  } Usage example In your .platform/services.yaml: queuekafka:type:kafka:2.4disk:512 In your .platform.app.yaml: relationships:kafkaqueue: queuekafka:kafka  Note: You will need to use the kafka type when defining the service # .platform/services.yamlservice_name:type:kafka:versiondisk:256 and the endpoint kafka when defining the relationship # .platform.app.yamlrelationships:relationship_name:“service_name:kafka” Your service_name and relationship_name are defined by you, but we recommend making them distinct from each other. You can then use the service in a configuration file of your application with something like: Java Python Ruby package sh.platform.languages.sample; import org.apache.kafka.clients.consumer.Consumer; import org.apache.kafka.clients.consumer.ConsumerConfig; import org.apache.kafka.clients.consumer.ConsumerRecords; import org.apache.kafka.clients.producer.Producer; import org.apache.kafka.clients.producer.ProducerConfig; import org.apache.kafka.clients.producer.ProducerRecord; import org.apache.kafka.clients.producer.RecordMetadata; import sh.platform.config.Config; import sh.platform.config.Kafka; import java.time.Duration; import java.util.HashMap; import java.util.Map; import java.util.function.Supplier; public class KafkaSample implements Supplier\u0026lt;String\u0026gt; { @Override public String get() { StringBuilder logger = new StringBuilder(); // Create a new config object to ease reading the Platform.sh environment variables. // You can alternatively use getenv() yourself. Config config = new Config(); try { // Get the credentials to connect to the Kafka service. final Kafka kafka = config.getCredential( kafka , Kafka::new); Map\u0026lt;String, Object\u0026gt; configProducer = new HashMap\u0026lt;\u0026gt;(); configProducer.putIfAbsent(ProducerConfig.CLIENT_ID_CONFIG,  animals ); final Producer\u0026lt;Long, String\u0026gt; producer = kafka.getProducer(configProducer); // Sending data into the stream. RecordMetadata metadata = producer.send(new ProducerRecord\u0026lt;\u0026gt;( animals ,  lion )).get(); logger.append( Record sent with to partition  ).append(metadata.partition()) .append(  with offset metadata = producer.send(new ProducerRecord\u0026lt;\u0026gt;( animals ,  dog )).get(); logger.append( Record sent with to partition  ).append(metadata.partition()) .append(  with offset metadata = producer.send(new ProducerRecord\u0026lt;\u0026gt;( animals ,  cat )).get(); logger.append( Record sent with to partition  ).append(metadata.partition()) .append(  with offset // Consumer, read data from the stream. final HashMap\u0026lt;String, Object\u0026gt; configConsumer = new HashMap\u0026lt;\u0026gt;(); configConsumer.put(ConsumerConfig.GROUP_ID_CONFIG,  consumerGroup1 ); configConsumer.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG,  earliest ); Consumer\u0026lt;Long, String\u0026gt; consumer = kafka.getConsumer(configConsumer,  animals ); ConsumerRecords\u0026lt;Long, String\u0026gt; consumerRecords = consumer.poll(Duration.ofSeconds(3)); // Print each record. consumerRecords.forEach(record -\u0026gt; { logger.append( Record: Key   \u0026#43; record.key()); logger.append(  value   \u0026#43; record.value()); logger.append(  partition   \u0026#43; record.partition()); logger.append(  offset   \u0026#43; }); // Commits the offset of record to broker. consumer.commitSync(); return logger.toString(); } catch (Exception exp) { throw new RuntimeException( An error when execute Kafka , exp); } } } from json import dumps from json import loads from kafka import KafkaConsumer, KafkaProducer from platformshconfig import Config def usage_example(): # Create a new Config object to ease reading the Platform.sh environment variables. # You can alternatively use os.environ yourself. config = Config() # Get the credentials to connect to the Kafka service. credentials = config.credentials(\u0026#39;kafka\u0026#39;) try: kafka_server = \u0026#39;{}:{}\u0026#39;.format(credentials[\u0026#39;host\u0026#39;], credentials[\u0026#39;port\u0026#39;]) # Producer producer = KafkaProducer( bootstrap_servers=[kafka_server], value_serializer=lambda x: dumps(x).encode(\u0026#39;utf-8\u0026#39;) ) for e in range(10): data = {\u0026#39;number\u0026#39; : e} producer.send(\u0026#39;numtest\u0026#39;, value=data) # Consumer consumer = KafkaConsumer( bootstrap_servers=[kafka_server], auto_offset_reset=\u0026#39;earliest\u0026#39; ) consumer.subscribe([\u0026#39;numtest\u0026#39;]) output = \u0026#39;\u0026#39; # For demonstration purposes so it doesn\u0026#39;t block. for e in range(10): message = next(consumer) output \u0026#43;= str(loads(message.value.decode(\u0026#39;UTF-8\u0026#39;))[ number ]) \u0026#43; \u0026#39;, \u0026#39; # What a real implementation would do instead. # for message in consumer: # output \u0026#43;= loads(message.value.decode(\u0026#39;UTF-8\u0026#39;))[ number ] return output except Exception as e: return e ## With the ruby-kafka gem # Producer require  kafka  kafka = Kafka.new([ kafka.internal:9092 ], client_id:  my-application ) kafka.deliver_message( Hello, World! , topic:  greetings ) # Consumer kafka.each_message(topic:  greetings ) do |message| puts message.offset, message.key, message.value end (The specific way to inject configuration into your application will vary. Consult your application or framework’s documentation.)",
        "section": "Configure services",
        "subsections": " Supported versions Relationship Usage example  ",
        "image": "",
        "url": "/configuration/services/kafka.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "4a5c71ddcdd2c3a97bcafcf4359a9b05",
        "title": "Management console",
        "description": "",
        "text": " Platform.sh provides a responsive management console which allows you to interact with your projects and manage your environments. Everything you can do with the management console you can also achieve with the CLI (Command Line Interface) . Environment List From your project’s main page, each of the environments are available from the pull-down menu ENVIRONMENT at the top of the page. There is also a graphic view of your environments on the right hand side, where you can view your environments as a list or as a project tree. The name of the environment is struck out if it’s been disabled. If it has an arrow next to it, this means the environment has children. Environments Once you select an environment, the management console can give you a great deal of information about it. Activity Feed The management console displays all the activity happening on your environments. You can filter messages per type. Header Within a project’s environment, the management console exposes 4 main actions and 4 drop-down command options that you can use to interface with your environments. Branch Branching an environment means creating a new branch in the Git repository, as well as an exact copy of that environment. The new branch includes code, all of the data that is stored on disk (database, Solr indexes, uploaded files, etc.), and also a new copy of the running services (and their configuration) that the application needs. This means that when you branch an environment, you also branch the complete infrastructure. During a branch, three things happen: A new branch is created in Git. The application is rebuilt on the new branch, if necessary. The new branch is deployed. After clicking Branch a dialog box will appear that will provide commands to execute future merges from the command line using the Platform.sh CLI. Merge Merging an environment means introducing the code changes from a branch to its parent branch and redeploying the parent. During a merge: The code changes are merged via Git to the parent branch. The application is rebuilt on the parent branch, if necessary. The parent branch is deployed. Rebuilding the application is not necessary if the same code was already built (for any environment): in this case you will see the message Slug already built for this tree id, skipping. After clicking Merge a dialog box will appear that will provide commands to execute future merges from the command line using the Platform.sh CLI. Sync Synchronization performs a merge from a parent into a child environment, and then redeploys that environment. You have the option of performing a Sync on only the code, replacing the data (i.e. databases) of that environment from its parent, or both. These options are provided in a separate dialog box that will appear when you click the Sync button, along with the Platform.sh CLI commands that perform the same action. Be aware that sync uses the Snapshot mechanism and will have the same caveats. Be aware that sync uses the Backup mechanism and will have the same caveats. Note that Sync is only available if your branch has no unmerged commits, and can be fast-forwarded. It is good practice to take a backup of your environment before performing a synchronization. Backup Creating a backup for an environment means saving a copy of the database so that it can be restored. You will see the backup in the activity feed of you environment in the Platform.sh management console where you can trigger the restore by clicking on the restore link. After clicking Backup a dialog box will appear that will provide commands to execute future merges from the command line using the Platform.sh CLI. You can also use the CLI with: $ platform environment:backup to create a backup, and $ platform environment:restore to restore an existing backup. URLs The URLs pull-down exposes the domains that can be used to access application environments from the web. GIT The Git pull-down displays the commands to use to clone the codebase via Git. CLI The CLI pull-down displays the commands to get your project set up locally with the Platform.sh CLI. SSH The SSH pull-down display the commands to access your project over SSH. Configuration settings From the management console you can also view information about how your routes, services, and applications are currently configured for the environment. At the top of the page, click the “Services” tab. Applications Select the application container on the left to show more detailed information for it on the right. The “Overview” tab gives you metadata information regarding the application. It tells you what size container it has been configured for, the amount of persistent disk, the number of active workers and cron jobs, as well as the command to ssh into that container. Each cron job associated with the application is listed with its frequency, the last time it was run, it’s status, and its command. The “Configuration” tab provides an overview of the application’s configuration pulled from its .platform.app.yaml file. Services Each service has a tab on the left, so select the one you are interested in. The overview tab gives you metadata information regarding the service. It tells you what size container it has been configured for and the amount of persistent disk given to it in your services.yaml file. The “Configuration” tab provides an overview of the service configuration that has been pulled from the services.yaml file. Routes Each route will appear when you select the Routes tab on the left and describe its type and whether caching and SSI have been enabled for it.",
        "section": "Administration",
        "subsections": " Environment List Environments  Activity Feed Header Configuration settings    ",
        "image": "",
        "url": "/administration/web.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "d00747ffdd16c906126219f9efbad199",
        "title": "Next steps",
        "description": "",
        "text": " In this guide you learned how to create and activate live feature environments, test them, and merge them into production safely using backups . Don’t stop now! There are many more features that make Platform.sh helpful to developers. Developing on Platform.sh The next guide shows how to set up your development workflow to benefit from Platform.sh. Local development Remotely connect to services and build your application locally during development. Additional Resources External Integrations Configure Platform.sh to mirror every push and pull request on GitHub, Gitlab, and Bitbucket. Going Live Set up your site for production, configure domains, and go live! Platform.sh Community Portal Check out how-tos, tutorials, and get help for your questions about Platform.sh. Platform.sh Blog Read news and how-to posts all about working with Platform.sh. Back",
        "section": "Getting started",
        "subsections": "   Developing on Platform.sh Additional Resources    ",
        "image": "",
        "url": "/gettingstarted/developing/dev-environments/next-steps.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "e861d84a0ca9ef0dd2ed9962461025de",
        "title": "Next steps",
        "description": "",
        "text": " In this guide you opened an SSH tunnel to your Platform.sh project and built your application locally. Don’t stop now! There are far more features that make Platform.sh profoundly helpful to developers that you have left to explore. Developing on Platform.sh Consult these additional resources to help improve your development life cycle. Development environments Activate development branches and test new features before merging into production. Additional Resources External Integrations Configure Platform.sh to mirror every push and pull request on GitHub, Gitlab, and Bitbucket. Going Live Set up your site for production, configure domains, and go live! Platform.sh Community Portal Check out how-tos, tutorials, and get help for your questions about Platform.sh. Platform.sh Blog Read news and how-to posts all about working with Platform.sh. Back",
        "section": "Getting started",
        "subsections": "   Developing on Platform.sh Additional Resources    ",
        "image": "",
        "url": "/gettingstarted/developing/local-development/next-steps.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "c1f9f10db9df5ee536f34a6d4e93a013",
        "title": "Next steps",
        "description": "",
        "text": "Now that your application is up and running, here are some additional pieces of information that will help you leverage every bit of technology Platform.sh has to offer.",
        "section": "Introduction",
        "subsections": "",
        "image": "",
        "url": "/gettingstarted/next-steps.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "9876ce8aa32050ca2db85566db1aff57",
        "title": "Next Steps: Requirements for the CLI",
        "description": "",
        "text": " With the management console you can start new projects from templates just as you did in the previous steps, but deploying your own applications requires you to also use the Platform.sh CLI . Before you install it there are a few requirements that must be met first. Git Git is the open source version control system used by Platform.sh. Any change you make to your Platform.sh project will need to be committed via Git. You can see all the Git commit messages of an environment in the Environment Activity feed of the management console for each project you create. Before getting started, make sure you have Git installed on your computer. SSH key pair Once your account has been set up and the CLI is installed, Platform.sh needs one additional piece of information about your computer so that you can access your projects from the command line. If you are unfamiliar with how to generate an SSH public and private key, there are instructions in the documentation about how to do so . Add your SSH public key to your account Add your SSH public key to your Platform.sh account so that you can communicate with your projects using the CLI. Access SSH key settings in the management console From the management console, move to the top right hand corner of the screen and click the dropdown menu to the left of the settings gear box icon. In the menu, click on Account. This next page lists all of your active projects, which now includes My First Project. Click on the Account Settings link at the top of the page, then click the SSH keys tab to the left of your account information. Add your SSH public key to your account Click the \u0026#43; Add public key button in the top right hand corner of the screen. This will open up another window with two fields. Name the key with something memorable, like home-computer, and in the field below that, paste the content of your public key. When you have finished, click Save to save the key. That’s it! Now that you have met the requirements and configured an SSH key, all that’s left is to install the Platform.sh CLI so you can interact with your projects from the command line. Back I\u0026#39;ve added my public SSH key",
        "section": "Getting started",
        "subsections": "   Git SSH key pair Add your SSH public key to your account    ",
        "image": "",
        "url": "/gettingstarted/gettingstarted/template/cli-requirements.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "b23fc77d8afd2f22b8cc81959e3700d3",
        "title": "PHP.ini settings",
        "description": "",
        "text": " There are two ways to customize php.ini values for your application. The recommended method is to use the variables property of .platform.app.yaml to set ini values using the php prefix. For example, to increase the PHP memory limit you’d put the following in .platform.app.yaml: variables:php:memory_limit:256MIt’s also possible to provide a custom php.ini file in the repository in the root of the application (where your .platform.app.yaml file is). ; php.ini ; Increase PHP memory limit memory_limit = 256M Another example is to set the timezone of the PHP runtime (though, the timezone settings of containers/services would remain in UTC): variables:php: date.timezone :  Europe/Paris or ; php.ini ; Set PHP runtime timezone date.timezone =  Europe/Paris  Environment-specific php.ini configuration directives can be provided via environment variables separately from the application code. See the note in the Environment variables section. Disabling functions A common recommendation for securing a PHP installation is to disable certain built-in functions that are frequently used in remote attacks. By default, Platform.sh does not disable any functions as they all do have some legitimate use in various applications. However, you may wish to disable them yourself if you know they are not needed. For example, to disable pcntl_exec and pcntl_fork (which are not usable in a web request anyway): variables:php: disable_functions :  pcntl_exec,pcntl_fork Common functions to disable include: create_function - create_function has no useful purpose since PHP 5.3 and should not be used, ever. It has been effectively replaced by anonymous functions. exec,passthru,shell_exec,system,proc_open,popen - These functions all allow a PHP script to run a bash shell command. That is rarely used by web applications, although build scripts may need them. pcntl_exec,pcntl_fork,pcntl_setpriority - The pcntl_* functions (including those not listed here) are responsible for process management. Most of them will cause a fatal error if used within a web request. Cron tasks or workers may make use of them, however. Most are safe to disable unless you know that you are using them. curl_exec,curl_multi_exec - These functions allow a PHP script to make arbitrary HTTP requests. Note that they are frequently used by other HTTP libraries such as Guzzle, in which case you should not disable them. show_source - This function shows a syntax highlighted version of a named PHP source file. That is rarely useful outside of development. Naturally if your application does make use of any of these functions, it will fail if you disable them. In that case, do not disable them. Default php.ini settings The default values for some frequently-modified php.ini settings are listed below. memory_limit=128M post_max_size=64M upload_max_filesize=64M display_errors=On This value is on by default to ease setting up a project on Platform.sh. We strongly recommend providing a custom error handler in your application or setting this value to Off before you make your site live. zend.assertions=-1 Assertions are optimized out of existence and have no impact at runtime. You should have assertions set to 1 for your local development system. opcache.memory_consumption=64 This is the number of megabytes available for the opcache. Large applications with many files may want to increase this value. opcache.validate_timestamps=On The opcache will check for updated files on disk. This is necessary to support applications that generate compiled PHP code from user configuration. If you are certain your application does not do so then you can disable this setting for a small performance boost. Warning: We do not limit what you can put in your php.ini file, but many settings can break your application. This is a facility for advanced users.",
        "section": "PHP",
        "subsections": " Disabling functions Default php.ini settings  ",
        "image": "",
        "url": "/languages/php/ini.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "be1c958468274a09d81d789f9a94a006",
        "title": "Platform.sh Third-Party Resources",
        "description": "",
        "text": " This is a Big List of known third party resources for Platform.sh. These resources are not vetted by Platform.sh, but may be useful for people working with the platform. Blogs “ The future of the PHP PaaS is here: Our journey to Platform.sh  , by Marcus Hausammann An introduction to Platform.sh from Chris Ward Guides Getting started \u0026amp; workflow Set up your Mac for Platform.sh using MAMP by @owntheweb How Platform.sh can simplify your contribution workflow on GitHub by Mickaël Andrieu from Akeneo A guide in French on deploying to Platform.sh by Thomas Asnar [FR] Nacho Digital has a guide on moving an existing site to Platform.sh All the stuff you need for a pro-dev-flow using platform.sh as your deploy target again by https://www.thinktandem.io Working with Platform.sh How to connect to your MySQL database using Sequel Pro How to set up XDebug Official Symfony documentation on deploying to Platform.sh Official Sylius documentation on deploying to Platform.sh How to install Apache Tika on Platform.sh How to store complete logs at AWS S3 by Contextual Code Automated SSL Certificates Export on Platform.sh by Contextual Code A Platform.sh region migration tool by Contextual Code Drupal Modifying distribution make files for Platform.sh Platform.sh Drupal 8 Development Workflow by @JohnatasJMO Syslogging is not supported on Platform.sh, instead, you can Log using Monolog to keep log files out of the database (and/or use whatever processors \u0026amp; handlers you want) Magento Deploying Magento 2 with Redis on Platform.sh by @rafaelcgstz Sylius The Sylius documentation has a solid set of instructions for setting up Sylius with Platform.sh. Examples Platform.sh lists maintained examples on its Github page, with some cross-referencing from http://docs.platform.sh. Examples listed below could work fine, or may be out-of-date or unmaintained. Use at your own risk. NodeJS Framework Credit Date added MEAN stack @OriPekelman May 2017 Python Framework Credit Date added Python Flask using gunicorn @etoulas May 2017 Odoo Open Source ERP and CRM @OriPekelman May 2017 PHP Framework Credit Date added Akeneo example @maciejzgadzaj May 2017 API Platform with a ReactJS client admin @GuGuss May 2017 Backdrop example @gmoigneu May 2017 Headless Drupal 8 with Angular @GuGuss May 2017 Headless Drupal 8 with React.js @systemseed Aug 2018 Joomla example @gmoigneu May 2017 Laravel example @JGrubb May 2017 Moodle example @JGrubb May 2017 Mouf framework example The Coding Machine May 2017 Flow Framework support package Dominique Feyer Jul 2017 Neos CMS support package Dominique Feyer Jul 2017 Silex example @JGrubb May 2017 Silverstripe example @gmoigneu May 2017 Thunder example maintained by the MD Systems team May 2017 WooCommerce example @Liip May 2017 Grav example Mike Crittenden August 2017 Ruby Framework Credit Date added Jekyll example @JGrubb May 2017 Rust Framework Credit Date added Rust with Rocket and webasm Royall Spence July 2018 Integrations Integrate GitLab with Platform.sh using Gitlab-CI , by @Axelerant Running Behat tests from CircleCI to a Platform.sh environment , by Matt Glaman Platform.sh’s original (unsupported) scripts for GitLab https://gist.github.com/pjcdawkins/0b3f7a6da963c129030961f0947746c4. Platform.sh now supports Gitlab natively. An adapter from platform.sh webhook to slack incoming webhook that can be hosted on a platform.sh app https://github.com/hanoii/platformsh2slack How to call the NewRelic API on deploy (by @christopher-hopper) A helper utility for running browser based tests on CircleCI against a Platform.sh environment. https://github.com/xendk/dais Tools \u0026amp; development MySQL disk space monitor https://github.com/galister/platformsh_mysqlmon Create deploy commands you can run from composer , using Symfony A small tool from Hanoii https://github.com/hanoii/drocal Script to sync a Drupal site from Production to Local https://github.com/pjcdawkins/platformsh-sync Matt Pope’s Platform.sh automated mysql and files backup script Development environments Beetbox , a pre-provisioned L*MP stack for Drupal and other frameworks, with Platform.sh CLI integration A Docker image with the Platform.sh CLI on it https://github.com/maxc0d3r/docker-platformshcli Some tips on using Platform.sh with DrupalVM https://github.com/geerlingguy/drupal-vm/issues/984 Vagrant with Ansible for Platform.sh, opinionated towards Drupal, by @mglaman. Ansible Playbook for setting up Vagrant and VirtualBox for use with a Platform.sh project PixelArt’s Platform.sh CLI role",
        "section": "Tutorials",
        "subsections": " Blogs Guides  Getting started \u0026amp; workflow Working with Platform.sh Drupal Magento Sylius   Examples  NodeJS Python PHP Ruby Rust   Integrations Tools \u0026amp; development  Development environments Ansible    ",
        "image": "",
        "url": "/tutorials/third-party.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "bfdf2db69e1bf1a8e36868a9183e196f",
        "title": "Scalability",
        "description": "",
        "text": " Part of the original design goal of Platform.sh’s Triple Redundant Architecture was to ensure scalability in times of load spikes outside of the bounds of the original traffic specs. Because the cluster is configured as an N\u0026#43;1 architecture, we can respond to legitimate traffic events by removing a node from the cluster, upsizing it, returning it into rotation, and then repeating the process on the next node in turn. Scaling Process and Procedure The scaling process is not automatic and requires manual effort. It may be initiated in two ways. On customer request via a ticket. We strongly recommend notifying us ahead of time if you know a large traffic event is coming (a major product launch, Black Friday, etc.) We cannot guarantee a turnaround time on a resizing unless given prior notice. High load incidents detected by our monitoring system. If the load is diagnosed to be due to a bot or crawler that we are able to block, we will attempt to block it. This prevents unnecessary scaling, which prevents unnecessary costs to you. If it is not a bot or is not blockable, then we will begin the upscaling process detailed above. Be advised that this process may take up to 60-90 minutes depending on the diagnostic steps needed. We will open a support ticket to notify you of such changes, but we will not wait for your response before upscaling your cluster. The uptime of your application is our top priority and reactive scaling events are part of how we ensure that we meet the obligations of our Service Level Agreement. You may opt-out of the upsizing service if you wish, but outages caused by high-traffic will not be considered to violate the Service Level Agreement.",
        "section": "Platform.sh Dedicated cluster specifications",
        "subsections": " Scaling Process and Procedure  ",
        "image": "",
        "url": "/dedicated/architecture/scalability.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "2b03d4587f37d8fe4745bda9ca5942a4",
        "title": "Server Side Includes",
        "description": "",
        "text": " Server side includes is a powerful mechanism by which you can at the same time leverage caching and serve dynamic content. You can activate or deactivate SSI on a per-route basis in your .platform/routes.yaml for example:  https://{default}/ :type:upstreamupstream: app:http cache:enabled:falsessi:enabled:true https://{default}/time.php :type:upstreamupstream: app:http cache:enabled:trueIt allows you to include in your HTML response directives that will make the server “fill-in” parts of the HTML respecting the caching you setup. For example you could in a dynamic non-cached page include a block that would have been cached for example in the /index.php page we would have: \u0026lt;?php echo date(DATE_RFC2822); ?\u0026gt; \u0026lt;!--#include virtual= time.php  --\u0026gt; and in time.php we had \u0026lt;?php header( Cache-Control: max-age=600 ); echo date(DATE_RFC2822); And you visit the home page you will see, as you refresh the page, the time on the top will continue to change, while the one on the bottom will only change every 600 seconds. For more on SSI functionality see the nginx documentation .",
        "section": "Configure routes",
        "subsections": "",
        "image": "",
        "url": "/configuration/routes/ssi.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "1f11414305f90c1ab394928de7df7b7d",
        "title": "Set up your local development environment",
        "description": "",
        "text": " While Platform.sh is great as a tool for hosting an application during both development and production, it’s naturally not the ideal place to edit code. You can’t, in fact, as the file system is read-only (as it should be). The proper place to edit your code is on your computer. You must have an SSH key already configured on your account, and have both Git and the Platform.sh CLI installed before continuing. Download the code If you don’t already have a local copy of your project’s code, run platform get to download one. You can also run platform projects to list all of the projects in your account. ~/htdocs $ platform projects Your projects are: +---------------+----------------------------+------------------------------------------------+ | ID | Name | URL | +---------------+----------------------------+------------------------------------------------+ | [project-id] | New Platform Project | https://eu.platform.sh/#/projects/[project-id] | +---------------+----------------------------+------------------------------------------------+ Get a project by running platform get [id]. List a project's environments by running platform environments. Now you can download the code using platform get [project-id] [folder-name]: ~/htdocs $ platform get [project-id] my-project Cloning into 'my-project/repository'... remote: counting objects: 11, done. Receiving objects: 100% (11/11), 1.36 KiB | 0 bytes/s, done. Checking connectivity... done. You should now have a repository folder, based on what you used for [folder-name] in the platform get command above. You will also notice a new directory in your project, .platform/local, which is excluded from Git. This directory contains builds and any local metadata about your project needed by the CLI. Building the site locally Run the platform build command to run through the same build process as would be run on Platform.sh. That will produce a _www directory in your project root that is a symlink to the currently active build in the .platform/local/builds folder. It should be used as the document root for your local web server. ~/htdocs/my-project $ platform build Building application myapp (runtime type: php) Beginning to build ~/htdocs/my-project/project.make. drupal-7.38 downloaded. drupal patched with install-redirect-on-empty-database-728702-36.patch. Generated PATCHES.txt file for drupal platform-7.x-1.3 downloaded. Running post-build hooks Symlinking files from the 'shared' directory to sites/default Build complete for application myapp Web root: ~/htdocs/my-project/_www ~/htdocs/my-project $ Be aware, of course, that the platform build command will run locally, and so require whatever appropriate runtime or other tools you specify. It may also result in packages referenced in your dependendencies block being installed on your local computer. If that is undesireable, a local virtual machine will let you create an enclosed local development environment that won’t affect your main system. Running the code Platform.sh supports whatever local development environment you wish to use. There is no dependency on any particular tool so if you already have a local development workflow you’re comfortable with you can keep using it without changes. That’s the “ untethered ” option. For quick changes, you can also run your code locally but use the services hosted on Platform.sh. That is, your site is “ tethered ” to Platform.sh. While this approach requires installing less on your system it can be quite slow as all communication with the database or cache server will need to travel from your computer to Platform.sh’s servers. Specific documentation is also available for the local development tools Lando and Docksal , which support most applications that Platform.sh supports.",
        "section": "Development",
        "subsections": " Download the code Building the site locally Running the code  ",
        "image": "",
        "url": "/development/local.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "ef5e8c58797815555eff6508ecb15a90",
        "title": "Set your domain",
        "description": "",
        "text": " You will need to configure your registered domain on your Platform.sh project before going live, and you can do that either through the management console or by using the CLI. Through the management console Now that you have changed your project to a production plan, you can click the same “Go live” button at the top of the project page. Alternatively, you can click “Settings” at the top of the page, and then visit the “Domains” section on the left. Click the “Add\u0026#43;” button in the top right hand corner of the page, enter your registered domain and select if you want it to be the default domain for the project. You can add multiple domains to a project, but only one can be set as the default. When you’re finished, click “Add domain”, and the project will once again redeploy to apply your changes. Using the CLI You can also add a domain to your project using the Platform.CLI. From a terminal window, type the command platform domain:add example.com --project \u0026lt;project ID\u0026gt; The CLI will validate your registered domain and provision Let’s Encrypt certificates for it. Back I have configured my registered domain",
        "section": "Getting started",
        "subsections": " Through the management console Using the CLI  ",
        "image": "",
        "url": "/gettingstarted/next-steps/going-live/set-domain.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "0ea7fa3e24cff48f1088f37b4aaa70d0",
        "title": "Single sign-on (SSO)",
        "description": "",
        "text": " Platform.sh allows you to set up mandatory SSO with a third-party identity provider (IdP) for all your users. Your SSO provider can be enabled for a specific email domain, for example @example.com. Every user with a matching email address will have to log in or register on Platform.sh using your SSO provider. Such users will not be able to use an alternative provider, or register a password, or change their email address. Mitigation controls If you deactivate a user on your identity provider, they will not be able to log in or register on Platform.sh. If the user is already logged in to Platform.sh, they will be automatically deactivated after their access token has expired (generally after 1 hour). A deactivated user will no longer be able to use SSH, Git, or other Platform.sh APIs. Service users If you have a service user with an email address under your SSO domain, e.g. machine-user@example.com, you can whitelist that user so that they will not be required to authenticate through your identity provider. Please open a support ticket if you need to whitelist a user. SSO providers Google Tier availability This feature is only available to Enterprise customers. Compare the Platform.sh tiers on our pricing page, or contact our sales team for more information. Enforce your users to authenticate with your OpenID Connect provider. Please open a support ticket to enable SSO with your OpenID Connect provider. OpenId Connect Tier availability This feature is only available to Elite customers. Compare the Platform.sh tiers on our pricing page, or contact our sales team for more information. Enforce your users to authenticate with Google. Please open a support ticket to enable Google SSO.",
        "section": "Administration",
        "subsections": " Mitigation controls Service users SSO providers  Google OpenId Connect    ",
        "image": "",
        "url": "/administration/sso.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "cbde876fb2cc1fa9b4df4e59d5483039",
        "title": "Using Memcached with Drupal 8.x",
        "description": "",
        "text": " Platform.sh recommends using Redis for caching with Drupal 8 over Memcached, as Redis offers better performance when dealing with larger values as Drupal tends to produce. However, Memcached is also available if desired and is fully supported. Requirements Add a Memcached service First you need to create a Memcached service. In your .platform/services.yaml file, add or uncomment the following: cacheservice:type:memcached:1.4That will create a service named cacheservice, of type memcached, specifically version 1.4. Expose the Memcached service to your application In your .platform.app.yaml file, we now need to open a connection to the new Memcached service. Under the relationships section, add the following: relationships:cache: cacheservice:memcached The key (left side) is the name that will be exposed to the application in the PLATFORM_RELATIONSHIPS variable . The right hand side is the name of the service we specified above (cacheservice) and the endpoint (memcached). If you named the service something different above, change cacheservice to that. Add the Memcached PHP extension You will need to enable the PHP Memcached extension. In your .platform.app.yaml file, add the following right after the type block: # Additional extensionsruntime:extensions:- memcachedAdd the Drupal module You will need to add the Memcache module to your project. If you are using Composer to manage your Drupal 8 site (which we recommend), simply run: composer require drupal/memcache Then commit the resulting changes to your composer.json and composer.lock files. Note: You must commit and deploy your code before continuing, then enable the module. The memcache module must be enabled before it is configured in the settings.platformsh.php file. Configuration The Drupal Memcache module must be configured via settings.platformsh.php. Place the following at the end of settings.platformsh.php. Note the inline comments, as you may wish to customize it further. Also review the README.txt file that comes with the memcache module, as it has a more information on possible configuration options. For instance, you may want to consider using memcache for locking as well and configuring cache stampede protection. The example below is intended as a “most common case”. \u0026lt;?php if (getenv(\u0026#39;PLATFORM_RELATIONSHIPS\u0026#39;) \u0026amp;\u0026amp; extension_loaded(\u0026#39;memcached\u0026#39;)) { $relationships = json_decode(base64_decode(getenv(\u0026#39;PLATFORM_RELATIONSHIPS\u0026#39;)), TRUE); // If you named your memcached relationship something other than  cache , set that here. $relationship_name = \u0026#39;cache\u0026#39;; if (!empty($relationships[$relationship_name])) { // This is the line that tells Drupal to use memcached as a backend. // Comment out just this line if you need to disable it for some reason and // fall back to the default database cache. $settings[\u0026#39;cache\u0026#39;][\u0026#39;default\u0026#39;] = \u0026#39;cache.backend.memcache\u0026#39;; foreach ($relationships[$relationship_name] as $endpoint) { $host = sprintf( %s:%d , $endpoint[\u0026#39;host\u0026#39;], $endpoint[\u0026#39;port\u0026#39;]); $settings[\u0026#39;memcache\u0026#39;][\u0026#39;servers\u0026#39;][$host] = \u0026#39;default\u0026#39;; } } // By default Drupal starts the cache_container on the database. The following // code overrides that. // Make sure that the $class_load-\u0026gt;addPsr4 is pointing to the right location of // the memcache module. The value below should be correct if memcache was installed // using Drupal Composer. $memcache_exists = class_exists(\u0026#39;Memcache\u0026#39;, FALSE); $memcached_exists = class_exists(\u0026#39;Memcached\u0026#39;, FALSE); if ($memcache_exists || $memcached_exists) { \u0026#39;modules/contrib/memcache/src\u0026#39;); // If using a multisite configuration, adapt this line to include a site-unique // value. $settings[\u0026#39;memcache\u0026#39;][\u0026#39;key_prefix\u0026#39;] = getenv(\u0026#39;PLATFORM_ENVIRONMENT\u0026#39;); // Define custom bootstrap container definition to use Memcache for cache.container. $settings[\u0026#39;bootstrap_container_definition\u0026#39;] = [ \u0026#39;parameters\u0026#39; =\u0026gt; [], \u0026#39;services\u0026#39; =\u0026gt; [ \u0026#39;database\u0026#39; =\u0026gt; [ \u0026#39;class\u0026#39; =\u0026gt; \u0026#39;factory\u0026#39; =\u0026gt; \u0026#39;arguments\u0026#39; =\u0026gt; [\u0026#39;default\u0026#39;], ], \u0026#39;settings\u0026#39; =\u0026gt; [ \u0026#39;class\u0026#39; =\u0026gt; \u0026#39;factory\u0026#39; =\u0026gt; ], \u0026#39;memcache.settings\u0026#39; =\u0026gt; [ \u0026#39;class\u0026#39; =\u0026gt; \u0026#39;arguments\u0026#39; =\u0026gt; [\u0026#39;@settings\u0026#39;], ], \u0026#39;memcache.factory\u0026#39; =\u0026gt; [ \u0026#39;class\u0026#39; =\u0026gt; \u0026#39;arguments\u0026#39; =\u0026gt; [\u0026#39;@memcache.settings\u0026#39;], ], \u0026#39;memcache.backend.cache.container\u0026#39; =\u0026gt; [ \u0026#39;class\u0026#39; =\u0026gt; \u0026#39;factory\u0026#39; =\u0026gt; [\u0026#39;@memcache.factory\u0026#39;, \u0026#39;get\u0026#39;], \u0026#39;arguments\u0026#39; =\u0026gt; [\u0026#39;container\u0026#39;], ], \u0026#39;lock.container\u0026#39; =\u0026gt; [ \u0026#39;class\u0026#39; =\u0026gt; \u0026#39;arguments\u0026#39; =\u0026gt; [\u0026#39;container\u0026#39;, \u0026#39;@memcache.backend.cache.container\u0026#39;], ], \u0026#39;cache_tags_provider.container\u0026#39; =\u0026gt; [ \u0026#39;class\u0026#39; =\u0026gt; \u0026#39;arguments\u0026#39; =\u0026gt; [\u0026#39;@database\u0026#39;], ], \u0026#39;cache.container\u0026#39; =\u0026gt; [ \u0026#39;class\u0026#39; =\u0026gt; \u0026#39;arguments\u0026#39; =\u0026gt; [\u0026#39;container\u0026#39;, \u0026#39;@memcache.backend.cache.container\u0026#39;, \u0026#39;@lock.container\u0026#39;, \u0026#39;@memcache.config\u0026#39;, \u0026#39;@cache_tags_provider.container\u0026#39;], ], ], ]; } }",
        "section": "Getting Started",
        "subsections": " Requirements  Add a Memcached service Expose the Memcached service to your application Add the Memcached PHP extension Add the Drupal module   Configuration  ",
        "image": "",
        "url": "/frameworks/drupal8/memcached.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "f2243791752080ebe4b295db34326cbd",
        "title": "Vouchers",
        "description": "",
        "text": " Applying a voucher to your project If you receive a Platform.sh voucher code, you can redeem it as follows: Go to your Account Settings, logging in if necessary, via the link in the top-right corner of these docs, or via this link: https://accounts.platform.sh/user . In the left navigation click on “VOUCHERS” On the page click on “Add a voucher code” Enter the code and click on the “ADD CODE” button Et voilà! Your account will now be credited with additional dollars pounds or euros. If you are assessing Platform.sh for your organization and think that you could benefit from a little more oomf in your test project why not contact us to request a voucher? You can tell us more at: https://platform.sh/contact/",
        "section": "Management console",
        "subsections": " Applying a voucher to your project  ",
        "image": "",
        "url": "/administration/web/vouchers.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "dece15b1c992422b272d240405960133",
        "title": " Working with Drush in Drupal 7",
        "description": "",
        "text": " Drush is a command-line shell and scripting interface for Drupal, a veritable Swiss Army knife designed to make life easier for those who spend their working hours hacking away at the command prompt. You can use the CLI to set up Drush aliases, to easily run Drush commands on specific remote Platform.sh environments. See the documentation on Drush in Drupal 8 for installation, drush aliases, and other general information. The installation procedure is the same for both Drupal 7 and 8. Drush make Platform.sh can automatically build your Drupal 7 site using Drush make files. This allows you to easily test specific versions, apply patches and keep your site up to date. It also keeps your working directory much cleaner as since it only contains your custom code. Your make file can be called: project.make or drupal-org.make. A basic make file looks like this: api = 2 core = 7.x ; Drupal core. projects[drupal][type] = core projects[drupal][version] = 7.67 projects[drupal][patch][] =  https://drupal.org/files/issues/install-redirect-on-empty-database-728702-36.patch  ; Drush make allows a default sub directory for all contributed projects. defaults[projects][subdir] = contrib ; Platform indicator module. projects[platform][version] = 1.4 When building as a profile, you need a make file for Drupal core called: project-core.make: api = 2 core = 7.x projects[drupal][type] = core Generate a make file from an existing site If you want to generate a make file from your existing site, you can run: $ drush make-generate project.make This will output a make file containing all your contributed modules, themes and libraries. Note: Make generate command Apply patches You can apply contributed patches to your modules, themes or libraries within your project.make: projects[features][version] =  2.2  projects[features][patch][] =  https://www.drupal.org/files/issues/alter_overrides-766264-45.patch  You can also apply self-hosted patches. Simply create a PATCHES folder at the root of your repository and add the patch as follow: projects[uuid][version] =  1.0-alpha5  projects[uuid][patch][] =  PATCHES/fix-non-uuid-entity-load.patch  Work with a DEV version When you are using a module that is in a DEV version, the best practice is to always target a specific commit ID so that you’re always building the same “version” of the module: ; CKEditor module: version 7.x-1.15\u0026#43;2-dev projects[ckeditor][download][revision] =  b29372fb446b547825dc6c30587eaf240717695c  projects[ckeditor][download][type] =  git  projects[ckeditor][download][branch] =  7.x-1.x  projects[ckeditor][type] =  module ",
        "section": "Getting Started",
        "subsections": " Drush make  Generate a make file from an existing site Apply patches Work with a DEV version    ",
        "image": "",
        "url": "/frameworks/drupal7/drush.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "3f3246dd84a0486dab2e0258351a3488",
        "title": "Configuring projects",
        "description": "",
        "text": " In the previous step, you created a new project on Platform.sh using the CLI. Now, there are a few configuration steps left that will help Platform.sh know what to do with your application during builds and deployments. Consult a template alongside this guide As you go through this guide, example files will be provided that will give you a good impression of how to configure applications on Platform.sh in the programming language they use. However, since they are simple examples and your own application may require more detailed configuration than those examples address, it is recommended that you take a look at our maintained templates for additional guidance. Select a language and choose one or more templates that most closely resemble your application and keep the template in another tab as you continue through this guide. Using these two resources together is the fastest way to correctly configure your project for Platform.sh. C#/.Net Core C#/.Net Core templates Platform.sh offers project templates for a number of C#/.Net Core applications. They can be used as a reference for importing your web application. Available templates: ASP.NET Core Go Go templates Platform.sh offers project templates for a number of Go applications. They can be used as a reference for importing your web application. Available templates: Basic Go Beego Echo Gin Hugo Mattermost Java Java templates Platform.sh offers project templates for a number of Java applications. They can be used as a reference for importing your web application. Available templates: Apache Tomcat Apache TomEE Helidon Jenkins Jetty KumuluzEE Micronaut Open Liberty Payara Micro Quarkus Spring Boot, Gradle, Mysql Spring Boot, Maven, Mysql Spring MVC, Maven, MongoDB Spring, Kotlin, Maven Thorntail xwiki Lisp Lisp templates Platform.sh offers project templates for a number of Lisp applications. They can be used as a reference for importing your web application. Available templates: Lisp Hunchentoot Node.js Node.js templates Platform.sh offers project templates for a number of Node.js applications. They can be used as a reference for importing your web application. Available templates: Express Gatsby Gatsby with Wordpress Koa Node.js Probot strapi PHP PHP templates Platform.sh offers project templates for a number of PHP applications. They can be used as a reference for importing your web application. Available templates: Backdrop Basic PHP Drupal 8 Drupal 8 Multisite Drupal 9 GovCMS 8 Laravel Magento 2 Community Edition Mautic Nextcloud Opigno Pimcore Sculpin Symfony 3 Symfony 4 Symfony 5 TYPO3 Wordpress Python Python templates Platform.sh offers project templates for a number of Python applications. They can be used as a reference for importing your web application. Available templates: Basic Python 2 Basic Python 3 Django 1 Django 2 Django 3 Flask MoinMoin Pelican Pyramid Python 3 running UWSGI Wagtail Ruby Ruby templates Platform.sh offers project templates for a number of Ruby applications. They can be used as a reference for importing your web application. Available templates: Ruby on Rails Create empty configuration files You will notice that each of the templates above contain the following structure around their application code: . ├── .platform │ ├── routes.yaml │ └── services.yaml ├── .platform.app.yaml └── \u0026lt; application code \u0026gt; In order to successfully deploy to Platform.sh you must add three YAML files: A .platform/routes.yaml file, which configures the routes used in your environments. That is, it describes how an incoming HTTP request is going to be processed by Platform.sh. A .platform/services.yaml file, which configures the services that will be used by the application. Connecting to Platform.sh’s maintained services only requires properly writing this file. While this file must be present, if your application does not require services it can remain empty. At least one .plaform.app.yaml file, which configures the application itself. It provides control over the way the application will be built and deployed on Platform.sh. When you set Platform.sh as a remote for your repository in the previous step, the CLI automatically created the hidden configuration directory .platform for you. The next steps will explore in more detail what each configuration files must include, but for now create empty files in their place. touch .platform/routes.yaml touch .platform/services.yaml touch .platform.app.yaml (Optional) Follow the Project Setup Wizard instructions in the management console All of the steps in this guide are also available in the Project Setup Wizard in your management console. Once you have created your project, the Wizard will appear at the top of your project page with detailed steps to help you properly configure your applications on Platform.sh. With the empty configuration files in place, you will need to specify your service configuration in .platform/services.yaml. Back I\u0026#39;ve created empty configuration files",
        "section": "Getting started",
        "subsections": "",
        "image": "",
        "url": "/gettingstarted/gettingstarted/own-code/project-configuration.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "0052c864679be1866aee2996f38b5e79",
        "title": "Data deletion",
        "description": "",
        "text": " Data deletion is handled via our backend providers. When a volume is released back to the provider, the provider will perform a wipe on the data utilizing either NIST 800-88 or DoD 5220.22-M depending upon the offering. This wipe is done immediately before reuse. All projects, except those hosted on Orange Cloud for Business, utilize encrypted volumes. The encryption key is destroyed when we release the volume back to the provider, adding another layer of protection. Media destruction Media destruction is handled via our backend providers. When the provider decommissions media it undergoes destruction as outlined in NIST 800-88. Data subject removal Data subject deletion requests where Platform is the controller are handled via a support ticket . For contracts designating Platform as the processor, deletion requests should be sent to the controller and we will forward any that we receive. Our product is a Platform as a Service. Platform does not directly edit customer data to ensure data confidentiality, security, and integrity. All data deletion requests for customer data must be handled by the concerned data controller. Resources AWS Security Whitepaper Azure Data Retention Google Cloud Platform Compliance Information Interoute Compliance Information Orange Cloud for Business certifications",
        "section": "Security and compliance",
        "subsections": " Media destruction Data subject removal Resources  ",
        "image": "",
        "url": "/security/data-deletion.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "b00b374267ecfbd49485fc6252c71b60",
        "title": "Delete a project",
        "description": "",
        "text": " To delete a Platform.sh project, including all data, code, and active environments: Go to your Account Settings, logging in if necessary, via the link in the top-right corner of these docs, or via this link: https://accounts.platform.sh/user . Locate the project you wish to delete in the project list. Hover the gear icon on the project and select “Delete”. Confirm your intent to delete the project. You will only be billed for the portion of a month during which the project was active. If you delete a project part way through the month the cost of the project will be prorated accordingly. A user account with no projects associated with it will have no charges.",
        "section": "Management console",
        "subsections": "",
        "image": "",
        "url": "/administration/web/delete.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "6d4ddc88e7d09f890c91f29939c408a4",
        "title": "Differences from Platform.sh on the Grid",
        "description": "",
        "text": " When using Platform.sh Dedicated, a few configuration options and tools function differently from Platform.sh on the Grid, aka the Development Environment. PHP Platform.sh Dedicated comes with pdo, apcu, curl, gd, imagick, ldap, mcrypt, mysqli, redis, soap, and opcache extensions enabled by default. In addition, we can enable enchant, gearman, geoip, gmp, http, pgsql, pinba, pspell, recode, tidy, xdebug, oci8 (PHP5.6 only), or any extension with a pre-existing package in the Debian Apt repository if desired. Please request such extensions via a ticket. Custom php.ini files are not supported on Platform.sh Dedicated. However, all PHP options that can be changed at runtime are still available. For example the memory limit can be changed using ini_set(\u0026#39;memory_limit\u0026#39;,\u0026#39;1024M\u0026#39;); PHP options that can we can change via support ticket include: max_execution_time max_input_time max_input_vars memory_limit post_max_size request_order upload_max_filesize Xdebug Platform.sh runs a second PHP-FPM process on all Dedicated clusters that has Xdebug enabled, but is only used if a request includes the appropriate Xdebug header. That means it’s safe to have Xdebug “always on”, as it will be ignored on most requests. To obtain the key you will need to file a ticket to have our support team provide it for you. Staging and Production have separate keys. Set that key in the Xdebug helper for your browser, and then whenever you have Xdebug enabled the request will use the alternate development PHP-FPM process with Xdebug. Cron tasks may be interrupted by deploys On Platform.sh Grid projects, a running cron task will block a deployment until it is complete. On Platform.sh Dedicated, however, a deploy will terminate a running cron task. Specifically, when a deploy to either Production or Staging begins, any active cron tasks are sent a SIGTERM message so that they can terminate gracefully if needed. If they are still running 2 seconds later a SIGKILL message will be sent to forcibly terminate the process. For that reason, it’s best to ensure your cron tasks can receive a SIGTERM message and terminate gracefully. Configuration \u0026amp; change management Some configuration parameters for Dedicated clusters cannot be managed via the YAML configuration files, and for those parameters you will need to open a support ticket to have the change applied. Further, the .platform/routes.yaml and .platform/services.yaml files do not automatically apply. Those will apply on the development environments but not on the staging and production instances. Any existing service upgrades or new service additions to staging and production will require a support ticket. It is possible to run different configurations for some (but not all) options between staging and production, such as cron tasks. By default we will make configuration changes to both instances unless you request otherwise. Specifically: Cron commands Worker instances Service versions and configuration (everything in .platform/services.yaml) Route, domain, and redirect configuration (everything in .platform/routes.yaml) Application container version Additional PHP extensions Web server configuration (the web.locations section of .platform.app.yaml) Cron Cron tasks may run up to once per minute. (They are limited to once every 5 minutes on Platform.sh Grid.) Cron tasks are always interpreted in UTC time. Logs Logs are available on the Dedicated Cluster at a different path than on Platform.sh Grid. Specifically, the can be found in: /var/log/platform/\u0026lt;application-name\u0026gt;/ This folder contains the application, cron, error and deployment logs.",
        "section": "Platform.sh Dedicated",
        "subsections": " PHP  Xdebug   Cron tasks may be interrupted by deploys Configuration \u0026amp; change management  Cron   Logs  ",
        "image": "",
        "url": "/dedicated/overview/grid.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "7d3060421a1a4550c36ef08e86fdb1b3",
        "title": "Example",
        "description": "",
        "text": " In this short section we will give you a very simple, typical example. More involved use-cases (such as site with many domains or multiple applications are simply variations on this). Suppose your project ID is abc123 in the US region, and you’ve registered mysite.com. You want www.mysite.com to be the “real” site and mysite.com to redirect to it. Configure routes.yaml First, configure your routes.yaml file like so:  https://www.{default}/ :type:upstreamupstream: app:http  https://{default}/ :type:redirectto: https://www.{default}/ That will result in two domains being created on Platform.sh: master-def456-abc123.eu-2.platformsh.site and www---master-def456-abc123.eu-2.platformsh.site. The former will automatically redirect to the latter. In the routes.yaml file, {default} will automatically be replaced with master-def456-abc123.eu-2.platformsh.site. In domain prefixes (like www), the . will be replaced with ---. Set your domain Now, add a single domain to your Platform.sh project for mysite.com. Using the CLI type: platform domain:add mysite.com You can also use the management console for that. As soon as you do, Platform.sh will no longer serve master-def456-abc123.eu-2.platformsh.site at all. Instead, {default} in routes.yaml will be replaced with mysite.com anywhere it appears when generating routes to respond to. You can still access the original internal domain by running platform environment:info edge_hostname -e master. Configure your DNS provider On your DNS provider, you would create two CNAMEs: mysite.com should be an ALIAS/CNAME/ANAME to master-def456-abc123.eu-2.platformsh.site. www.mysite.com should be a CNAME to master-def456-abc123.eu-2.platformsh.site. Note: Both point to the same name. See the note above regarding how different registrars handle dynamic apex domains. Result Here’s what will now happen under the hood. Assume for a moment that all caches everywhere are empty. An incoming request for mysite.com will result in the following: Your browser asks the DNS network for mysite.com\u0026#39;s DNS A record (the IP address of this host). It responds with “it’s an alias for www.master-def456-abc123.eu-2.platformsh.site” (the CNAME) which itself resolves to the A record with IP address 1.2.3.4 (Or whatever the actual address is). By default DNS requests by browsers are recursive, so there is no performance penalty for using CNAMEs. Your browser sends a request to 1.2.3.4 for domain mysite.com. Your router responds with an HTTP 301 redirect to www.mysite.com (because that’s what routes.yaml specified). Your browser looks up www.mysite.com and, as above, gets an alias for www.master-def456-abc123.eu-2.platformsh.site, which is IP 1.2.3.4. Your browser sends a request to 1.2.3.4 for domain www.mysite.com. Your router passes the request through to your application which in turn responds with whatever it’s supposed to do. On subsequent requests, your browser will know to simply connect to 1.2.3.4 for domain www.mysite.com and skip the rest. The entire process takes only a few milliseconds.",
        "section": "Going live",
        "subsections": " Configure routes.yaml Set your domain Configure your DNS provider Result  ",
        "image": "",
        "url": "/golive/example.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "bdb721610052260e3a58e1a0bbb7aa27",
        "title": "Install the CLI",
        "description": "",
        "text": " With all of the requirements met, install the CLI to start developing with Platform.sh. Install the CLI In your terminal run the following command: Installing on OSX or Linux curl -sS https://platform.sh/cli/installer | php Installing on Windows curl https://platform.sh/cli/installer -o cli-installer.php php cli-installer.php Authenticate and Verify Once the installation has completed, you can run the CLI in your terminal with the command platform Note: If you opened your free trial account using another login (i.e. GitHub), you will not be able to authenticate with this command until you setup your account password with Platform.sh in the console. You should now be able to see a list of your Platform.sh projects, including the template you made in this guide. You can copy its project ID hash, and then download a local copy of the repository with the command platform get \u0026lt;project ID\u0026gt; With a local copy, you can create branches, commit to them, and push your changes to Platform.sh right away! git push platform master Take a minute to explore some of the commands available with the CLI by using the command platform list. That’s it! Now that you have the management console set up and the CLI installed on your computer, you’re well on your way to exploring all of the ways that Platform.sh can improve your development workflow. Back I\u0026#39;ve installed the CLI",
        "section": "Getting started",
        "subsections": "",
        "image": "",
        "url": "/gettingstarted/gettingstarted/template/cli-install.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "cbf5ecf034e398d561aa46492ecb961d",
        "title": "Jakarta EE/Eclipse MicroProfile",
        "description": "",
        "text": " Eclipse MicroProfile is a community dedicated to optimizing the Enterprise Java mission for microservice-based architectures. The goal is to define a microservices application platform that is portable across multiple runtimes. Currently, the leading players in this group are IBM, Red Hat, Tomitribe, Payara, the London Java Community (LJC), and SouJava. Java Enterprise Edition (Java EE) is an umbrella that holds specifications and APIs with enterprise features, like distributed computing and web services. Widely used in Java, Java EE runs on reference runtimes that can be anything from microservices to application servers that handle transactions, security, scalability, concurrency, and management for the components it’s deploying. Now, Enterprise Java has been standardized under the Eclipse Foundation with the name Jakarta EE . Services The configuration reader library for Java is used in these examples, so be sure to check out the documentation for installation instructions and the latest version. MongoDB You can use Jakarta NoSQL / JNoSQL to use MongoDB with your application by first determining the MongoDB client programmatically. import com.mongodb.MongoClient; import jakarta.nosql.document.DocumentCollectionManager; import jakarta.nosql.document.DocumentCollectionManagerFactory; import org.jnosql.diana.mongodb.document.MongoDBDocumentConfiguration; import sh.platform.config.Config; import sh.platform.config.MongoDB; import javax.annotation.PostConstruct; import javax.enterprise.context.ApplicationScoped; import javax.enterprise.inject.Disposes; import javax.enterprise.inject.Produces; @ApplicationScoped class DocumentManagerProducer { private DocumentCollectionManagerFactory managerFactory; private MongoDB mongoDB; @PostConstruct public void init() { Config config = new Config(); this.mongoDB = config.getCredential( database , MongoDB::new); final MongoClient mongoClient = mongoDB.get(); MongoDBDocumentConfiguration configuration = new MongoDBDocumentConfiguration(); this.managerFactory = configuration.get(mongoClient); } @Produces public DocumentCollectionManager getManager() { return managerFactory.get(mongoDB.getDatabase()); } public void destroy(@Disposes DocumentCollectionManager manager) { this.manager.close(); } } Apache Solr You can use Jakarta NoSQL / JNoSQL to use Solr with your application by first determining the Solr client programmatically. import jakarta.nosql.document.DocumentCollectionManager; import jakarta.nosql.document.DocumentCollectionManagerFactory; import org.apache.solr.client.solrj.impl.HttpSolrClient; import org.jnosql.diana.solr.document.SolrDocumentConfiguration; import sh.platform.config.Config; import sh.platform.config.Solr; import javax.annotation.PostConstruct; import javax.enterprise.context.ApplicationScoped; import javax.enterprise.inject.Disposes; import javax.enterprise.inject.Produces; @ApplicationScoped class DocumentManagerProducer { private DocumentCollectionManagerFactory managerFactory; @PostConstruct public void init() { Config config = new Config(); Solr solr = config.getCredential( database , Solr::new); final HttpSolrClient httpSolrClient = solr.get(); SolrDocumentConfiguration configuration = new SolrDocumentConfiguration(); this.managerFactory = configuration.get(httpSolrClient); } @Produces public DocumentCollectionManager getManager() { return managerFactory.get( collection ); } public void destroy(@Disposes DocumentCollectionManager manager) { this.manager.close(); } } Elasticsearch You can use Jakarta NoSQL / JNoSQL to use Elasticsearch with your application by first determining the Elasticsearch client programmatically. import jakarta.nosql.document.DocumentCollectionManager; import jakarta.nosql.document.DocumentCollectionManagerFactory; import org.elasticsearch.client.RestHighLevelClient; import org.jnosql.diana.elasticsearch.document.ElasticsearchDocumentConfiguration; import sh.platform.config.Config; import sh.platform.config.Elasticsearch; import javax.annotation.PostConstruct; import javax.enterprise.context.ApplicationScoped; import javax.enterprise.inject.Disposes; import javax.enterprise.inject.Produces; @ApplicationScoped class DocumentManagerProducer { private DocumentCollectionManagerFactory managerFactory; @PostConstruct public void init() { Config config = new Config(); Elasticsearch elasticsearch = config.getCredential( database , Elasticsearch::new); final RestHighLevelClient client = elasticsearch.get(); ElasticsearchDocumentConfiguration configuration = new ElasticsearchDocumentConfiguration(); this.managerFactory = configuration.get(client); } @Produces public DocumentCollectionManager getManager() { return managerFactory.get( collection ); } public void destroy(@Disposes DocumentCollectionManager manager) { this.manager.close(); } } Redis You can use Jakarta NoSQL / JNoSQL to use Redis with your application by first determining the Redis client programmatically. import jakarta.nosql.keyvalue.BucketManager; import org.jnosql.diana.redis.keyvalue.RedisBucketManagerFactory; import org.jnosql.diana.redis.keyvalue.RedisConfiguration; import redis.clients.jedis.JedisPool; import sh.platform.config.Config; import sh.platform.config.Redis; import javax.annotation.PostConstruct; import javax.enterprise.context.ApplicationScoped; import javax.enterprise.inject.Disposes; import javax.enterprise.inject.Produces; @ApplicationScoped class BucketManagerProducer { private static final String BUCKET =  olympus ; private RedisBucketManagerFactory managerFactory; @PostConstruct public void init() { Config config = new Config(); Redis redis = config.getCredential( redis , Redis::new); final JedisPool jedisPool = redis.get(); RedisConfiguration configuration = new RedisConfiguration(); managerFactory = configuration.get(jedisPool); } @Produces public BucketManager getManager() { return managerFactory.getBucketManager(BUCKET); } public void destroy(@Disposes BucketManager manager) { manager.close(); } } MySQL MySQL is an open-source relational database technology, and Jakarta EE supports a robust integration with it: JPA . The first step is to choose the database that you would like to use in your project. Define the driver for MySQL and the Java dependencies. Then determine the DataSource client programmatically: import sh.platform.config.Config; import sh.platform.config.JPA; import javax.annotation.PostConstruct; import javax.enterprise.context.ApplicationScoped; import javax.enterprise.inject.Disposes; import javax.enterprise.inject.Produces; import javax.persistence.EntityManager; import javax.persistence.EntityManagerFactory; @ApplicationScoped class EntityManagerConfiguration { private EntityManagerFactory entityManagerFactory; private EntityManager entityManager; @PostConstruct void setUp() { Config config = new Config(); final JPA credential = config.getCredential( postgresql , JPA::new); entityManagerFactory = credential.getMySQL( jpa-example ); this.entityManager = entityManagerFactory.createEntityManager(); } @Produces @ApplicationScoped EntityManagerFactory getEntityManagerFactory() { return entityManagerFactory; } @Produces @ApplicationScoped EntityManager getEntityManager() { return entityManager; } void close(@Disposes EntityManagerFactory entityManagerFactory) { entityManagerFactory.close(); } void close(@Disposes EntityManager entityManager) { entityManager.close(); } } Note: You can use the same MySQL driver for MariaDB as well if you wish to do so. MariaDB MariaDB is an open-source relational database technology, and Jakarta EE supports a robust integration with it: JPA . The first step is to choose the database that you would like to use in your project. Define the driver for MariaDB and the Java dependencies. Then determine the DataSource client programmatically: import sh.platform.config.Config; import sh.platform.config.JPA; import javax.annotation.PostConstruct; import javax.enterprise.context.ApplicationScoped; import javax.enterprise.inject.Disposes; import javax.enterprise.inject.Produces; import javax.persistence.EntityManager; import javax.persistence.EntityManagerFactory; @ApplicationScoped class EntityManagerConfiguration { private EntityManagerFactory entityManagerFactory; private EntityManager entityManager; @PostConstruct void setUp() { Config config = new Config(); final JPA credential = config.getCredential( postgresql , JPA::new); entityManagerFactory = credential.getMariaDB( jpa-example ); this.entityManager = entityManagerFactory.createEntityManager(); } @Produces @ApplicationScoped EntityManagerFactory getEntityManagerFactory() { return entityManagerFactory; } @Produces @ApplicationScoped EntityManager getEntityManager() { return entityManager; } void close(@Disposes EntityManagerFactory entityManagerFactory) { entityManagerFactory.close(); } void close(@Disposes EntityManager entityManager) { entityManager.close(); } } PostgreSQL PostgreSQL is an open-source relational database technology, and Jakarta EE supports a robust integration with it: JPA . The first step is to choose the database that you would like to use in your project. Define the driver for PostgreSQL and the Java dependencies. Then determine the DataSource client programmatically: import sh.platform.config.Config; import sh.platform.config.JPA; import javax.annotation.PostConstruct; import javax.enterprise.context.ApplicationScoped; import javax.enterprise.inject.Disposes; import javax.enterprise.inject.Produces; import javax.persistence.EntityManager; import javax.persistence.EntityManagerFactory; @ApplicationScoped class EntityManagerConfiguration { private EntityManagerFactory entityManagerFactory; private EntityManager entityManager; @PostConstruct void setUp() { Config config = new Config(); final JPA credential = config.getCredential( postgresql , JPA::new); entityManagerFactory = credential.getPostgreSQL( jpa-example ); entityManager = entityManagerFactory.createEntityManager(); } @Produces @ApplicationScoped EntityManagerFactory getEntityManagerFactory() { return entityManagerFactory; } @Produces @ApplicationScoped EntityManager getEntityManager() { return entityManager; } void close(@Disposes EntityManagerFactory entityManagerFactory) { entityManagerFactory.close(); } void close(@Disposes EntityManager entityManager) { entityManager.close(); } } Transaction To any Eclipse Microprofile or any non-JTA application is essential to point out, CDI does not provide transaction management implementation as part of its specs. Transaction management is left to be implemented by the programmer through the interceptors, such as the code below. import javax.annotation.Priority; import javax.inject.Inject; import javax.interceptor.AroundInvoke; import javax.interceptor.Interceptor; import javax.interceptor.InvocationContext; import javax.persistence.EntityManager; import javax.persistence.EntityTransaction; import javax.transaction.Transactional; @Transactional @Interceptor @Priority(Interceptor.Priority.APPLICATION) public class TransactionInterceptor { @Inject private EntityManager manager; @AroundInvoke public Object manageTransaction(InvocationContext context) throws Exception { final EntityTransaction transaction = manager.getTransaction(); transaction.begin(); try { Object result = context.proceed(); transaction.commit(); return result; } catch (Exception exp) { transaction.rollback(); throw exp; } } } Furthermore, Apache Delta Spike has a post for treating this problem. Templates Apache Tomee Thorntail Payara Micro KumuluzEE Helidon Open Liberty Quarkus Tomcat",
        "section": "Featured frameworks",
        "subsections": " Services  MongoDB Apache Solr Elasticsearch Redis MySQL MariaDB PostgreSQL   Transaction Templates  ",
        "image": "",
        "url": "/frameworks/jakarta.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "7243925f0793ac942dcc633580b13c07",
        "title": "Memcached (Object cache)",
        "description": "",
        "text": " Memcached is a simple in-memory object store well-suited for application level caching. See the Memcached for more information. Both Memcached and Redis can be used for application caching. As a general rule, Memcached is simpler and thus more widely supported while Redis is more robust. Platform.sh recommends using Redis if possible but Memcached is fully supported if an application favors that cache service.” Supported versions Grid Dedicated 1.4 1.5 1.6 None available Relationship The format exposed in the $PLATFORM_RELATIONSHIPS environment variable : {  cluster :  rjify4yjcwxaa-master-7rqtwti ,  host :  memcached.internal ,  hostname :  2fw7ykay5vo5cez7cgwccz5kqa.memcached.service._.eu-3.platformsh.site ,  ip :  169.254.235.192 ,  port : 11211,  rel :  memcached ,  scheme :  memcached ,  service :  memcached ,  type :  memcached:1.4  } Usage example In your .platform/services.yaml: cachemc:type:memcached:1.6 Now add a relationship in your .platform.app.yaml file: relationships:memcachedcache: cachemc:memcached  Note: You will need to use the memcached type when defining the service # .platform/services.yamlservice_name:type:memcached:version and the endpoint memcached when defining the relationship # .platform.app.yamlrelationships:relationship_name:“service_name:memcached” Your service_name and relationship_name are defined by you, but we recommend making them distinct from each other. If you are using PHP, configure the relationship and enable the PHP memcached extension in your .platform.app.yaml. (Note that the memcached extension requires igbinary and msgpack as well, but those will be enabled automatically.) runtime:extensions:- memcachedFor Python you will need to include a dependency for a Memcached library, either via your requirements.txt file or a global dependency. As a global dependency you would add the following to .platform.app.yaml: dependencies:python:python-memcached:\u0026#39;*\u0026#39;You can then use the service in a configuration file of your application with something like: Go Java Node.js PHP PHP package examples import (  fmt   github.com/bradfitz/gomemcache/memcache  psh  github.com/platformsh/config-reader-go/v2  gomemcache  github.com/platformsh/config-reader-go/v2/gomemcache  ) func UsageExampleMemcached() string { // Create a NewRuntimeConfig object to ease reading the Platform.sh environment variables. // You can alternatively use os.Getenv() yourself. config, err := psh.NewRuntimeConfig() checkErr(err) // Get the credentials to connect to the Solr service. credentials, err := config.Credentials( memcached ) checkErr(err) // Retrieve formatted credentials for gomemcache. formatted, err := gomemcache.FormattedCredentials(credentials) checkErr(err) // Connect to Memcached. mc := memcache.New(formatted) // Set a value. key :=  Deploy_day  value :=  Friday  err = mc.Set(\u0026amp;memcache.Item{Key: key, Value: []byte(value)}) // Read it back. test, err := mc.Get(key) return fmt.Sprintf( Found value \u0026lt;strong\u0026gt;%s\u0026lt;/strong\u0026gt; for key \u0026lt;strong\u0026gt;%s\u0026lt;/strong\u0026gt;. , test.Value, key) } package sh.platform.languages.sample; import net.spy.memcached.MemcachedClient; import sh.platform.config.Config; import java.util.function.Supplier; import sh.platform.config.Memcached; public class MemcachedSample implements Supplier\u0026lt;String\u0026gt; { @Override public String get() { StringBuilder logger = new StringBuilder(); // Create a new config object to ease reading the Platform.sh environment variables. // You can alternatively use getenv() yourself. Config config = new Config(); // Get the credentials to connect to the Memcached service. Memcached memcached = config.getCredential( memcached , Memcached::new); final MemcachedClient client = memcached.get(); String key =  cloud ; String value =  platformsh ; // Set a value. client.set(key, 0, value); // Read it back. Object test = client.get(key); logger.append(String.format( Found value %s for key %s. , test, key)); return logger.toString(); } } const Memcached = require(\u0026#39;memcached\u0026#39;); const config = require( platformsh-config ).config(); const { promisify } = require(\u0026#39;util\u0026#39;); exports.usageExample = async function() { const credentials = config.credentials(\u0026#39;memcached\u0026#39;); let client = new Memcached(`${credentials.host}:${credentials.port}`); // The MemcacheD client is not Promise-aware, so make it so. const memcachedGet = promisify(client.get).bind(client); const memcachedSet = promisify(client.set).bind(client); let key = \u0026#39;Deploy-day\u0026#39;; let value = \u0026#39;Friday\u0026#39;; // Set a value. await memcachedSet(key, value, 10); // Read it back. let test = await memcachedGet(key); let output = `Found value \u0026lt;strong\u0026gt;${test}\u0026lt;/strong\u0026gt; for key \u0026lt;strong\u0026gt;${key}\u0026lt;/strong\u0026gt;.`; return output; }; \u0026lt;?php declare(strict_types=1); use // Create a new config object to ease reading the Platform.sh environment variables. // You can alternatively use getenv() yourself. $config = new Config(); // Get the credentials to connect to the Memcached service. $credentials = $config-\u0026gt;credentials(\u0026#39;memcached\u0026#39;); try { // Connecting to Memcached server. $memcached = new Memcached(); $memcached-\u0026gt;addServer($credentials[\u0026#39;host\u0026#39;], $credentials[\u0026#39;port\u0026#39;]); $memcached-\u0026gt;setOption(Memcached::OPT_BINARY_PROTOCOL, true); $key =  Deploy day ; $value =  Friday ; // Set a value. $memcached-\u0026gt;set($key, $value); // Read it back. $test = $memcached-\u0026gt;get($key); printf(\u0026#39;Found value \u0026lt;strong\u0026gt;%s\u0026lt;/strong\u0026gt; for key \u0026lt;strong\u0026gt;%s\u0026lt;/strong\u0026gt;.\u0026#39;, $test, $key); } catch (Exception $e) { print $e-\u0026gt;getMessage(); } import pymemcache from platformshconfig import Config def usage_example(): # Create a new Config object to ease reading the Platform.sh environment variables. # You can alternatively use os.environ yourself. config = Config() # Get the credentials to connect to the Memcached service. credentials = config.credentials(\u0026#39;memcached\u0026#39;) try: # Try connecting to Memached server. memcached = pymemcache.Client((credentials[\u0026#39;host\u0026#39;], credentials[\u0026#39;port\u0026#39;])) memcached.set(\u0026#39;Memcached::OPT_BINARY_PROTOCOL\u0026#39;, True) key =  Deploy_day  value =  Friday  # Set a value. memcached.set(key, value) # Read it back. test = memcached.get(key) return \u0026#39;Found value \u0026lt;strong\u0026gt;{0}\u0026lt;/strong\u0026gt; for key \u0026lt;strong\u0026gt;{1}\u0026lt;/strong\u0026gt;.\u0026#39;.format(test.decode( utf-8 ), key) except Exception as e: return e Accessing Memcached directly To access the Memcached service directly you can simply use netcat as Memcached does not have a dedicated client tool. Assuming your Memcached relationship is named cache, the host name and port number obtained from PLATFORM_RELATIONSHIPS would be cache.internal and 11211. Open an SSH session and access the Memcached server as follows: netcat cache.internal 11211",
        "section": "Configure services",
        "subsections": " Supported versions Relationship Usage example Accessing Memcached directly  ",
        "image": "",
        "url": "/configuration/services/memcached.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "99d0f5a7f3e218cf2b284512684b4ede",
        "title": "Next steps",
        "description": "",
        "text": " In this guide you created a project using the management console and installed the Platform.sh CLI . Now you can explore some of the next steps for working with Platform.sh. Import your own code Templates are great, but configuring your own application to run on Platform.sh is the goal. Import your own code Use the CLI and a few configuration files to deploy your code on Platform.sh. Developing on Platform.sh Once an application has been migrated to Platform.sh, there’s plenty more features that will help improve your development life cycle. Local development Remotely connect to services and build your application locally during development. Development environments Activate development branches and test new features before merging into production. Additional Resources External Integrations Configure Platform.sh to mirror every push and pull request on GitHub, Gitlab, and Bitbucket. Going Live Set up your site for production, configure domains, and go live! Platform.sh Community Portal Check out how-tos, tutorials, and get help for your questions about Platform.sh. Platform.sh Blog Read news and how-to posts all about working with Platform.sh. Back",
        "section": "Getting started",
        "subsections": " Import your own code Developing on Platform.sh Additional Resources  ",
        "image": "",
        "url": "/gettingstarted/gettingstarted/template/next-steps.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "eccbc17c368c7aecee8ed162a98e45a8",
        "title": "Options",
        "description": "",
        "text": " Staging environments By default, the staging instance and production instance run on the same trio of virtual machines. That ensures identical configuration between them but can incur a performance penalty for production if the load generated during QA and UAT in staging is of any appreciable size. A dedicated single-node staging machine can be provisioned for your application with an identical software configuration to your production hardware, but reduced hardware specs. This gives the advantages of isolating the staging load from the production hardware as well as having an identical software configuration to perform UAT, but this option does not provide a bed for performance testing as the physical hardware configuration is not the same as production. Multiple applications Each application deployed to the Dedicated Cluster corresponds to a single Git repository in the Development Environment. Multiple .platform.app.yaml files are not supported. While it is possible to host multiple application code bases in separate subdirectories/subpaths of the application (such as /drupal, /api, /symfony, etc.) controlled by a single .platform.app.yaml, it is not recommended and requires additional configuration. One or more domains may be mapped to the application. Our experience has shown that hosting multiple applications on a common resource pool is often bad for all applications on the cluster. We therefore limit the number of applications that may be hosted on a single Dedicated Cluster. On a D6 instance, only one application is supported. On D12 and larger Dedicated plans multiple applications are supported at an extra cost. Each application would correspond to a different Development Environment and Git repository and cannot share data or files with other applications. This configuration is discouraged. Multiple-AZ The default configuration for Platform.sh Dedicated clusters is to launch into a single Availability Zone (AZ). This is for a few reasons: Because the members of your cluster communicate with each other via TCP to perform DB replication, cache lookup, and other associated tasks, the latency between data centers/AZs can become a significant performance liability. Having your entire cluster within one AZ ensures that the latency between cluster members is minimal, having a direct effect on perceived end-user performance. Network traffic between AZs is billed, whereas intra-AZ traffic is not. That leads to higher costs for this decreased performance. Some clients prefer the peace of mind of hosting across multiple AZs, but it should be noted that multiple-AZ configurations do not improve the contractual 99.99% uptime SLA, nor does our standard, single-AZ configuration decrease the 99.99% uptime SLA. We are responsible for meeting the 99.99% uptime SLA no matter what, so multiple-AZ deployments should only be considered in cases where it is truly appropriate. Multi-AZ deployments are available only on select AWS regions. Additional application servers For especially high-traffic sites we can also add additional application-only servers. These servers will contain just the application code; data storage services (SQL, Solr, Redis, etc.) are limited to the standard three. The cluster begins to look more like a standard N-Tier architecture at this point, with a horizontal line of web application servers in front of a 3 node (N\u0026#43;1) cluster of Galera database servers. Speak to your sales representative about the costs associated with adding additional application servers. This configuration requires a separate setup from the default so advanced planning is required. SFTP accounts In addition to SSH accounts, SFTP accounts can be created with a custom user/password that are restricted to certain directories. These directories must be one of the writeable mounts (or rather, there’s no point assigning them to the read-only code directory). There is no cost for this configuration, and it can be requested at any time via a support ticket. SSH public key based authentication is also supported on the SFTP account. Error handling On Platform.sh Professional, incoming requests are held at the edge router temporarily during a deploy. That allows a site to simply “respond slowly” rather than be offline during a deploy, provided the deploy time is short (a few seconds). On Platform.sh Dedicated, incoming requests are not held during deploy and receive a 503 error. As the Dedicated Cluster is almost always fronted by a CDN, the CDN will continue to serve cached pages during the few seconds of deploy, so for the vast majority of users there is no downtime or even slowdown. If a request does pass the CDN during a deploy that is not counted as downtime covered by our Service Level Agreement. By default, Platform.sh will serve generic Platform.sh-branded error pages for errors generated before a request reaches the application. (500 errors, some 400 errors, etc.) Alternatively you may provide a static error page for each desired error code via a ticket for us to configure with the CDN. This file may be any static HTML file but is limited to 64 KB in size. IP restrictions Platform.sh supports project-level IP restrictions (whitelisting) and HTTP Basic authentication. These may be configured through the Development Environment and will be automatically replicated from the production and staging branches to the production and staging environments, respectively. Note: Changing access control will trigger a new deploy of the current environment. However, the changes will not propagate to child environments until they are manually redeployed. Remote logging Platform.sh Dedicated supports sending logs to a remote logging service such as Loggly, Papertrail, or Logz.io using the rsyslog service. This is an optional feature and you can request that it be enabled via a support ticket. Once enabled and configured your application can direct log output to the system syslog facility and it will be replicated to the remote service you have configured. When contacting support to enable rsyslog, you will need: The name of the remote logging service you will be using. The message template format used by your logging service. The specific log files you want forwarded to your logging service. There is no cost for this functionality.",
        "section": "Platform.sh Dedicated cluster specifications",
        "subsections": " Staging environments Multiple applications Multiple-AZ Additional application servers SFTP accounts Error handling IP restrictions Remote logging  ",
        "image": "",
        "url": "/dedicated/architecture/options.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "0f5b183b69473bc449924aeb12dd1b19",
        "title": "PHP-FPM sizing",
        "description": "",
        "text": " Platform.sh uses a heuristic to automatically set the number of workers of the PHP-FPM runtime based on the memory available in the container. This heuristic is based on assumptions about the memory necessary on average to process a request. You can tweak those assumptions if your application will typically use considerably more or less memory. Note that this value is independent of the memory_limit set in php.ini, which is the maximum amount of memory a single PHP process can use before it is automatically terminated. These estimates are used only for determining the number of PHP-FPM workers to start. The heuristic The heuristic is based on three input parameters: The memory available for the container, which depends on the size of the container (S, M, L), The memory that an average request is expected to require, The memory that should be reserved for things that are not specific to a request (memory for nginx, the op-code cache, some OS page cache, etc.) The number of workers is calculated as: Defaults The default assumptions are: 45 MB for the average per-request memory 70 MB for the reserved memory These are deliberately conservative values that should allow most programs to run without modification. You can change them by using the runtime.sizing_hints.reserved_memory and runtime.sizing_hints.request_memory in your .platform.app.yaml. For example, if your application consumes on average 110 MB of memory for a request use: runtime:sizing_hints:request_memory:110The request_memory has a lower limit of 10 MB while reserved_memory has a lower limit of 70 MB. Values lower than those will be replaced with those minimums. You can check the maximum number of PHP-FPM workers by opening an SSH session and running following command (example for PHP 7.x): grep -e \u0026#39;^pm.max_children\u0026#39; /etc/php/*/fpm/php-fpm.conf pm.max_children = 2 Measuring PHP worker memory usage To see how much memory your PHP worker processes are using, you can open an SSH session and look at the PHP access log: less /var/log/php.access.log In the fifth column, you’ll see the peak memory usage that occurred while each request was handled. The peak usage will probably vary between requests, but in order to avoid the severe performance costs that come from swapping, your size hint should be somewhere between the average and worst case memory usages that you observe. A good way to determine an optimal request memory is with the following command: tail -n5000 /var/log/php.access.log | awk \u0026#39;{print $6}\u0026#39; | sort -n | uniq -c This will print out a table of how many requests used how much memory, in KB, for the last 5000 requests that reached PHP-FPM. (On an especially busy site you may need to increase that number). As an example, consider the following output: 1 4800 2048 948 4096 785 6144 584 8192 889 10240 492 12288 196 14336 68 16384 2 18432 1 22528 6 131072 This indicates that the majority of requests (4800) used 2048 KB of memory. In this case that’s likely application caching at work. Most requests used up to around 10 MB of memory, while a few used as much as 18 MB and a very very few (6 requests) peaked at 131 MB. (In this example those are probably cache clears.) A conservative approach would suggest an average request memory of 16 MB should be sufficient. A more aggressive stance would suggest 10 MB. The more aggressive approach would potentially allow for more concurrent requests at the risk of some requests needing to use swap memory, thus slowing them down. The web agency Pixelant has also published a log analyzer tool for Platform.sh that offers a better visualization of access logs to determine how much memory requests are using on average. It also offers additional insights into the operation of your site that can suggest places to further optimize your configuration and provide guidance on when it’s time to increase your plan size. (Please note that this tool is maintained by a 3rd party, not by Platform.sh.) Note: If you are running on PHP 5.x then don’t bother adjusting the worker memory usage until you upgrade to PHP 7.x. PHP 7 is vastly more memory efficient than PHP 5 and you will likely need less than half as much memory per process under PHP 7.",
        "section": "PHP",
        "subsections": " The heuristic Defaults Measuring PHP worker memory usage  ",
        "image": "",
        "url": "/languages/php/fpm.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "14e33caadcebae385e008c0ec8905e49",
        "title": "Region migration",
        "description": "",
        "text": " Platform.sh is available in a number of different Regions. Each region is a self-contained copy of Platform.sh in a single datacenter. When you first create a project you can specify which region it should be in. Platform.sh does not offer an automated way to migrate a project from one region to another after it is created. However, the process to do so manually is fairly straightforward and scriptable. Why migrate between regions? Different datacenters are located in different geographic areas, and you may want to keep your site physically close to the bulk of your user base for reduced latency. Only selected regions offer European Sovereign hosting. If you created a project in a non-Sovereign region you may need to migrate to a Sovereign region. Some regions are running older versions of the Platform.sh orchestration system that offers fewer features. In particular, the US-1 and EU-1 regions do not offer XL and 2XL plans, self-terminating builds in case of a build process that runs too long, or distributing environments across different grid hosts. If you are on one of those regions and desire those features you will need to migrate to the newer US-2 or EU-2 regions. Scripted migration process Although not directly supported by Platform.sh, an agency named Contextual Code has built a bash migration script to automate most common configurations. If your site is a typical single application with a single SQL database, the script should take care of most of the process for you. (If you have additional backend systems you may need to do some additional work manually, as documented below.) 0. Preparation Plan a timeframe in which to handle the migration. You will want to avoid developing any new code during that period (as your Git repository will change) and be prepared for a brief site outage when you migrate. You are essentially relaunching the site, just with the same host as previously. Plan accordingly. Set your DNS Time-to-Live as low as possible. 1. Create a new project Create a new Platform.sh Project in the desired region. You can initially create it as a Development project and change the plan size immediately before switching over or go ahead and use the desired size from the beginning. When the system asks if you want to use an existing template or provide your own code, select provide your own code. However, you do not need to push any code to it yet. Note the new project’s ID from the URL. 2. Download and invoke the script Download the Platform Migration tool from Contextual Code. The README file explains the step it uses in more detail. With a typical site it will carry you through the full process of transfering code, data, configuration, and the domain name to your new project. Note: You will still need to update your DNS record with your registrar to point to the new project when you are ready to go live. Also be aware that the script does not transfer external integrations — such as health notifications or 3rd party Git provider integrations — so you will need to re-enable those manually. 3. Remove the old project Once the new project is running and the DNS has fully propagated you can delete the old project. Manual migration process 0. Preparation Plan a timeframe in which to handle the migration. You will want to avoid developing any new code during that period (as your Git repository will change) and be prepared for a brief site outage when you migrate. You are essentially relaunching the site, just with the same host as previously. Plan accordingly. Set your DNS Time-to-Live as low as possible. 1. Create and populate a new project Create a new Platform.sh project in the desired region. You can initially create it as a Development project and change the plan size immediately before switching over or go ahead and use the desired size from the beginning. When the system asks if you want to use an existing template or provide your own code, select provide your own code. Make a Git clone of your existing project. Then add a Git remote to the new project, using the Git URL shown in the management console. Push the code for at least your master branch to the new project. (You can also transfer other branches if desired. That’s optional.) Alternatively, if you are using a 3rd party Git repository (GitHub, BitBucket, GitLab, etc.), you can add an integration to the new project just as you did the old one. It will automatically mirror your 3rd party repository exactly the same way as the old project and you won’t need to update it manually. Copy your existing user files on the old project to your computer using rsync. See the exporting page for details. Then use rsync to copy them to the same directory on the new project. See the migrating page for details. Export your database from the old project and import it into the new project. Again, see the exporting and migration pages, as well as the instructions for your specific database services. Re-enter any project or environment variables you’ve defined on your old project in your new project. Add any users to your new project that you want to continue to have access. If you have any 3rd party integrations active, especially the Health Notification checks, add them to the new project. 2. Maintain the mirror Most sites have generated data in Solr, Elasticsearch, or similar that needs to be regenerated. Take whatever steps are needed to reindex such systems. That may simply be allowing cron to run for a while, or your system may have a command to reindex everything faster. That will vary by your application. You can also periodically re-sync your data. For rsync the process should be quite fast as long as you maintain your local copy of it, as rsync will transfer only content that has changed. For the database it may take longer depending on the size of your data. Depending on your site’s size and your schedule, you can have the old and new project overlapping for only an hour or two or several weeks. That’s up to you. Be sure to verify that the new site is working as desired before continuing. 3. Launch the new site Once your new project is on the right production plan size you can cut over to it. Add your domain name(s) to your new project. If you have a custom SSL certificate you will need to add that at the same time. (Because the projects are in separate regions it’s safe to add the domain name to both at the same time, which reduces apparent downtime.) If possible, put your site into read-only mode or maintenance mode. Then do one final data sync (code and database) to ensure the new project starts with all fo the data from the old one. Once the domain is set, update your DNS provider’s records to point to the new site. Run platform environment:info edge_hostname -p \u0026lt;NEW_PROJECT_ID\u0026gt; to get the domain name to point the CNAME at. It may take some time for the DNS change and SSL change to propagate. Until it does, some browsers may not see the new site or may get an SSL mismatch error. In most cases that will resolve itself in 1-3 hours. 4. Remove the old project Once the new project is running and the DNS has fully propagated you can delete the old project.",
        "section": "Tutorials",
        "subsections": " Why migrate between regions? Scripted migration process  0. Preparation 1. Create a new project 2. Download and invoke the script 3. Remove the old project   Manual migration process  0. Preparation 1. Create and populate a new project 2. Maintain the mirror 3. Launch the new site 4. Remove the old project    ",
        "image": "",
        "url": "/tutorials/region-migration.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "aab5900a5b02608c3630b6f27a0692aa",
        "title": "Relationships",
        "description": "",
        "text": " The relationships block defines how services are mapped within your application. By default, your application may not talk to any other container within a project. To access another container you must define a relationship to it. Each relationship has an arbitrary name, although by convention the primary SQL database of an application (if any) is usually database. That is the name by which the relationship will be known in the PLATFORM_RELATIONSHIPS environment variable, which will include credentials for accessing the service. The relationship is specified in the form service_name:endpoint_name. The “service name” is the name of the service given in .platform/services.yaml, or the name of another application in the same project (that is, the name property of the .platform.app.yaml file for that application). The “endpoint” is the exposed functionality of the service to use. For most services the endpoint is the same as the service type. On a few services (i.e. MariaDB and Solr ) you can define additional explicit endpoints for multiple databases and cores in your services.yaml file, and you will need to match those endpoints in your relationships. See the Services documentation for a full list of currently supported service types and service endpoints. How do I get access to multiple services? In the following example, there is a single MySQL service named mysqldb offering two databases, a Redis cache service named rediscache, and an Elasticsearch service named searchserver. relationships:database:\u0026#39;mysqldb:db1\u0026#39;database2:\u0026#39;mysqldb:db2\u0026#39;cache:\u0026#39;rediscache:redis\u0026#39;search:\u0026#39;searchserver:elasticsearch\u0026#39;",
        "section": "Configure your application",
        "subsections": " How do I get access to multiple services?  ",
        "image": "",
        "url": "/configuration/app/relationships.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "974f0e315c9b3c74f5a0f411c8786a35",
        "title": "Troubleshooting",
        "description": "",
        "text": " Now your application has been taken live! As mentioned before, it may take a little time for the DNS to fully propagate, depending on the registrar. Otherwise, your domain should now point to the master production environment of your project. Additional information If through the following steps your project did not successfully configure to your domain, you can consult the Troubleshooting guide here: Going Live: Troubleshooting Consider using the Fastly CDN for increased performance and more control over caching: Going Live: Fastly Consider using Cloudflare’s TLS/SSL service to secure your site via HTTPS when using a CDN: Going Live: Cloudflare Back",
        "section": "Getting started",
        "subsections": "   Additional information    ",
        "image": "",
        "url": "/gettingstarted/next-steps/going-live/troubleshooting.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "c7ae3b87756f3ffdedf3bedc6693017b",
        "title": "Using Elasticsearch with Drupal 8.x",
        "description": "",
        "text": " Requirements Add the Drupal modules You will need to add the Search API and Elasticsearch Connector modules to your project. If you are using composer, the easiest way to add them is to simply run: $ composer require drupal/search_api drupal/elasticsearch_connector And then commit the changes to composer.json and composer.lock. Add an Elasticsearch service First you need to create an Elasticsearch service. In your .platform/services.yaml file, add or uncomment the following: elasticsearch:type:elasticsearch:6.5disk:2014The above definition defines a single Elasticsearch 6.5 server. Because Elasticsearch defines additional indexes dynamically there is no need to define custom endpoints. Expose the Elasticsearch service to your application In your .platform.app.yaml file, you now need to open a connection to the new Elasticsearch service. Under the relationships section, add or uncomment the following: relationships:elasticsearch:\u0026#39;elasticsearch:elasticsearch\u0026#39;Configuration Because Drupal defines connection information via the Configuration Management system, you will need to first define an Elasticsearch “Cluster” at admin/config/search/elasticsearch-connector. Note the “machine name” the server is given. Then, paste the following code snippet into your settings.platformsh.php file. Note: If you do not already have the Platform.sh Config Reader library installed and referenced at the top of the file, you will need to install it with composer require platformsh/config-reader and then add the following code before the block below: \u0026lt;?php $platformsh = new if (!$platformsh-\u0026gt;inRuntime()) { return; } Edit the value of $relationship_name if you are using a different relationship. Edit the value of $es_server_name to match the machine name of your cluster in Drupal. \u0026lt;?php // Update these values to the relationship name (from .platform.app.yaml) // and the machine name of the server from your Drupal configuration. $relationship_name = \u0026#39;elasticsearch\u0026#39;; $es_cluster_name = \u0026#39;YOUR_CLUSTER_HERE\u0026#39;; if ($platformsh-\u0026gt;hasRelationship($relationship_name)) { $platformsh-\u0026gt;registerFormatter(\u0026#39;drupal-elastic\u0026#39;, function($creds) { return sprintf(\u0026#39;http://%s:%s\u0026#39;, $creds[\u0026#39;host\u0026#39;], $creds[\u0026#39;port\u0026#39;]); }); // Set the connector configuration to the appropriate value, as defined by the formatter above. $config[\u0026#39;elasticsearch_connector.cluster.\u0026#39; . $es_cluster_name][\u0026#39;url\u0026#39;] = $platformsh-\u0026gt;formattedCredentials($relationship_name, \u0026#39;drupal-elastic\u0026#39;); } Commit that code and push. The specified cluster will now always point to the Elasticsearch service. Then configure Search API as normal.",
        "section": "Getting Started",
        "subsections": " Requirements  Add the Drupal modules Add an Elasticsearch service Expose the Elasticsearch service to your application   Configuration  ",
        "image": "",
        "url": "/frameworks/drupal8/elasticsearch.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "0b747495f2ea60c887be32512df5f482",
        "title": "Variables",
        "description": "",
        "text": " Platform.sh allows a high degree of control over both the build process and the runtime environment of a project. Part of that control comes in the form of variables that are set independently of the project’s code base but available either at build or runtime for your code to leverage. Platform.sh also exposes additional information to your application that way, including information like database credentials, the host or port it can use, and so forth. Type Definer Scope Inheritance Build Runtime Application Application Application n/a Yes Yes Project User Project n/a Yes Yes Environment User Environment Optional No Yes Platform.sh Pre-defined Environment n/a Some Yes All of those may be simple strings or base64-encoded JSON-serialized values. In case of name collisions, Platform.sh-provided values override user-provided environment variables, which override user-provided project-level variables, which override application-provided variables. (That is, lower items in the list above take precedence.) Types Application-provided variables Variables may be set in code , using the .platform.app.yaml file. These values of course will be the same across all environments and present in the Git repository, which makes them a poor fit for API keys and such. This capability is mainly to define values that an application expects via an environment variable that should be consistent across all environments. For example, the PHP Symfony framework has a SYMFONY_ENV property that users may wish to set to prod on all environments to ensure a consistent build, or it may be used to set PHP configuration values . Application-provided variables are available at both build time and runtime. Project variables Project variables are defined by the user and bound to a whole project. They are available both at build time (and therefore from a build hook) and at runtime, and are the same for all environments in the project. New project variables can be added using the CLI. For example, the following command creates a project-level variable “foo” with the value “bar”: platform variable:create --level project --name foo --value bar Project variables are a good place to store secret information that is needed at build time, such as credentials for a private 3rd party code repository. By default, project variables will be available at both build time and runtime. You can suppress one or the other with the --no-visible-build and --no-visible-runtime flags, such as if you want to hide certain credentials from runtime entirely. For example, the following (silly) example will define a project variable but hide it from both build and runtime: platform variable:create --level project --name foo --value bar --visible-build false --visible-runtime false Naturally in practice you’ll want to use only one or the other, or allow the variable to be visible in both cases. Project variables may also be marked --sensitive true. That flag will mark the variable to not be readable through the management console once it is set. That makes it somewhat more private as requests through the Platform.sh CLI will not be able to view the variable. However, it will still be readable from within the application container like any other variable. Environment variables Environment-level variables can also be set through the management console , or using the CLI. Environment variables are bound to a specific environment or branch. An environment will also inherit variables from its parent environment, unless it has a variable defined with the same name. That allows you to define your development variables only once, and use them on all the child environments. For instance, to create an environment variable “foo” with the value “bar” on the currently checked out environment/branch, run: $ platform variable:create --level environment --name foo --value bar That will set a variable on the currently active environment (that is, the branch you have checked out). To set a variable on a different environment include the -e switch to specify the environment name. There are two additional flags available on environment variables: --inheritable and --sensitive. Setting --inheritable false will cause the variable to not be inherited by child environments. That is useful for setting production-only values on the master branch, and allowing all other environments to use a project-level variable of the same name. Setting --sensitive true flag will mark the variable to not be readable through the management console once it is set. That makes it somewhat more private as requests through the Platform.sh CLI will not be able to view the variable. However, it will still be readable from within the application container like any other variable. For example, the following command will allow you to set a PayPal secret value on the master branch only; other environments will not inherit it and either get a project variable of the same name if it exists or no value at all. It will also not be readable through the API. $ platform variable:create --name paypal_id --inheritable false --sensitive true If you omit the variable --value from the command line as above, you will be prompted to enter the value interactively. Changing an environment variable will cause that environment to be redeployed so that it gets the new value. However, it will not redeploy any child environments. If you want those to get the new value you will need to redeploy them yourself. Environment variables are a good place to store values that apply only on Platform.sh and not on your local development environment. This includes API credentials for 3rd party services, mode settings if your application has a separate “Dev” and “Prod” runtime toggle, etc. Platform.sh-provided variables Platform.sh also provides a series of variables by default. These inform an application about its runtime configuration. The most important of these is relationship information, which tells the application how to connect to databases and other services defined in services.yaml. They are always prefixed with PLATFORM_* to differentiate them from user-provided values. The following variables are only available at build time, and may be used in a build hook: PLATFORM_OUTPUT_DIR: The output directory for compiled languages at build time. Will be equivalent to PLATFORM_APP_DIR in most cases. The following variables are available at both runtime and at build time, and may be used in a build hook: PLATFORM_APP_DIR: The absolute path to the application directory. PLATFORM_APPLICATION: A base64-encoded JSON object that describes the application. It maps the content of the .platform.app.yaml that you have in Git and it has a few subkeys. PLATFORM_APPLICATION_NAME: The name of the application, as configured in the .platform.app.yaml file. PLATFORM_PROJECT: The ID of the project. PLATFORM_TREE_ID: The ID of the tree the application was built from. It’s essentially the SHA hash of the tree in Git. If you need a unique ID for each build for whatever reason this is the value you should use. PLATFORM_VARIABLES: A base64-encoded JSON object which keys are variables names and values are variable values (see below). Note that the values available in this structure may vary between build and runtime depending on the variable type as described above. PLATFORM_PROJECT_ENTROPY: A random value created when the project is first created, which is then stable throughout the project’s life. This can be used for Drupal hash salt, Symfony secret, or other similar values in other frameworks. The following variables exist only at runtime. If used in a build hook they will evaluate to an empty string like any other unset variable: PLATFORM_BRANCH: The name of the Git branch. PLATFORM_DOCUMENT_ROOT: The absolute path to the web document root, if applicable. PLATFORM_ENVIRONMENT: The name of the environment generated by the name of the Git branch. PLATFORM_SMTP_HOST: The SMTP host that email messages should be sent through. This value will be empty if mail is disabled for the current environment. PLATFORM_RELATIONSHIPS: A base64-encoded JSON object whose keys are the relationship name and the values are arrays of relationship endpoint definitions. See the documentation for each Service for details on each service type’s schema. PLATFORM_ROUTES: A base64-encoded JSON object that describes the routes that you defined in the environment. It maps the content of the .platform/routes.yaml file. On a Dedicated instance, the following additional variables are available at runtime only: PLATFORM_MODE: Set to enterprise in an Dedicated environment, both production and staging. Note that an Enterprise support plan doesn’t always imply a Dedicated production, but Dedicated production always implies an Enterprise support plan. PLATFORM_CLUSTER: Set to the cluster ID. PLATFORM_PROJECT: Set to the document root. This is typically the same as your cluster name for the production environment, while staging will have _stg or similar appended. Since values can change over time, the best thing is to inspect the variable at runtime then use it to configure your application. For example: echo $PLATFORM_RELATIONSHIPS | base64 --decode | json_pp {  database : [ {  host :  database.internal ,  ip :  246.0.97.91 ,  password :   ,  path :  main ,  port : 3306,  query : {  is_master : true },  scheme :  mysql ,  username :  user  } ],  redis : [ {  host :  redis.internal ,  ip :  246.0.97.88 ,  port : 6379,  scheme :  redis  } ] } Accessing variables You can get a list of all variables defined on a given environment either via the management console or using the CLI: $ platform variables \u0026#43;---------\u0026#43;-------\u0026#43;-----------\u0026#43;------\u0026#43; | ID | Value | Inherited | JSON | \u0026#43;---------\u0026#43;-------\u0026#43;-----------\u0026#43;------\u0026#43; | env:FOO | bar | No | No | \u0026#43;---------\u0026#43;-------\u0026#43;-----------\u0026#43;------\u0026#43; At build time Only Project variables are available at build time. They will be listed together in a single JSON array and exposed in the $PLATFORM_VARIABLES Unix environment variable. echo $PLATFORM_VARIABLES | base64 --decode { my_var :  this is a value } They can also be accessed from within a non-shell script via the language’s standard way of accessing environment variables. For instance, in PHP you would use getenv(\u0026#39;PLATFORM_VARIABLES\u0026#39;). Remember that in some cases they may be base64 JSON strings and will need to be unpacked. To do so from the shell, for instance, you would do: echo $PLATFORM_VARIABLES | base64 --decode { myvar :  this is a value } See below for how to expose a project variable as its own Unix environment variable. At runtime In a running container, which includes the deploy hook, your Project variables, Environment variables, and Platform.sh-provided variables are all exposed as Unix environment variables and can be accessed by your application through your language’s standard way of accessing environment variables. Platform.sh-defined variables will be exposed directly with the names listed above. Project and environment variables will be merged together into a single JSON array and exposed in the $PLATFORM_VARIABLES environment variable. In case of a matching name, an environment variable will override a variable of the same name in a parent environment, and both will override a project variable. For example, suppose we have the following variables defined: $ platform variables -e master Variables on the project Example (abcdef123456), environment master: \u0026#43;----------------\u0026#43;-------------\u0026#43;--------\u0026#43; | Name | Level | Value | \u0026#43;----------------\u0026#43;-------------\u0026#43;--------\u0026#43; | system_name | project | Spiffy | | system_version | project | 1.5 | | api_key | environment | abc123 | \u0026#43;----------------\u0026#43;-------------\u0026#43;--------\u0026#43; And the following variables defined on the branch feature-x, a child environment (and branch of) master: $ platform variables -e master Variables on the project Example (abcdef123456), environment feature-x: \u0026#43;----------------\u0026#43;-------------\u0026#43;--------\u0026#43; | Name | Level | Value | \u0026#43;----------------\u0026#43;-------------\u0026#43;--------\u0026#43; | system_name | project | Spiffy | | system_version | project | 1.5 | | api_key | environment | def456 | | system_version | environment | 1.7 | | debug_mode | environment | 1 | \u0026#43;----------------\u0026#43;-------------\u0026#43;--------\u0026#43; In this case, on the master environment $PLATFORM_VARIABLES would look like this: echo $PLATFORM_VARIABLES | base64 --decode | json_pp {  system_name :  Spiffy ,  system_version :  1.5 ,  api_key :  abc123  } While the same command on the feature-x branch would produce: {  system_name :  Spiffy ,  system_version :  1.7 ,  api_key :  def456 ,  debug_mode :  1  } In your application Check the individual documentation pages for accessing environment variables for your given application language. Shell: the jq utility PHP: the getenv() function Node.js: the process.env object Python: the os.environ object Ruby: the ENV accessor Java: the java.lang.System accessor Shell PHP Python Node.js Ruby Java export DB_USER= $(echo  $PLATFORM_RELATIONSHIPS  | base64 --decode | jq -r \u0026#39;.database[0].username\u0026#39;)  \u0026lt;?php // A simple variable. $projectId = getenv(\u0026#39;PLATFORM_PROJECT\u0026#39;); // A JSON-encoded value. $variables = json_decode(base64_decode(getenv(\u0026#39;PLATFORM_VARIABLES\u0026#39;)), TRUE); import os import json import base64 // A simple variable. project_id = os.getenv(\u0026#39;PLATFORM_PROJECT\u0026#39;) // A JSON-encoded value. variables = json.loads(base64.b64decode(os.getenv(\u0026#39;PLATFORM_VARIABLES\u0026#39;)).decode(\u0026#39;utf-8\u0026#39;)) // Utility to assist in decoding a packed JSON variable. function read_base64_json(varName) { try { return JSON.parse(new Buffer(process.env[varName], \u0026#39;base64\u0026#39;).toString()); } catch (err) { throw new Error(`no ${varName}environment variable`); } }; // A simple variable. let projectId = process.env.PLATFORM_PROJECT; // A JSON-encoded value. let variables = read_base64_json(\u0026#39;PLATFORM_VARIABLES\u0026#39;); // A simple variable. project_id = ENV[ PLATFORM_PROJECT ] || nil // A JSON-encoded value. variables = JSON.parse(Base64.decode64(ENV[ PLATFORM_VARIABLES ])) import com.fasterxml.jackson.databind.ObjectMapper; import java.io.IOException; import java.util.Base64; import java.util.Map; import static java.lang.System.getenv; import static java.util.Base64.getDecoder; public class App { public static void main(String[] args) throws IOException { // A simple variable. final String project = getenv( PLATFORM_PROJECT ); // A JSON-encoded value. ObjectMapper mapper = new ObjectMapper(); final Map\u0026lt;String, Object\u0026gt; variables = mapper.readValue( String.valueOf(getDecoder().decode(getenv( PLATFORM_VARIABLES ))), Map.class); } } Variable prefixes Certain variable name prefixes have special meaning. A few of these are defined by Platform.sh and are built-in. Others are simply available as a convention for your own application code to follow. Top-level environment variables By default, project and environment variables are only added as part of the $PLATFORM_VARIABLES Unix environment variable. However, you can also expose a variable as its own Unix environment variable by giving it the prefix env:. For example, the variable env:foo will create a Unix environment variable called FOO. (Note the automatic upper-casing.) $ platform variable:create --name env:foo --value bar With PHP, you can then access that variable with getenv(\u0026#39;FOO\u0026#39;). PHP-specific variables Any variable with the prefix php: will also be added to the php.ini configuration of all PHP-based application containers. For example, an environment variable named php:display_errors with value On is equivalent to placing the following in php.ini: display_errors = On This feature is primarily useful to override debug configuration on development environments, such as enabling errors or configuring the XDebug extension. For applying a configuration setting to all environments, or to vary them between different PHP containers in the same project, specify the variables in the .platform.app.yaml file for your application. See the PHP configuration page for more information. Drupal-specific variables As a convention, our provided Drupal template code will automatically map variables to Drupal’s configuration system. The logic varies slightly depending on the Drupal version. On Drupal 7 , any variable that begins with drupal: will be mapped to the global $conf array, which overrides Drupal’s variable_get() system. For instance, to force a site name from the Platform.sh variables (say to set it “This is a Dev site”) you would set the drupal:site_name variable. On Drupal 8 , any variable that begins with drupal: will be mapped to the global $settings array. That is intended for very low-level configuration. Also on Drupal 8, any variable that begins with d8config: will be mapped to the global $config array, which is useful for overriding drupal’s exportable configuration system. The variable name will need to contain two colons, one for d8config: and one for the name of the configuration object to override. For example, a variable named d8config:system.site:name will override the name property of the system.site configuration object. As the above logic is defined in a file in your Git repository you are free to change it if desired. The same behavior can also be easily implemented for any other application or framework. Shell variables You can also provide a .environment file as part of your application, in your application root (as a sibling of your .platform.app.yaml file, or files in the case of a multi-app configuration). That file will be sourced as a bash script when the container starts and on all SSH logins. It can be used to set any environment variables directly, such as the PATH variable. For example, the following .environment file will allow any executable installed using Composer as part of a project to be run regardless of the current directory: export PATH=/app/vendor/bin:$PATH Note that the file is sourced after all other environment variables above are defined, so they will be available to the script. That also means the .environment script has the “last word” on environment variable values and can override anything it wants to. How can I have a script behave differently on a dedicated cluster than on development? The following sample shell script will output a different value on the Dedicated cluster than the Development environment. if [  $PLATFORM_MODE  =  enterprise  ] ; then echo  Hello from the Enterprise  else echo  We\u0026#39;re on Development  fi How can I have a script behave differently on Production and Staging? In most Enterprise configurations the production branch is named production (whereas it is always master on Platform.sh Professional). The following test therefore should work in almost all cases: if [  $PLATFORM_MODE  =  enterprise  ] ; then if [  $PLATFORM_BRANCH  =  production  ] ; then echo  This is live on production  else echo  This is on staging  fi else echo  We\u0026#39;re on Development  fi",
        "section": "Development",
        "subsections": " Types  Application-provided variables Project variables Environment variables Platform.sh-provided variables   Accessing variables  At build time At runtime In your application   Variable prefixes  Top-level environment variables PHP-specific variables Drupal-specific variables   Shell variables How can I have a script behave differently on a dedicated cluster than on development? How can I have a script behave differently on Production and Staging?  ",
        "image": "",
        "url": "/development/variables.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "6fc12b48f3dc6db6e2ac5ef4d56e3d80",
        "title": "Access",
        "description": "",
        "text": " The access key in .platform.app.yaml defines the user roles who can log in via SSH to the environments they have permission to access. The specified role is a minimum; anyone with an access level of this role or higher can access the container via SSH. Possible values are admin, contributor, and viewer. The default is contributor. How do I restrict SSH access only to project administrators? The following block in .platform.app.yaml will restrict SSH access to just those users with “admin” privileges on the project or on the specific deployed environment. access:ssh:admin",
        "section": "Configure your application",
        "subsections": " How do I restrict SSH access only to project administrators?  ",
        "image": "",
        "url": "/configuration/app/access.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "dd81c9a0bdf8da0d4509b1a424a06ee4",
        "title": "Accessing your site",
        "description": "",
        "text": " Once you have an environment running, there are many ways to access it to perform needed tasks. The most obvious of course is to view it in a web browser; the available URLs are shown in the Platform.sh management console and on the command line after every Git push. By design, the only way to deploy new code is to push to the corresponding branch. That ensures a consistent, repeatable, auditable application instance at all times. Visiting the site on the web The web URL(s) for the site are listed in the management console under “Access site”. They can also be found on the command line, using the Platform.sh CLI : platform url Generally there will be two URLs created per route in your routes.yaml file: One HTTPS and one HTTP route that just redirects to HTTPS. If you are using the {all} placeholder in your routes.yaml file then there will be more, depending on how many domains you have configured in your project. Accessing the application with SSH Most interactions with Platform.sh require SSH key authentication, and you will need to set up your SSH keys before working on a site. Once that’s done, you can easily access the command line on your application over SSH. To log in to the environment that corresponds to your current branch, simply type: platform ssh To log in to some other environment, use the -e flag to specify the environment. The application container is a fully working Linux environment using the bash shell. Most of the system consists of a read-only file system (either the underlying container image or your built application image), so you cannot edit code live, but otherwise the full system is available to read and peruse. Any file mounts you have declared in your .platform.app.yaml will be writable. Additionally, you will be logged in as the same user that the web server runs as; that means you needn’t worry about the common problem of editing a file from the command line and from your application resulting in inconsistent and broken file ownership and permissions. Uploading and downloading files The writable static files in an application - including uploads, temporary and private files - are stored in mounts . The Platform.sh CLI can list mounts inside an application: $ platform mounts Mounts in the app drupal (environment master): \u0026#43;-------------------------\u0026#43;----------------------\u0026#43; | Path | Definition | \u0026#43;-------------------------\u0026#43;----------------------\u0026#43; | web/sites/default/files | shared:files/files | | private | shared:files/private | | tmp | shared:files/tmp | \u0026#43;-------------------------\u0026#43;----------------------\u0026#43; The CLI also helps transferring files to and from a mount, using the mount:upload and mount:download commands. These commands use the rsync utility, which in turn uses SSH. For example, to download files from the ‘private’ mount: $ platform mount:download --mount private --target ./private This will add, replace, and delete files in the local directory \u0026#39;private\u0026#39;. Are you sure you want to continue? [Y/n] Downloading files from the remote mount /app/private to /Users/alice/Projects/foo/private receiving file list ... done sent 16 bytes received 3.73K bytes 2.50K bytes/sec total size is 1.77M speedup is 471.78 time: 0.91s The download completed successfully. Uploading files to a mount is similar: $ platform mount:upload --mount private --source ./private This will add, replace, and delete files in the remote mount \u0026#39;private\u0026#39;. Are you sure you want to continue? [Y/n] Uploading files from /Users/alice/Projects/foo/private to the remote mount /app/private building file list ... done sent 2.35K bytes received 20 bytes 1.58K bytes/sec total size is 1.77M speedup is 745.09 time: 0.72s The upload completed successfully. Using SSH clients Many applications and protocols run on top of SSH, including SFTP, scp, and rsync. To obtain the SSH connection details for the environment either copy them out of the Platform.sh management console (under the “Access site” dropdown) or run: platform ssh --pipe That will output the connection string for SSH, including the username and host for the current project and environment. It will look something like \u0026lt;project ID\u0026gt;-\u0026lt;environment ID\u0026gt;--app@ssh.us.platform.sh. The part before the @ is the username, the part after is the host. Enter both of those into your SSH/SFTP client. No password is necessary, but your client will need to have access to the SSH private key that corresponds to the public key on Platform.sh. SFTP SFTP is another way to upload and download files to and from a remote environment. There are many SFTP clients available for every operating system; use whichever one works for you. SCP SCP is a simple command-line utility to copy files to and from a remote environment. For example, this command: scp  $(platform ssh --pipe) :web/uploads/diagram.png . will copy the file named diagram.png in the web/uploads directory (relative to the application root) to the current local directory. Reversing the order of the parameters will copy files up to the Platform.sh environment. Consult the SCP documentation for other possible options. Rsync For copying files to and from a remote environment, rsync is the best tool available. It is a little more complicated to use than scp, but it can also be a lot more efficient, especially if you are simply updating files that are already partially copied. The Platform.sh CLI mount:upload and mount:download commands (described above) are helpful wrappers around rsync that make it a little easier to use. However, it is also possible to use rsync on its own, for example: rsync -az  $(platform ssh --pipe) :web/uploads/ ./uploads/ This command will copy all files in the web/uploads directory on the remote environment to the uploads directory locally. Note that rsync is very sensitive about trailing / characters, so that may change the meaning of a command. Consult the rsync documentation for more details. Also see our migrating and exporting guides for more examples using rsync.",
        "section": "Development",
        "subsections": " Visiting the site on the web Accessing the application with SSH Uploading and downloading files Using SSH clients  SFTP SCP Rsync    ",
        "image": "",
        "url": "/development/access-site.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "ed19d198473b5dec3ab9b92ca8dc136e",
        "title": "Backup and restore",
        "description": "",
        "text": " Backups vary by our offering and their retention is governed by our [data retention]({{\u0026lt; relref “/security/data-retention.md” \u0026gt;}}). This section details our Recovery Point Objective (RPO) and Recovery Time Objective (RTO) for our Platform.sh Professional and Platform.sh Enterprise offerings. Platform.sh Professional With Platform.sh Professional, we have enabled our clients to manage their own backup and restore functions. Please see the backup and restore page for details. RPO: User defined. The RPO is configured by our clients. RTO: Variable. Recovery time will depend upon the size of the data we are recovering Platform.sh Enterprise-Grid and Enterprise-Dedicated RPO: 6 hours. Platform.sh takes a backup of Platform.sh Enterprise environments every 6 hours. RTO: Variable. Recovery time will depend upon the size of the data we are recovering",
        "section": "Security and compliance",
        "subsections": " Platform.sh Professional Platform.sh Enterprise-Grid and Enterprise-Dedicated  ",
        "image": "",
        "url": "/security/backups.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "6c121dcf0b7b4eb3df9def89475e1528",
        "title": "Configure services",
        "description": "",
        "text": " In the previous step, you created a collection of empty configuration files that will allow the project to be deployed on Platform.sh. Now you will need to include information that will tell Platform.sh how you want your application to connect to its services in a .platform/services.yaml file. With the following project structure: . ├── .platform │ ├── routes.yaml │ └── services.yaml ├── .platform.app.yaml └── \u0026lt; application code \u0026gt; An example .platform/services.yaml will look something like this: .platform/services.yaml # This file defines services you want available to your application.elasticsearch:type:elasticsearch:7.2disk:256size:Sinfluxdb:type:influxdb:1.7disk:256size:Skafka:type:kafka:2.2disk:512size:Smemcached:type:memcached:1.4size:Smongodb:type:mongodb:3.6disk:1024size:Smysql:type:mariadb:10.4disk:256size:Spostgresql:type:postgresql:11disk:256size:Srabbitmq:type:rabbitmq:3.7disk:256size:Sredis:type:redis:5.0size:Ssolr:type:solr:8.0disk:256size:Sconfiguration:cores:maincore:conf_dir:!archive solr-config endpoints:solr:core:maincore If your application does not use any services at this point then you can leave it blank, but it must exist in your repository to run on Platform.sh. If your application does use a database or other services, you can configure them with the following attributes: name: Provide a name for the service, so long as it is alphanumeric. If your application requires multiple services of the same type, make sure to give them different names so that your data from one service is never overwritten by another. type: This specifies the service type and its version using the format type:versionConsult the table below that lists all Platform.sh maintained services, along with their type and supported versions. The links will take you to each service’s dedicated page in the documentation. Service type Supported version Headless Chrome chrome-headless 73 Elasticsearch elasticsearch 6.5, 7.2 InfluxDB influxdb 1.2, 1.3, 1.7 Kafka kafka 2.1, 2.2, 2.3, 2.4 MariaDB mariadb 10.0, 10.1, 10.2, 10.3, 10.4 Memcached memcached 1.4, 1.5, 1.6 MongoDB mongodb 3.0, 3.2, 3.4, 3.6 Network Storage network-storage 1.0 Oracle MySQL oracle-mysql 5.7, 8.0 PostgreSQL postgresql 9.6, 10, 11, 12 RabbitMQ rabbitmq 3.5, 3.6, 3.7, 3.8 Redis redis 3.2, 4.0, 5.0 Solr solr 3.6, 4.1, 6.3, 6.6, 7.6, 7.7, 8.0, 8.4 Varnish varnish 5.6, 6.0 disk: The disk attribute configures the amount of persistent disk that will be allocated between all of your services. Projects by default are allocated 5 GB (5120 MB), and that space can be distributed across all of your services. Note that not all services require disk space. If you specify a disk attribute for a service that doesn’t use it, like Redis, you will receive an error when trying to push your changes. Note: Each language and framework may have additional attributes that you will need to include in .platform/services.yaml depending on the needs of your application. To find out what else you may need to include to configure your services, consult The Services documentation for Platform.sh The documentation goes into far more extensive detail of which attributes can also be included for service configuration, and should be used as your primary reference. Language-specific templates for Platform.sh Projects: Compare the .platform/services.yaml file from the simple template above to other templates when writing your own. Platform.sh provides managed services, and each service comes with considerable default configuration that you will not have to include yourself in .services.yaml. Next, you will next need to tell Platform.sh how to build and deploy your application using the .platform.app.yaml file. Back I\u0026#39;ve configured my services",
        "section": "Getting started",
        "subsections": "",
        "image": "",
        "url": "/gettingstarted/gettingstarted/own-code/service-configuration.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "400014aac1482ba72a62ecbb87b9c99e",
        "title": "MongoDB (Database service)",
        "description": "",
        "text": " Supported versions Grid Dedicated 3.0 3.2 3.4 3.6 None available Note: Downgrades of MongoDB are not supported. MongoDB will update its own datafiles to a new version automatically but cannot downgrade them. If you want to experiment with a later version without committing to it use a non-master environment. Relationship The format exposed in the $PLATFORM_RELATIONSHIPS environment variable : {  cluster :  rjify4yjcwxaa-master-7rqtwti ,  host :  mongodb.internal ,  hostname :  ldh423mk2e7o6qto2syljqbg5u.mongodb.service._.eu-3.platformsh.site ,  ip :  169.254.117.167 ,  password :  main ,  path :  main ,  port : 27017,  query : {  is_master : true },  rel :  mongodb ,  scheme :  mongodb ,  service :  mongodb ,  type :  mongodb:3.6 ,  username :  main  } Usage example In your .platform/services.yaml: dbmongo:type:mongodb:3.6disk:512 The minimum disk size for MongoDB is 512 (MB). relationships:mongodatabase: dbmongo:mongodb  Note: You will need to use the mongodb type when defining the service # .platform/services.yamlservice_name:type:mongodb:versiondisk:256 and the endpoint mongodb when defining the relationship # .platform.app.yamlrelationships:relationship_name:“service_name:mongodb” Your service_name and relationship_name are defined by you, but we recommend making them distinct from each other. For PHP, in your .platform.app.yaml add: runtime:extensions:- mongodb(Before PHP 7, use mongo instead.) You can then use the service in a configuration file of your application with something like: Go Java Node.js PHP Python package examples import (  context   fmt  psh  github.com/platformsh/config-reader-go/v2  mongoPsh  github.com/platformsh/config-reader-go/v2/mongo   go.mongodb.org/mongo-driver/bson   go.mongodb.org/mongo-driver/mongo   go.mongodb.org/mongo-driver/mongo/options   time  ) func UsageExampleMongoDB() string { // Create a NewRuntimeConfig object to ease reading the Platform.sh environment variables. // You can alternatively use os.Getenv() yourself. config, err := psh.NewRuntimeConfig() checkErr(err) // Get the credentials to connect to the Solr service. credentials, err := config.Credentials( mongodb ) checkErr(err) // Retrieve the formatted credentials for mongo-driver. formatted, err := mongoPsh.FormattedCredentials(credentials) checkErr(err) // Connect to MongoDB using the formatted credentials. ctx, _ := context.WithTimeout(context.Background(), 10*time.Second) client, err := mongo.Connect(ctx, options.Client().ApplyURI(formatted)) checkErr(err) // Create a new collection. collection := client.Database( main ).Collection( starwars ) // Clean up after ourselves. err = collection.Drop(context.Background()) checkErr(err) // Create an entry. res, err := collection.InsertOne(ctx, bson.M{ name :  Rey ,  occupation :  Jedi }) checkErr(err) id := res.InsertedID // Read it back. cursor, err := collection.Find(context.Background(), bson.M{ _id : id}) checkErr(err) var name string var occupation string for cursor.Next(context.Background()) { document := struct { Name string Occupation string }{} err := cursor.Decode(\u0026amp;document) checkErr(err) name = document.Name occupation = document.Occupation } return fmt.Sprintf( Found %s (%s) , name, occupation) } package sh.platform.languages.sample; import com.mongodb.MongoClient; import com.mongodb.client.MongoCollection; import com.mongodb.client.MongoDatabase; import org.bson.Document; import sh.platform.config.Config; import sh.platform.config.MongoDB; import java.util.function.Supplier; import static com.mongodb.client.model.Filters.eq; public class MongoDBSample implements Supplier\u0026lt;String\u0026gt; { @Override public String get() { StringBuilder logger = new StringBuilder(); // Create a new config object to ease reading the Platform.sh environment variables. // You can alternatively use getenv() yourself. Config config = new Config(); // The \u0026#39;database\u0026#39; relationship is generally the name of primary database of an application. // It could be anything, though, as in the case here here where it\u0026#39;s called  mongodb . MongoDB database = config.getCredential( mongodb , MongoDB::new); MongoClient mongoClient = database.get(); final MongoDatabase mongoDatabase = mongoClient.getDatabase(database.getDatabase()); MongoCollection\u0026lt;Document\u0026gt; collection = mongoDatabase.getCollection( scientist ); Document doc = new Document( name ,  Ada Lovelace ) .append( city ,  London ); collection.insertOne(doc); Document myDoc = collection.find(eq( _id , doc.get( _id ))).first(); logger.append(collection.deleteOne(eq( _id , doc.get( _id )))); return logger.toString(); } } const mongodb = require(\u0026#39;mongodb\u0026#39;); const config = require( platformsh-config ).config(); exports.usageExample = async function() { const credentials = config.credentials(\u0026#39;mongodb\u0026#39;); const MongoClient = mongodb.MongoClient; var client = await MongoClient.connect(config.formattedCredentials(\u0026#39;mongodb\u0026#39;, \u0026#39;mongodb\u0026#39;)); let db = client.db(credentials[ path ]); let collection = db.collection( startrek ); const documents = [ {\u0026#39;name\u0026#39;: \u0026#39;James Kirk\u0026#39;, \u0026#39;rank\u0026#39;: \u0026#39;Admiral\u0026#39;}, {\u0026#39;name\u0026#39;: \u0026#39;Jean-Luc Picard\u0026#39;, \u0026#39;rank\u0026#39;: \u0026#39;Captain\u0026#39;}, {\u0026#39;name\u0026#39;: \u0026#39;Benjamin Sisko\u0026#39;, \u0026#39;rank\u0026#39;: \u0026#39;Prophet\u0026#39;}, {\u0026#39;name\u0026#39;: \u0026#39;Katheryn Janeway\u0026#39;, \u0026#39;rank\u0026#39;: \u0026#39;Captain\u0026#39;}, ]; await collection.insert(documents, {w: 1}); let result = await collection.find({rank: Captain }).toArray(); let output = \u0026#39;\u0026#39;; output \u0026#43;= `\u0026lt;table\u0026gt; \u0026lt;thead\u0026gt; \u0026lt;tr\u0026gt;\u0026lt;th\u0026gt;Name\u0026lt;/th\u0026gt;\u0026lt;th\u0026gt;Rank\u0026lt;/th\u0026gt;\u0026lt;/tr\u0026gt; \u0026lt;/thead\u0026gt; \u0026lt;tbody\u0026gt;`; Object.keys(result).forEach((key) =\u0026gt; { output \u0026#43;= }); output \u0026#43;= // Clean up after ourselves. collection.remove(); return output; }; \u0026lt;?php declare(strict_types=1); use use // Create a new config object to ease reading the Platform.sh environment variables. // You can alternatively use getenv() yourself. $config = new Config(); // The \u0026#39;database\u0026#39; relationship is generally the name of primary database of an application. // It could be anything, though, as in the case here here where it\u0026#39;s called  mongodb . $credentials = $config-\u0026gt;credentials(\u0026#39;mongodb\u0026#39;); try { $server = sprintf(\u0026#39;%s://%s:%s@%s:%d/%s\u0026#39;, $credentials[\u0026#39;scheme\u0026#39;], $credentials[\u0026#39;username\u0026#39;], $credentials[\u0026#39;password\u0026#39;], $credentials[\u0026#39;host\u0026#39;], $credentials[\u0026#39;port\u0026#39;], $credentials[\u0026#39;path\u0026#39;] ); $client = new Client($server); $collection = $client-\u0026gt;main-\u0026gt;starwars; $result = $collection-\u0026gt;insertOne([ \u0026#39;name\u0026#39; =\u0026gt; \u0026#39;Rey\u0026#39;, \u0026#39;occupation\u0026#39; =\u0026gt; \u0026#39;Jedi\u0026#39;, ]); $id = $result-\u0026gt;getInsertedId(); $document = $collection-\u0026gt;findOne([ \u0026#39;_id\u0026#39; =\u0026gt; $id, ]); // Clean up after ourselves. $collection-\u0026gt;drop(); printf( Found %s (%s)\u0026lt;br $document-\u0026gt;name, $document-\u0026gt;occupation); } catch $e) { print $e-\u0026gt;getMessage(); } from pymongo import MongoClient from platformshconfig import Config def usage_example(): # Create a new Config object to ease reading the Platform.sh environment variables. # You can alternatively use os.environ yourself. config = Config() # The \u0026#39;database\u0026#39; relationship is generally the name of primary SQL database of an application. # It could be anything, though, as in the case here here where it\u0026#39;s called  mongodb . credentials = config.credentials(\u0026#39;mongodb\u0026#39;) try: formatted = config.formatted_credentials(\u0026#39;mongodb\u0026#39;, \u0026#39;pymongo\u0026#39;) server = \u0026#39;{0}://{1}:{2}@{3}\u0026#39;.format( credentials[\u0026#39;scheme\u0026#39;], credentials[\u0026#39;username\u0026#39;], credentials[\u0026#39;password\u0026#39;], formatted ) client = MongoClient(server) collection = client.main.starwars post = {  name :  Rey ,  occupation :  Jedi  } post_id = collection.insert_one(post).inserted_id document = collection.find_one( { _id : post_id} ) # Clean up after ourselves. collection.drop() return \u0026#39;Found {0} ({1})\u0026lt;br /\u0026gt;\u0026#39;.format(document[\u0026#39;name\u0026#39;], document[\u0026#39;occupation\u0026#39;]) except Exception as e: return e Exporting data The most straightforward way to export data from a MongoDB database is to open an SSH tunnel to it and simply export the data directly using MongoDB’s tools. First, open an SSH tunnel with the Platform.sh CLI: platform tunnel:open That will open an SSH tunnel to all services on your current environment, and produce output something like the following: SSH tunnel opened on port 30000 to relationship: database SSH tunnel opened on port 30001 to relationship: redis The port may vary in your case. You will also need to obtain the user, password, and database name from the relationships array, as above. Then, simply connect to that port locally using mongodump (or your favorite MongoDB tools) to export all data in that server: mongodump --port 30000 -u main -p main --authenticationDatabase main --db main (If necessary, vary the -u, -p, --authenticationDatabase and --db flags.) As with any other shell command it can be piped to another command to compress the output or redirect it to a specific file. For further references please see the official mongodump documentation . Upgrading To upgrade to 3.6 from a version earlier than 3.4, you must successively upgrade major releases until you have upgraded to 3.4. For example, if you are running a 3.0 image, you must upgrade first to 3.2 and then upgrade to 3.4 before you can upgrade to 3.6. For more details on upgrading and how to handle potential application backward compatibility issues, please see Release Notes for MongoDB .",
        "section": "Configure services",
        "subsections": " Supported versions Relationship Usage example Exporting data Upgrading  ",
        "image": "",
        "url": "/configuration/services/mongodb.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "2f655ad81e670538712fff005b813864",
        "title": "Spring",
        "description": "",
        "text": " The Spring Framework provides a comprehensive programming and configuration model for modern Java-based enterprise applications - on any kind of deployment platform. Platform.sh is flexible, and allows you to use Spring Framework in several flavors such as Spring MVC and Spring Boot . Services The configuration reader library for Java is used in these examples, so be sure to check out the documentation for installation instructions and the latest version. MongoDB You can use Spring Data MongoDB to use MongoDB with your application by first determining the MongoDB client programmatically. import com.mongodb.MongoClient; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import org.springframework.data.mongodb.config.AbstractMongoConfiguration; import sh.platform.config.Config; import sh.platform.config.MongoDB; @Configuration public class MongoConfig extends AbstractMongoConfiguration { private Config config = new Config(); @Override @Bean public MongoClient mongoClient() { MongoDB mongoDB = config.getCredential( database , MongoDB::new); return mongoDB.get(); } @Override protected String getDatabaseName() { return config.getCredential( database , MongoDB::new).getDatabase(); } } Apache Solr You can use Spring Data Solr to use Solr with your application by first determining the Solr client programmatically. import org.apache.solr.client.solrj.impl.HttpSolrClient; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import org.springframework.data.solr.core.SolrTemplate; import sh.platform.config.Config; import sh.platform.config.Solr; @Configuration public class SolrConfig { @Bean public HttpSolrClient elasticsearchTemplate() { Config config = new Config(); final Solr credential = config.getCredential( solr , Solr::new); final HttpSolrClient httpSolrClient = credential.get(); String url = httpSolrClient.getBaseURL(); httpSolrClient.setBaseURL(url.substring(0, url.lastIndexOf(\u0026#39;/\u0026#39;))); return httpSolrClient; } @Bean public SolrTemplate solrTemplate(HttpSolrClient client) { return new SolrTemplate(client); } } Redis You can use Spring Data Redis to use Redis with your application by first determining the Redis client programmatically. import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import org.springframework.data.redis.connection.jedis.JedisConnectionFactory; import org.springframework.data.redis.core.RedisTemplate; import org.springframework.data.redis.serializer.GenericToStringSerializer; @Configuration public class RedisConfig { @Bean JedisConnectionFactory jedisConnectionFactory() { Config config = new Config(); RedisSpring redis = config.getCredential( redis , RedisSpring::new); return redis.get(); } @Bean public RedisTemplate\u0026lt;String, Object\u0026gt; redisTemplate() { final RedisTemplate\u0026lt;String, Object\u0026gt; template = new RedisTemplate\u0026lt;String, Object\u0026gt;(); template.setConnectionFactory(jedisConnectionFactory()); template.setValueSerializer(new GenericToStringSerializer\u0026lt;Object\u0026gt;(Object.class)); return template; } } MySQL MySQL is an open-source relational database technology. Spring has robust integration with this technology: Spring Data JPA . The first step is to choose the database that you would like to use in your project. Define the driver for MySQL and the Java dependencies, then determine the DataSource client programmatically: import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import sh.platform.config.Config; import sh.platform.config.MySQL; import javax.sql.DataSource; @Configuration public class DataSourceConfig { @Bean(name =  dataSource ) public DataSource getDataSource() { Config config = new Config(); MySQL database = config.getCredential( database , MySQL::new); return database.get(); } } Note: You can use the same MySQL driver for MariaDB as well if you wish to do so. MariaDB MariaDB is an open-source relational database technology. Spring has robust integration with this technology: Spring Data JPA . The first step is to choose the database that you would like to use in your project. Define the driver for MariaDB and the Java dependencies, then determine the DataSource client programmatically: import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import sh.platform.config.Config; import sh.platform.config.MariaDB; import javax.sql.DataSource; @Configuration public class DataSourceConfig { @Bean(name =  dataSource ) public DataSource getDataSource() { Config config = new Config(); MariaDB database = config.getCredential( database , MariaDB::new); return database.get(); } } PostgreSQL PostgreSQL is an open-source relational database technology. Spring has robust integration with this technology: Spring Data JPA . The first step is to choose the database that you would like to use in your project. Define the driver for PostgreSQL and the Java dependencies, then determine the DataSource client programmatically: import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import sh.platform.config.Config; import sh.platform.config.PostgreSQL; import javax.sql.DataSource; @Configuration public class DataSourceConfig { @Bean(name =  dataSource ) public DataSource getDataSource() { Config config = new Config(); PostgreSQL database = config.getCredential( database , PostgreSQL::new); return database.get(); } } RabbitMQ You can use Spring JMS to use RabbitMQ with your application by first determining the RabbitMQ client programmatically. import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import org.springframework.jms.annotation.EnableJms; import org.springframework.jms.connection.CachingConnectionFactory; import org.springframework.jms.support.converter.MappingJackson2MessageConverter; import org.springframework.jms.support.converter.MessageConverter; import org.springframework.jms.support.converter.MessageType; import sh.platform.config.Config; import sh.platform.config.RabbitMQ; import javax.jms.ConnectionFactory; @Configuration @EnableJms public class JMSConfig { private ConnectionFactory getConnectionFactory() { Config config = new Config(); final RabbitMQ rabbitMQ = config.getCredential( rabbitmq , RabbitMQ::new); return rabbitMQ.get(); } @Bean public MessageConverter getMessageConverter() { MappingJackson2MessageConverter converter = new MappingJackson2MessageConverter(); converter.setTargetType(MessageType.TEXT); converter.setTypeIdPropertyName( _type ); return converter; } @Bean public CachingConnectionFactory getCachingConnectionFactory() { ConnectionFactory connectionFactory = getConnectionFactory(); return new CachingConnectionFactory(connectionFactory); } } Templates Spring Boot MySQL Spring Boot MongoDB",
        "section": "Featured frameworks",
        "subsections": " Services  MongoDB Apache Solr Redis MySQL MariaDB PostgreSQL RabbitMQ   Templates  ",
        "image": "",
        "url": "/frameworks/spring.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "bbf975afd8c0db4f2c472daa517d43e8",
        "title": "Upgrading to the Integrated UI",
        "description": "",
        "text": " Older Platform.sh Dedicated projects (created prior to October 2017) used a separate Git repository for Production and Staging. That also necessitated running most configuration changes through a ticket, and maintaining separate SSH credentials for each environment. These older projects can be upgraded to the new Integrated UI, which eliminates the extra Git repositories, many “must be a ticket” configuration changes, and makes the Production and Staging environments available in the UI. To add these environments to the Project Web Interface, review this entire document, complete a few preparatory steps, and submit a ticket. Your ticket is added to a queue for updating existing Dedicated projects. The process may take time to complete, so check your ticket for details, timing, and other important information. We recommend this upgrade for all users. New Features The new Project Web Interface provides the following features for the Pro plan Staging and Production environments: Add and manage user access to the environments Sync code between Staging and Production to Integration environments Merge code from Integration environment to Staging environment to Production environment Add and manage environment variables Manage build and deploy hooks with the .platform.app.yaml file Manage PHP versions and variables with the .platform.app.yaml file Manage cron jobs with the .platform.app.yaml file Configure environment settings Access the environments using SSH and URL View status, build logs, and deployment history You must still submit a support ticket to update and modify the following in the Staging and Production environments information: Redirects from routes.yaml file Managing PHP extensions Managing mounts You cannot perform the following: Branch from the Staging and Production environments Synchronize data from the Staging and Production environments Snapshot the Staging and Production environments Branching hierarchy Before converting your project, the branches include a repository for Development, Staging, and Production. Each repository has a master branch with deployment targets configured for Staging and Production. After converting your project, the hierarchical relationships appear in your Project Web Interface with two, main environment branches for Staging and Production: Before you upgrade When we add Staging and Production access to the Project Web Interface, we leverage the user accounts, branch user permissions, and environment variables from your Development master environment. To prepare, verify that your settings and environment variables are correct. Verify code matches across environments Verify user account access Prepare variables Verify code We strongly recommend working in your local development environment, deploying to Development, deploying to Staging, and, finally, deploying to Production. All code should match 100% across each of these environments. Before submitting a ticket, make sure you sync your code. This process creates a new branch of code for Staging and Production environments. If you have additional code, such as new extensions in your Production environment without following this workflow, then deployments from Integration or Staging overwrite your Production code. Verify user account access We recommend verifying your user account access and permissions set in the Integration environment. When adding Staging and Production to the Project Web Interface, the process includes all user accounts and settings. You can modify the settings and values for these environments after they are added. Log in to your Platform.sh account. From your project, click Master to view the environment information and settings. Click Configure environment. Click the Users tab to review the user accounts and permission configurations. Add, delete, or update users, if needed. Prepare variables When we convert your project to the new Project Web Interface, we add variables from Development environment to the Staging and Production environments. You can review, modify, and add variables through the current Project Web Interface prior to conversion. Log in to your Platform.sh account. From your project, click the master branch to view the environment information and settings. Click Configure environment. On the Variables tab, review the environment variables. To create a new variable, click Add Variable. To update an existing variable, click Edit next to the variable. For environment-specific variables, including sensitive data and values, you can add those variables after we update your Project Web Interface. Environment variables defined in .platform.app.yaml or a .environment file will continue to work. You can add and manage these variables via SSH and CLI commands directly into the Staging and Production environments. Enter a ticket for updating the Project Web Interface Enter a Support ticket with the suggested title “Connect Stg / Prod to Project’s UI”. In the ticket, request to have your project enabled with Staging and Production in the UI and confirm that you’ve taken the steps above to prepare your project. We will review the infrastructure and settings, create user and environment variables for Staging and Production environments, and update the ticket with results. Once started the process usually takes less than an hour. There should be no downtime on your production site, although you should not push any code to Git while the upgrade is in progress. When done, you can access review your project through the Project Web Interface. (Optional) Migrate environment variables After conversion, you can manually migrate specific environment variables for Staging and Production. Open a terminal and checkout a branch in your local environment. List all environment variables: platform variable:list Log in to your Platform.sh account. Click the Projects tab and the name of your project. Click the Staging or Production environment. On the Variables tab, review the environment variables. Enter the variable name and value. Select the Override checkbox if you want variables in the Project Web Interface to override local CLI or database values.",
        "section": "Platform.sh Dedicated",
        "subsections": " New Features  Branching hierarchy   Before you upgrade  Verify code Verify user account access Prepare variables   Enter a ticket for updating the Project Web Interface  (Optional) Migrate environment variables    ",
        "image": "",
        "url": "/dedicated/overview/upgrading.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "dad2fdf067ad82c29edf30f2421234ce",
        "title": "Using Redis with Drupal 7.x",
        "description": "",
        "text": " There are two options for using Redis with Drupal on Platform.sh, you can either use the PhpRedis extension or the Predis library. PhpRedis requires a PHP extension and should therefore be faster in most situations. Predis is written entirely in PHP and so would require no PHP extension to install locally, but at the cost of some performance. If you are unsure which to use, we recommend using PhpRedis. Requirements Add a Redis service First you need to create a Redis service. In your .platform/services.yaml file, add or uncomment the following: rediscache:type:redis:5.0That will create a service named rediscache, of type redis, specifically version 3.0. Expose the Redis service to your application In your .platform.app.yaml file, you now need to open a connection to the new Redis service. Under the relationships section, add the following: relationships:redis: rediscache:redis The key (left side) is the name that will be exposed to the application in the PLATFORM_RELATIONSHIPS variable. The right hand side is the name of the service you specified above (rediscache) and the endpoint (redis). If you named the service something different above, change rediscache to that. Add the Redis PHP extension Because the Redis extension for PHP has been known to have BC breaks at times, we do not bundle a specific verison by default. Instead, we provide a script to allow you to build your desired version in the build hook. See the PHP-Redis page for a simple-to-install script and instructions. (Skip this part if using Predis.) Add the Drupal module You will need to add the Redis module to your project. If you are using Drush Make, you can add these lines to your project.make file: projects[redis][version] = 3.15 To use the Predis library, also add it to your make file: libraries[predis][download][type] = get libraries[predis][download][url] = https://github.com/nrk/predis/archive/v1.0.3.tar.gz libraries[predis][directory_name] = predis libraries[predis][destination] = libraries Configuration To make use of the Redis cache you will need to set some Drupal variables. You can either do this in your settings.php file or by setting Platform Variables directly via the management console. In general, using the settings.php file is easier. Via settings.php To configure Drupal 7 to use our Redis server for caching, place the following at the end of settings.php, after the include directive for settings.local.php: \u0026lt;?php if (!empty($_ENV[\u0026#39;PLATFORM_RELATIONSHIPS\u0026#39;])) { $relationships = json_decode(base64_decode($_ENV[\u0026#39;PLATFORM_RELATIONSHIPS\u0026#39;]), TRUE); if (!empty($relationships[\u0026#39;redis\u0026#39;])) { $conf[\u0026#39;redis_client_host\u0026#39;] = $relationships[\u0026#39;redis\u0026#39;][0][\u0026#39;host\u0026#39;]; $conf[\u0026#39;redis_client_port\u0026#39;] = $relationships[\u0026#39;redis\u0026#39;][0][\u0026#39;port\u0026#39;]; $conf[\u0026#39;redis_client_interface\u0026#39;] = \u0026#39;PhpRedis\u0026#39;; $conf[\u0026#39;cache_backends\u0026#39;][] = \u0026#39;sites/all/modules/contrib/redis/redis.autoload.inc\u0026#39;; $conf[\u0026#39;cache_default_class\u0026#39;] = \u0026#39;Redis_Cache\u0026#39;; // The \u0026#39;cache_form\u0026#39; bin must be assigned to non-volatile storage. $conf[\u0026#39;cache_class_cache_form\u0026#39;] = \u0026#39;DrupalDatabaseCache\u0026#39;; // The \u0026#39;cache_field\u0026#39; bin must be transactional. $conf[\u0026#39;cache_class_cache_field\u0026#39;] = \u0026#39;DrupalDatabaseCache\u0026#39;; } } If using Predis, change the PhpRedis reference to Predis (case-sensitive). If your redis module is not installed in sites/all/modules/contrib, modify the cache_backends line accordingly. Via the management console Alternatively, add the following environment variables using the Platform.sh management console. Note, if you set a directory in the make file you will need to alter the variables to match. drupal:cache_backends [  sites/all/modules/contrib/redis/redis.autoload.inc  ] Note: Remember to tick the JSON Value box. Use the actual path to your Redis module in case it is in a different location. For example: sites/all/modules/redis. The location used above is the default when using Drush on Platform.sh. drupal:redis_client_host redis.internal drupal:cache_default_class Redis_Cache drupal:cache_class_cache_form DrupalDatabaseCache drupal:cache_class_cache_field DrupalDatabaseCache And finally, set the client interface to either PhpRedis or Predis. drupal:redis_client_interface PhpRedis Or Predis Verifying Redis is running Run this command in a SSH session in your environment redis-cli -h redis.internal info. You should run it before you push all this new code to your repository. This should give you a baseline of activity on your Redis installation. There should be very little memory allocated to the Redis cache. After you push this code, you should run the command and notice that allocated memory will start jumping. Note: If you use Domain Access and Redis, ensure that your Redis settings (particularly $conf[\u0026#39;cache_backends\u0026#39;]) are included before the Domain Access settings.inc file - see this Drupal.org issue for more information.",
        "section": "Getting Started",
        "subsections": " Requirements  Add a Redis service Expose the Redis service to your application Add the Redis PHP extension Add the Drupal module   Configuration  Via settings.php Via the management console Verifying Redis is running    ",
        "image": "",
        "url": "/frameworks/drupal7/redis.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "e58bb863ace9a398c828ebf5e6718f12",
        "title": "Using Solr with Drupal 8.x",
        "description": "",
        "text": " The Drupal Search API Solr module has a somewhat involved setup process, as it requires live access to the Solr server in order to generate the configuration files for it. The following procedure is therefore necessary to ensure each step is able to proceed. Search API Solr stores its configuration in the Drupal Configuration API. However, that system does not easily support environment-aware information. The setup process therefore depends on config-overrides in settings.platformsh.php, which may need to be modified slightly depending on your Solr configuration. Search API Solr requires Solr 6.6 or higher, and recommends Solr 8 or higher. Advanced Solr service configuration and implementation in other frameworks other than Drupal can be found at the Solr services page . Steps 0. Upgrade Symfony Event Dispatcher If you are running Drupal older than 9.0, a small workaround will be needed. The Solarium library used by Search API Solr requires the 4.3 version of the Symfony Event Dispatcher, whereas Drupal core ships with 3.4. The Search API Solr issue queue has a more detailed description of the problem. As noted there, the workaround for now is to run: composer require symfony/event-dispatcher: 4.3.4 as 3.4.35  in your project root and commit the resulting change to composer.json and composer.lock. That will cause Composer to install the 4.3 version of Event Dispatcher. Once this issue is resolved in core this step will no longer be necessary. 1. Add the Drupal modules You will need to add the Search API and Search API Solr modules to your project. If you are using composer, the easiest way to add them is to simply run: $ composer require drupal/search_api_solr And then commit the changes to composer.json and composer.lock. 2. Add a default Solr service Add the following to your .platform/services.yaml file. search:type:solr:8.0disk:1024configuration:cores:maincore:conf_dir:{}endpoints:main:core:maincoreThe above definition defines a single Solr 8.0 server. That server has 1 core defined: maincore, which will use a default configuration. (The default configuration is not suitable for production but will allow the module to connect to it.) It then defines one endpoint, main, which is connected to the maincore. 3. Expose the Solr service to your application In your .platform.app.yaml file, we now need to open a connection to the new Solr service. Under the relationships section, add the following: relationships:solrsearch:\u0026#39;search:main\u0026#39;That is, the application’s environment would include a solrsearch relationship that connects to the main endpoint, which is the maincore core. The key (left side) is the name that will be exposed to the application in the PLATFORM_RELATIONSHIPS variable . The right hand side is the name of the service specified above (search) and the endpoint (main). If you named the service or endpoint something than different above, change those values accordingly. 4. Add auto-configuration code to settings.platformsh.php The configuration can be managed from settings.platformsh.php by adding the following code snippet. It will override the environment-specific parts of the configuration object with the correct values to connect to the Platform.sh Solr instance. Note: If you do not already have the Platform.sh Config Reader library installed and referenced at the top of the file, you will need to install it with composer require platformsh/config-reader and then add the following code before the block below: \u0026lt;?php $platformsh = new if (!$platformsh-\u0026gt;inRuntime()) { return; } Edit the value of $relationship_name if you are using a different relationship. Edit the value of $solr_server_name if you want to configure a Solr server in Drupal other than the default server automatically created by Search API Solr module. \u0026lt;?php $platformsh-\u0026gt;registerFormatter(\u0026#39;drupal-solr\u0026#39;, function($solr) { // Default the solr core name to `collection1` for pre-Solr-6.x instances. return [ \u0026#39;core\u0026#39; =\u0026gt; substr($solr[\u0026#39;path\u0026#39;], 5) ? : \u0026#39;collection1\u0026#39;, \u0026#39;path\u0026#39; =\u0026gt; \u0026#39;\u0026#39;, \u0026#39;host\u0026#39; =\u0026gt; $solr[\u0026#39;host\u0026#39;], \u0026#39;port\u0026#39; =\u0026gt; $solr[\u0026#39;port\u0026#39;], ]; }); // Update these values to the relationship name (from .platform.app.yaml) // and the machine name of the server from your Drupal configuration. $relationship_name = \u0026#39;solrsearch\u0026#39;; $solr_server_name = \u0026#39;default_solr_server\u0026#39;; if ($platformsh-\u0026gt;hasRelationship($relationship_name)) { // Set the connector configuration to the appropriate value, as defined by the formatter above. $config[\u0026#39;search_api.server.\u0026#39; . $solr_server_name][\u0026#39;backend_config\u0026#39;][\u0026#39;connector_config\u0026#39;] = $platformsh-\u0026gt;formattedCredentials($relationship_name, \u0026#39;drupal-solr\u0026#39;); } If you are connecting to multiple Solr cores, repeat the second block above for each relationship/server, modifying the two variables accordingly. Commit all of the changes above and then push to deploy. 5. Enable the modules Once the site is deployed, go to the /admin/modules page and enable the “Search API Solr” module. Also enable the “Search API Solr Search Defaults” module in order to get a default server configuration. If you would rather create one yourself you may do so but then you must change the value of $solr_server_name in the code snippet in settings.platformsh.php. 6. Export and modify configuration In the Drupal admin area, go to /admin/config/search/search-api and select your server. (If you used the Search Defaults module, it will be named simply “Solr Server”). First verify that the module is able to connect to your Solr instance by ensuring that the “Server connection” reports “The Solr server could be reached.” You can now generate a config.zip file using the button at the top of the page. That will produce a Solr configuration that is customized for your current field configuration. Extract the file into the .platform directory of your site. It should unpack into a directory named solr_8.x_config or similar. Inside that directory, locate the solrcore.properties file. In that file, delete the entry for solr.install.dir. Its default value will not work and it is not required for Solr to operate. (The server already knows its installation directory.) Finally, move that directory to .platform/, and update the conf_dir to point to it. The services.yaml entry should now look approximately like this: search:type:solr:8.0disk:1024configuration:cores:maincore:conf_dir:!archive solr_8.x_config/ endpoints:main:core:maincoreAdd the new directory and updated services.yaml to Git, commit, and push. Note: If you change your Solr configuration in Drupal, say to change the Solr field configuration, you may need to regenerate your configuration. If so, repeat this entire step. 7. Verify that it worked Return to the Drupal UI for your server page. After deploying and reloading the page, the “Core Connection” field should now read “The Solr core could be accessed”.",
        "section": "Getting Started",
        "subsections": " Steps  0. Upgrade Symfony Event Dispatcher 1. Add the Drupal modules 2. Add a default Solr service 3. Expose the Solr service to your application 4. Add auto-configuration code to settings.platformsh.php 5. Enable the modules 6. Export and modify configuration 7. Verify that it worked    ",
        "image": "",
        "url": "/frameworks/drupal8/solr.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "47e22330347056aef010efaf1e9f3cf7",
        "title": "Using Xdebug",
        "description": "",
        "text": " Xdebug is a real-time debugger extension for PHP. While usually used for local development, it can also be helpful for debugging aberrant behavior on the server. It is available on Platform.sh Grid instances running PHP 7.2 and higher. As configured on Platform.sh, it avoids any runtime overhead for non-debug requests, even in production, and only allows SSH-tunneled connections to avoid any security issues. Setting up Xdebug Xdebug is not enabled the same way as other extensions, as it should not be active on most requests. Xdebug has a substantial impact on performance and should not be run in production. Instead, Xdebug can be enabled by adding the following configuration to the application’s .platform.app.yaml file: runtime:xdebug:idekey:PHPSTORMThe idekey value can be any arbitrary alphanumeric string, as long as it matches your IDE’s configuration. When that key is defined, Platform.sh will start a second PHP-FPM process on the container that is identically configured but also has Xdebug enabled. Only incoming requests that have an Xdebug cookie or query parameter set will be forwarded to the debug PHP-FPM process. All other requests will be directed to the normal PHP-FPM process and thus have no performance impact. Xdebug has numerous other configuration options available. They are all set as php.ini values, and can be configured the same way as any other php.ini setting . Consult the Xdebug documentation for a full list of available options, although in most cases the default configuration is sufficient. Using Xdebug Open a tunnel From your local checkout of your application, run platform environment:xdebug (or just platform xdebug) to open an SSH tunnel to the server. That SSH tunnel will allow your IDE and the server to communicate debug information securely. By default, Xdebug operates on port 9000. Generally, it is best to configure your IDE to use that port. If you wish to use an alternate port use the --port flag. To close the tunnel and terminate the debug connection, press Ctrl-C. Install an Xdebug helper While Xdebug can be triggered from the browser by adding a special query parameter, the preferred way is to use a browser plugin helper. One is available for Firefox and for Chrome . Their respective plugin pages document how to trigger them when needed. Using PHPStorm The configuration for Xdebug will be slightly different for each IDE. Platform.sh has no preference as to the IDE or editor you use, but we have provided configuration instructions for PHPStorm/IntelliJ due to its popularity in the PHP ecosystem. 1. Configure Xdebug In your PHPStorm Settings window, go to Languages \u0026amp; Frameworks \u0026gt; PHP \u0026gt; Debug. Ensure that the “Debug port” is set to the expected value (9000, or whatever you want to use in the --port flag) and that “Can accept external connections” is checked. Other settings are at your discretion. 2. Set DBGp Proxy In your PHPStorm Settings window, go to Languages \u0026amp; Frameworks \u0026gt; PHP \u0026gt; Debug \u0026gt; DBGp Proxy. Ensure that the “IDE key” field is set to the same value as the idekey in .platform.app.yaml. The exact value doesn’t matter as long as it matches. 3. Configure a server In your PHPStorm Settings window, go to Languages \u0026amp; Frameworks \u0026gt; PHP \u0026gt; Servers. Add a new server for your Platform.sh environment. The “Host” should be the hostname of the environment on Platform.sh you will be debugging. The “Port” should always be 443 and the “Debugger” set to Xdebug. Ensure that “Use path mappings” is checked, which will make available a tree of your project with a column to configure the remote path that it should map to. This page lets you define what remote paths on the server correspond to what path on your local machine. In the majority of cases you can just define the root of your application (either the repository root or the root of your PHP code base specifically in a multi-app setup) to map to app, as in the example below. Note: It may be easier to allow the debug process to connect once, allow it to fail, and then select the “Configure server mappings” error message. That will pre-populate most of the fields in this page and only require you to set the app root mapping. 4. Listen for connections Toggle on PHPStorm’s Xdebug listener. Either select Run \u0026gt; Start listening for PHP debug connections from the menu or click the icon in the toolbar. To disable PHPStorm’s listener, either select Run \u0026gt; Stop listening for PHP debug connections from the menu or toggle the icon in the toolbar. 5. Start debugging While in listen mode, start the platform xdebug tunnel. Use the Xdebug helper plugin for your browser to enable debugging. Set a breakpoint in your application, then load a page in your browser. The request should pause at the breakpoint and allow you to examine the running application.",
        "section": "PHP",
        "subsections": " Setting up Xdebug Using Xdebug  Open a tunnel Install an Xdebug helper   Using PHPStorm  1. Configure Xdebug 2. Set DBGp Proxy 3. Configure a server 4. Listen for connections 5. Start debugging    ",
        "image": "",
        "url": "/languages/php/xdebug.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "1cc3e9ece5a5478cb425f71eeffd3339",
        "title": "Configure application",
        "description": "",
        "text": " You will next need to include information that defines how you want your application to behave each time it is built and deployed on Platform.sh in a .platform.app.yaml file. With the following project structure: . ├── .platform │ ├── routes.yaml │ └── services.yaml ├── .platform.app.yaml └── \u0026lt; application code \u0026gt; An example .platform.app.yaml looks like this: Go Node.js PHP Python # This file describes an application. You can have multiple applications# in the same project.## See https://docs.platform.sh/user_guide/reference/platform-app-yaml.html# The name of this app. Must be unique within a project.name:app# The runtime the application uses.type:golang:1.14# The hooks executed at various points in the lifecycle of the application.hooks:build:| # Modify this line if you want to build differently or use an alternate name for your executable.gobuild-obin/app# The relationships of the application with services or other applications.## The left-hand side is the name of the relationship as it will be exposed# to the application in the PLATFORM_RELATIONSHIPS variable. The right-hand# side is in the form `\u0026lt;service name\u0026gt;:\u0026lt;endpoint name\u0026gt;`.relationships:database: db:mysql # The configuration of app when it is exposed to the web.web:upstream:socket_family:tcpprotocol:httpcommands:# If you change the build output in the build hook above, update this line as well.start:./bin/applocations:/:# Route all requests to the Go app, unconditionally.# If you want some files served directly by the web server without hitting Go, see# https://docs.platform.sh/configuration/app/web.htmlallow:falsepassthru:true# The size of the persistent disk of the application (in MB).disk:1024 # This file describes an application. You can have multiple applications# in the same project.## See https://docs.platform.sh/user_guide/reference/platform-app-yaml.html# The name of this app. Must be unique within a project.name:app# The runtime the application uses.type:nodejs:10# The relationships of the application with services or other applications.## The left-hand side is the name of the relationship as it will be exposed# to the application in the PLATFORM_RELATIONSHIPS variable. The right-hand# side is in the form `\u0026lt;service name\u0026gt;:\u0026lt;endpoint name\u0026gt;`.relationships:database: db:mysql # The configuration of app when it is exposed to the web.web:commands:start: node index.js # The size of the persistent disk of the application (in MB).disk:512mounts:\u0026#39;run\u0026#39;:source:localsource_path:run # This file describes an application. You can have multiple applications# in the same project.# The name of this app. Must be unique within a project.name:app# The type of the application to build.type:php:7.3build:flavor:composer# The hooks that will be performed when the package is deployed.hooks:build:| set -edeploy:| set -e# The relationships of the application with services or other applications.# The left-hand side is the name of the relationship as it will be exposed# to the application in the PLATFORM_RELATIONSHIPS variable. The right-hand# side is in the form `\u0026lt;service name\u0026gt;:\u0026lt;endpoint name\u0026gt;`.#relationships:# database:  db:mysql # The size of the persistent disk of the application (in MB).disk:2048# The mounts that will be performed when the package is deployed.mounts:# Because this directory is in the webroot, files here will be web-accessible.\u0026#39;web/uploads\u0026#39;:source:localsource_path:uploads# Files in this directory will not be web-accessible.\u0026#39;private\u0026#39;:source:localsource_path:private# The configuration of app when it is exposed to the web.web:locations: / :# The public directory of the app, relative to its root.root: web # The front-controller script to send non-static requests to.passthru: /index.php  # This file describes an application. You can have multiple applications# in the same project.## See https://docs.platform.sh/user_guide/reference/platform-app-yaml.html# The name of this app. Must be unique within a project.name:app# The runtime the application uses.type: python:3.7 # The build-time dependencies of the app.dependencies:python3:pipenv: 2018.10.13 # The hooks executed at various points in the lifecycle of the application.hooks:build:| pipenv install --system --deploy# The size of the persistent disk of the application (in MB).disk:1024# The relationships of the application with services or other applications.## The left-hand side is the name of the relationship as it will be exposed# to the application in the PLATFORM_RELATIONSHIPS variable. The right-hand# side is in the form `\u0026lt;service name\u0026gt;:\u0026lt;endpoint name\u0026gt;`.relationships:database: db:mysql rediscache: cache:redis # The configuration of app when it is exposed to the web.web:commands:start:pythonserver.py The .platform.app.yaml file is extremely flexible, and can contain many lines with very fine-grained control over your application. At the very least, Platform.sh requires three principle attributes in this file to control your builds: name: The name of your application container does not have to be the same as your project name, and in most single application cases you can simply name it app. You should notice in the next step, when you configure how requests are handled in .platform/routes.yaml that name is reused there, and it is important that they are the same. Note: If you are trying to to deploy microservices, the only constraint is that each of these application names must be unique. type: The type attribute in .platform.app.yaml sets the container base image for the application, and sets the primary language. In general, type should have the form type:\u0026lt;runtime\u0026gt;:\u0026lt;version\u0026gt;Set version to one supported by Platform.sh, which you can find below as well as in the documentation for each language: Language runtime Supported version C#/.Net Core dotnet 2.0, 2.1, 2.2, 3.1 Elixir elixir 1.9 Go golang 1.11, 1.12, 1.13, 1.14 Java java 11, 12, 8, 13 Lisp lisp 1.5 Node.js nodejs 6, 8, 10, 12 PHP php 7.2, 7.3, 7.4 Python python 2.7, 3.5, 3.6, 3.7, 3.8 Ruby ruby 2.3, 2.4, 2.5, 2.6, 2.7 disk: The disk attribute defines that amount of persistent storage you need to have available for your application, and requires a minimum value of 256 MB. There are a few additional keys in .platform.app.yaml you will likely need to use to fully configure your application, but are not required: relationships: Relationships define how services are mapped within your application. Without this block, an application cannot by default communicate with a service container. Provide a unique name for each relationship and associate it with a service. For example, if in the previous step you defined a MariaDB container in your .platform/services.yaml with db:type:mysql:10.4disk:256 You must define a relationship (i.e. database) in .platform.app.yaml to connect to it: relationships:database: db:mysql  Build and deploy tasks : There are a number of ways in which your Git repository is turned into a running application. In general, the build process will run the the build flavor, install dependencies, and then execute the build hook you provide. When the build process is completed, the deploy process will run the deploy hook. build: The build key defines what happens during the build process using the flavor property. This is a common inclusion for PHP and Node.js applications, so check the the documentation to see if your configuration requires this key. dependencies: This key makes it possible to install system-level dependencies as part of the build process. hooks: Hooks define custom scripts that you want to run at different points during the deployment process. build: The build hook is run after the build flavor if that is present. The file system is fully writable, but no services and only a subset of variables are available at this point. The full list of build time and runtime variables is available on the variables section of the public documentation. deploy: The deploy hook is run after the application container has been started, but before it has started accepting requests. Services are now available, but the file system will be read-only from this point forward. post-deploy: The post-deploy hook functions exactly the same as the deploy hook, but after the container is accepting connections. web: The web key configures the web server through a single web instance container running a single Nginx server process, behind which runs your application. commands: Defines the command to actually launch the application. The start key launches your application. In all languages except for PHP, web.commands.start should be treated as required. For PHP, you will instead need to define a script name in passthru, described below in locations. locations: Allows you to control how the application container responds to incoming requests at a very fine-grained level. The simplest possible locations configuration is one that simply passes all requests on to your application unconditionally: web:locations:\u0026#39;/\u0026#39;:passthru:trueThe above configuration forwards all requests to /* to the process started by web.commands.start. In the case of PHP containers, passthru must specify what PHP file to forward the request to, as well as the docroot under which the file lives. For example, web:locations:\u0026#39;/\u0026#39;:root:\u0026#39;web\u0026#39;passthru:\u0026#39;/app.php\u0026#39; mounts: Configuring mounts are not required, unless part of your application requires write-access. By default, Platform.sh provided a read-only filesystem for your projects so that you can be confident in the health and security of your application once it has deployed. If your application requires writable storage to function properly (i.e., saving files; mounts should not contain code) it can be defined like so: mounts:\u0026#39;web/uploads\u0026#39;:source:localsource_path:uploadsIn this case, the application will be able to write to a mount that is visible in the /app/web/uploads directory of the application container, and which has a local source at /mnt/uploads. Consult the mounts documentation for a more thorough discussion of how these attributes should be written. Note: Each language and framework may have additional attributes that you will need to include in .platform.app.yaml depending on the needs of your application. To find out what else you may need to include to configure your application, consult The Application documentation for Platform.sh: The documentation goes into far more extensive detail of which attributes can also be included for application configuration, and should be used as your primary reference. Language-specific templates for Platform.sh Projects: Compare the .platform.app.yaml file from the simple template above to other templates when writing your own. Now that you have configured your application, you will next need to handle HTTP requests to your application using the .platform/routes.yaml file. Back I\u0026#39;ve configured my application",
        "section": "Getting started",
        "subsections": "",
        "image": "",
        "url": "/gettingstarted/gettingstarted/own-code/app-configuration.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "cb7db858ee8667698ac553db1f4966c0",
        "title": "Encryption",
        "description": "",
        "text": " Data in Transit Data in transit between the World and Platform.sh is always encrypted as all of the sites and tools which Platform.sh supports and maintains require TLS or SSH to access. This includes the Platform.sh management console, Accounts site, Git repositories, Documentation, and Helpdesk. Data in transit between the World and customer applications is encrypted by default. Only SSH and HTTPS connections are generally accepted, with HTTP request redirected to HTTPS. Users may opt-out of that redirect and accept HTTP requests via routes.yaml configuration, although that is not recommended. By default HTTPS connections use an automatically generated Let’s Encrypt certificate or users may provide their own TLS certificate. Data in transit on Platform.sh controlled networks (eg. between the application and a database) may or may not be encrypted, but is nonetheless protected by private networking rules. Data at Rest All application data is encrypted at rest by default using encrypted ephemeral storage (typically using an AES-256 block cipher). Some Enterprise-Dedicated clusters do not have full encryption at rest. If you have specific audit requirements surrounding data at rest encryption please contact us .",
        "section": "Security and compliance",
        "subsections": " Data in Transit Data at Rest  ",
        "image": "",
        "url": "/security/encryption.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "6c064bc9632b648e9a498ab19bdb8aac",
        "title": "HTTP Headers",
        "description": "",
        "text": " Platform.sh adds a number of HTTP headers to both inbound and outbound messages. We do not, however, modify or block existing headers on either request or response. Request headers Platform.sh adds the following HTTP headers in the router to give the application information about the connection. These are stable and may be examined by the application as necessary. X-Forwarded-Proto: The protocol forwarded to the application, e.g. “http”, “https”. X-Client-IP: The remote IP address of the request. X-Client-SSL: Set “on” only if the client is using SSL connection, otherwise the header is not added. X-Original-Route: The route in .platform/routes.yaml which is used currently, e.g. https://{default}/. Response headers Platform.sh adds a number of response headers automatically to assist in debugging connections. These headers should be treated as a semi-private API. Do not code against them, but they may be inspected to help determine how Platform.sh handled the request to aid in debugging. X-Platform-Cache: Either HIT or MISS to indicate if the router in your cluster served the response from its own cache or if the request was passed through to the application. X-Platform-Cluster: The ID of the cluster that received the request. The cluster name is formed from the project ID and environment ID. X-Platform-Processor: The ID of the container that generated the response. The container ID is the cluster ID plus the container name. X-Platform-Router: The ID of the router that served the request. The router ID is the processor ID of the router container, specifically. Custom headers Apart from those listed above, your application is responsible for setting its own response headers. To add headers to static files, use the headers key in the application’s web locations configuration .",
        "section": "Development",
        "subsections": " Request headers Response headers Custom headers  ",
        "image": "",
        "url": "/development/headers.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "67a6e4084a2d7235abcb4663ad45f6a1",
        "title": "MariaDB/MySQL (Database service)",
        "description": "",
        "text": " Platform.sh supports both MariaDB and Oracle MySQL. While there are some differences at the application level for developers, they function nearly identically from an infrastructure point of view. See the MariaDB documentation or MySQL documentation for more information. Supported versions The service types mariadb and mysql both refer to MariaDB for compatibility reasons. The service type oracle-mysql refers to MySQL as released by Oracle, Inc. Other than the type, MySQL and MariaDB are otherwise identical and the rest of this page refers to both equally. mariadb mysql oracle-mysql 10.0 10.1 10.2 10.3 10.4 10.0 10.1 10.2 10.3 10.4 5.7 8.0 Only MariaDB is available on Dedicated environments, using Galera for replication: 10.0 Galera 10.1 Galera 10.2 Galera Note: Downgrades of MySQL or MariaDB are not supported. Both will update their own datafiles to a new version automatically but cannot downgrade them. If you want to experiment with a later version without committing to it use a non-master environment. Dedicated environments do not support any storage engine other than InnoDB. Tables created using the MyISAM storage engine on dedicated environments will not replicate between cluster nodes. Deprecated versions The following versions are available but are not receiving security updates from upstream, so their use is not recommended. They will be removed at some point in the future. mariadb mysql oracle-mysql 5.5 5.5 Relationship The format exposed in the $PLATFORM_RELATIONSHIPS environment variable : {  cluster :  rjify4yjcwxaa-master-7rqtwti ,  fragment : null,  host :  mysql.internal ,  host_mapped : false,  hostname :  nha5q7m5ik526umqw6cwrcvpvi.mysql.service._.eu-3.platformsh.site ,  ip :  169.254.4.210 ,  password :   ,  path :  main ,  port : 3306,  public : false,  query : {  is_master : true },  rel :  mysql ,  scheme :  mysql ,  service :  mysql ,  type :  mariadb:10.4 ,  username :  user  } Usage example For MariaDB your .platform/services.yaml can use the mysql service type: db:type:mysql:10.4disk:256 or the mariadb service type. db:type:mariadb:10.4disk:256 Oracle-mysql uses the oracle-mysql service type: dbmysql:type:oracle-mysql:8.0disk:256 Note that the minimum disk size for mysql/oracle-mysql is 256MB. Despite these service type differences, MariaDB and Oracle MySQL both use the mysql endpoint in their configuration. Note: You will need to use either the mariadb, mysql, or oracle-mysql type when defining the service # .platform/services.yamlservice_name:type:mariadb:versiondisk:256 and the endpoint mysql when defining the relationship # .platform.app.yamlrelationships:relationship_name:“service_name:mysql” Your service_name and relationship_name are defined by you, but we recommend making them distinct from each other. Exception: This pattern will be the case unless you explictly set additional endpoints for multiple databases, as shown in the section below. You can then use the service in a configuration file of your application with something like: Go Java Node.js PHP Python package examples import (  database/sql   fmt  _  github.com/go-sql-driver/mysql  psh  github.com/platformsh/config-reader-go/v2  sqldsn  github.com/platformsh/config-reader-go/v2/sqldsn  ) func UsageExampleMySQL() string { // Create a NewRuntimeConfig object to ease reading the Platform.sh environment variables. // You can alternatively use os.Getenv() yourself. config, err := psh.NewRuntimeConfig() checkErr(err) // The \u0026#39;database\u0026#39; relationship is generally the name of the primary SQL database of an application. // That\u0026#39;s not required, but much of our default automation code assumes it. credentials, err := config.Credentials( database ) checkErr(err) // Using the sqldsn formatted credentials package. formatted, err := sqldsn.FormattedCredentials(credentials) checkErr(err) db, err := sql.Open( mysql , formatted) checkErr(err) defer db.Close() // Force MySQL into modern mode. db.Exec( SET NAMES=utf8 ) sql_mode = \u0026#39;ANSI,STRICT_TRANS_TABLES,STRICT_ALL_TABLES, NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO, // Creating a table. sqlCreate := CREATE TABLE IF NOT EXISTS PeopleGo ( id SERIAL PRIMARY KEY, name VARCHAR(30) NOT NULL, city VARCHAR(30) NOT _, err = db.Exec(sqlCreate) checkErr(err) // Insert data. sqlInsert := INSERT INTO PeopleGo (name, city) VALUES (\u0026#39;Neil Armstrong\u0026#39;, \u0026#39;Moon\u0026#39;), (\u0026#39;Buzz Aldrin\u0026#39;, \u0026#39;Glen Ridge\u0026#39;), (\u0026#39;Sally Ride\u0026#39;, \u0026#39;La _, err = db.Exec(sqlInsert) checkErr(err) table := \u0026lt;thead\u0026gt; \u0026lt;tr\u0026gt;\u0026lt;th\u0026gt;Name\u0026lt;/th\u0026gt;\u0026lt;th\u0026gt;City\u0026lt;/th\u0026gt;\u0026lt;/tr\u0026gt; \u0026lt;/thead\u0026gt; var id int var name string var city string rows, err := db.Query( SELECT * FROM PeopleGo ) if err != nil { panic(err) } else { for rows.Next() { err = rows.Scan(\u0026amp;id, \u0026amp;name, \u0026amp;city) checkErr(err) table \u0026#43;= name, city) } table \u0026#43;= } _, err = db.Exec( DROP TABLE PeopleGo; ) checkErr(err) return table } package sh.platform.languages.sample; import sh.platform.config.Config; import sh.platform.config.MySQL; import javax.sql.DataSource; import java.sql.Connection; import java.sql.ResultSet; import java.sql.SQLException; import java.sql.Statement; import java.util.function.Supplier; public class MySQLSample implements Supplier\u0026lt;String\u0026gt; { @Override public String get() { StringBuilder logger = new StringBuilder(); // Create a new config object to ease reading the Platform.sh environment variables. // You can alternatively use getenv() yourself. Config config = new Config(); // The \u0026#39;database\u0026#39; relationship is generally the name of primary SQL database of an application. // That\u0026#39;s not required, but much of our default automation code assumes it. MySQL database = config.getCredential( database , MySQL::new); DataSource dataSource = database.get(); // Connect to the database try (Connection connection = dataSource.getConnection()) { // Creating a table. String sql =  CREATE TABLE JAVA_PEOPLE (  \u0026#43;   id INT(6) UNSIGNED AUTO_INCREMENT PRIMARY KEY,  \u0026#43;  name VARCHAR(30) NOT NULL,  \u0026#43;  city VARCHAR(30) NOT NULL) ; final Statement statement = connection.createStatement(); statement.execute(sql); // Insert data. sql =  INSERT INTO JAVA_PEOPLE (name, city) VALUES  \u0026#43;  (\u0026#39;Neil Armstrong\u0026#39;, \u0026#39;Moon\u0026#39;),  \u0026#43;  (\u0026#39;Buzz Aldrin\u0026#39;, \u0026#39;Glen Ridge\u0026#39;),  \u0026#43;  (\u0026#39;Sally Ride\u0026#39;, \u0026#39;La Jolla\u0026#39;) ; statement.execute(sql); // Show table. sql =  SELECT * FROM JAVA_PEOPLE ; final ResultSet resultSet = statement.executeQuery(sql); while (resultSet.next()) { int id = resultSet.getInt( id ); String name = resultSet.getString( name ); String city = resultSet.getString( city ); logger.append(String.format( the JAVA_PEOPLE id %d the name %s and city %s , id, name, city)); } statement.execute( DROP TABLE JAVA_PEOPLE ); return logger.toString(); } catch (SQLException exp) { throw new RuntimeException( An error when execute MySQL , exp); } } } const mysql = require(\u0026#39;mysql2/promise\u0026#39;); const config = require( platformsh-config ).config(); exports.usageExample = async function() { const credentials = config.credentials(\u0026#39;database\u0026#39;); const connection = await mysql.createConnection({ host: credentials.host, port: credentials.port, user: credentials.username, password: credentials.password, database: credentials.path, }); let sql = \u0026#39;\u0026#39;; // Creating a table. sql = `CREATE TABLE IF NOT EXISTS People ( id INT(6) UNSIGNED AUTO_INCREMENT PRIMARY KEY, name VARCHAR(30) NOT NULL, city VARCHAR(30) NOT NULL )`; await connection.query(sql); // Insert data. sql = `INSERT INTO People (name, city) VALUES (\u0026#39;Neil Armstrong\u0026#39;, \u0026#39;Moon\u0026#39;), (\u0026#39;Buzz Aldrin\u0026#39;, \u0026#39;Glen Ridge\u0026#39;), (\u0026#39;Sally Ride\u0026#39;, \u0026#39;La Jolla\u0026#39;);`; await connection.query(sql); // Show table. sql = `SELECT * FROM People`; let [rows] = await connection.query(sql); let output = \u0026#39;\u0026#39;; if (rows.length \u0026gt; 0) { output \u0026#43;=`\u0026lt;table\u0026gt; \u0026lt;thead\u0026gt; \u0026lt;tr\u0026gt;\u0026lt;th\u0026gt;Name\u0026lt;/th\u0026gt;\u0026lt;th\u0026gt;City\u0026lt;/th\u0026gt;\u0026lt;/tr\u0026gt; \u0026lt;/thead\u0026gt; \u0026lt;tbody\u0026gt;`; rows.forEach((row) =\u0026gt; { output \u0026#43;= }); output \u0026#43;= } // Drop table. sql = `DROP TABLE People`; await connection.query(sql); return output; }; \u0026lt;?php declare(strict_types=1); use // Create a new config object to ease reading the Platform.sh environment variables. // You can alternatively use getenv() yourself. $config = new Config(); // The \u0026#39;database\u0026#39; relationship is generally the name of primary SQL database of an application. // That\u0026#39;s not required, but much of our default automation code assumes it. $credentials = $config-\u0026gt;credentials(\u0026#39;database\u0026#39;); try { // Connect to the database using PDO. If using some other abstraction layer you would // inject the values from $database into whatever your abstraction layer asks for. $dsn = sprintf(\u0026#39;mysql:host=%s;port=%d;dbname=%s\u0026#39;, $credentials[\u0026#39;host\u0026#39;], $credentials[\u0026#39;port\u0026#39;], $credentials[\u0026#39;path\u0026#39;]); $conn = new $credentials[\u0026#39;username\u0026#39;], $credentials[\u0026#39;password\u0026#39;], [ // Always use Exception error mode with PDO, as it\u0026#39;s more reliable. =\u0026gt; // So we don\u0026#39;t have to mess around with cursors and unbuffered queries by default. =\u0026gt; TRUE, // Make sure MySQL returns all matched rows on update queries including // rows that actually didn\u0026#39;t have to be updated because the values didn\u0026#39;t // change. This matches common behavior among other database systems. =\u0026gt; TRUE, ]); // Creating a table. $sql =  CREATE TABLE People ( id INT(6) UNSIGNED AUTO_INCREMENT PRIMARY KEY, name VARCHAR(30) NOT NULL, city VARCHAR(30) NOT NULL ) ; $conn-\u0026gt;query($sql); // Insert data. $sql =  INSERT INTO People (name, city) VALUES (\u0026#39;Neil Armstrong\u0026#39;, \u0026#39;Moon\u0026#39;), (\u0026#39;Buzz Aldrin\u0026#39;, \u0026#39;Glen Ridge\u0026#39;), (\u0026#39;Sally Ride\u0026#39;, \u0026#39;La Jolla\u0026#39;); ; $conn-\u0026gt;query($sql); // Show table. $sql =  SELECT * FROM People ; $result = $conn-\u0026gt;query($sql); if ($result) { print \u0026lt;\u0026lt;\u0026lt;TABLE\u0026lt;table\u0026gt; \u0026lt;thead\u0026gt; \u0026lt;tr\u0026gt;\u0026lt;th\u0026gt;Name\u0026lt;/th\u0026gt;\u0026lt;th\u0026gt;City\u0026lt;/th\u0026gt;\u0026lt;/tr\u0026gt; \u0026lt;/thead\u0026gt; \u0026lt;tbody\u0026gt; TABLE; foreach ($result as $record) { $record-\u0026gt;name, $record-\u0026gt;city); } print } // Drop table $sql =  DROP TABLE People ; $conn-\u0026gt;query($sql); } catch $e) { print $e-\u0026gt;getMessage(); } import pymysql from platformshconfig import Config def usage_example(): # Create a new Config object to ease reading the Platform.sh environment variables. # You can alternatively use os.environ yourself. config = Config() # The \u0026#39;database\u0026#39; relationship is generally the name of primary SQL database of an application. # That\u0026#39;s not required, but much of our default automation code assumes it.\u0026#39; credentials = config.credentials(\u0026#39;database\u0026#39;) try: # Connect to the database using PDO. If using some other abstraction layer you would inject the values # from `database` into whatever your abstraction layer asks for. conn = pymysql.connect(host=credentials[\u0026#39;host\u0026#39;], port=credentials[\u0026#39;port\u0026#39;], database=credentials[\u0026#39;path\u0026#39;], user=credentials[\u0026#39;username\u0026#39;], password=credentials[\u0026#39;password\u0026#39;]) sql = \u0026#39;\u0026#39;\u0026#39; CREATE TABLE People ( id SERIAL PRIMARY KEY, name VARCHAR(30) NOT NULL, city VARCHAR(30) NOT NULL ) \u0026#39;\u0026#39;\u0026#39; cur = conn.cursor() cur.execute(sql) sql = \u0026#39;\u0026#39;\u0026#39; INSERT INTO People (name, city) VALUES (\u0026#39;Neil Armstrong\u0026#39;, \u0026#39;Moon\u0026#39;), (\u0026#39;Buzz Aldrin\u0026#39;, \u0026#39;Glen Ridge\u0026#39;), (\u0026#39;Sally Ride\u0026#39;, \u0026#39;La Jolla\u0026#39;); \u0026#39;\u0026#39;\u0026#39; cur.execute(sql) # Show table. sql = \u0026#39;\u0026#39;\u0026#39;SELECT * FROM People\u0026#39;\u0026#39;\u0026#39; cur.execute(sql) result = cur.fetchall() table = \u0026#39;\u0026#39;\u0026#39;\u0026lt;table\u0026gt; \u0026lt;thead\u0026gt; \u0026lt;tr\u0026gt;\u0026lt;th\u0026gt;Name\u0026lt;/th\u0026gt;\u0026lt;th\u0026gt;City\u0026lt;/th\u0026gt;\u0026lt;/tr\u0026gt; \u0026lt;/thead\u0026gt; \u0026lt;tbody\u0026gt;\u0026#39;\u0026#39;\u0026#39; if result: for record in result: table \u0026#43;= record[2]) table \u0026#43;= # Drop table sql = \u0026#39;\u0026#39;\u0026#39;DROP TABLE People\u0026#39;\u0026#39;\u0026#39; cur.execute(sql) # Close communication with the database cur.close() conn.close() return table except Exception as e: return e Note: MySQL schema names can not use system reserved namespace. (mysql, information_schema, etc) Multiple databases If you are using version 10.0 or later of this service it is possible to define multiple databases as well as multiple users with different permissions. To do so requires defining multiple endpoints. Under the configuration key of your service there are two additional keys: schemas: This is a YAML array listing the databases that should be created. If not specified, a single database named main will be created. endpoints: This is a nested YAML array defining different credentials. Each endpoint may have access to one or more schemas (databases), and may have different levels of permission on each. The valid permission levels are: ro: Using this endpoint only SELECT queries are allowed. rw: Using this endpoint SELECT queries as well INSERT/UPDATE/DELETE queries are allowed. admin: Using this endpoint all queries are allowed, including DDL queries (CREATE TABLE, DROP TABLE, etc.). Consider the following illustrative example: db:type:mariadb:10.4disk:2048configuration:schemas:- main- legacyendpoints:admin:default_schema:mainprivileges:main:adminlegacy:adminreporter:privileges:main:roimporter:default_schema:legacyprivileges:legacy:rwThis example creates a single MySQL/MariaDB service named mysqldb. That server will have two databases, main and legacy. There will be three endpoints created. The first, named admin, will have full access to both databases. The second, reporter, will have SELECT query access to the main DB but no access to legacy at all. The importer user will have SELECT/INSERT/UPDATE/DELETE access (but not DDL access) to the legacy database but no access to main. If a given endpoint has access to multiple databases you should also specify which will be listed by default in the relationships array. If one isn’t specified the path property of the relationship will be null. While that may be acceptable for an application that knows the name of the database to connect to, it would mean that automated tools such as the Platform CLI will not be able to access the database on that relationship. For that reason the default_schema property is always recommended. Once those endpoints are defined, you need to expose them to your application as a relationship. Continuing with our example, this would be a possible corresponding block from .platform.app.yaml: relationships:database: db:admin reports: db:reporter imports: db:importer This block defines three relationships, database, reports, and imports. They’ll be available in the PLATFORM_RELATIONSHIPS environment variable and all have the same structure documented above, but with different credentials. You can use those to connect to the appropriate database with the specified restrictions using whatever the SQL access tools are for your language and application. If no configuration block is specified at all, it is equivalent to the following default: configuration:schemas:- mainendpoints:mysql:default_schema:mainprivileges:main:adminIf either schemas or endpoints are defined, then no default will be applied and you must specify the full configuration. Adjusting database configuration For MariaDB 10.1 and later Oracle MySQL 8.0 and later, a select few configuration properties from the my.cnf file are available for adjustment. Packet and connection sizing This value defaults to 16 (in MB). Legal values are from 1 to 100. db:type:mariadb:10.4disk:2048configuration:properties:max_allowed_packet:64The above code will increase the maximum allowed packet size (the size of a query or response) to 64 MB. However, increasing the size of the maximum packet will also automatically decrease the max_connections value. The number of connections allowed will depend on the packet size and the memory available to the service. In most cases leaving this value at the default is recommended. Character encoding For services created prior to February 2020, the default character set and collation is latin1, which is the default in most MySQL/MariaDB. For services created after February 2020, the default character set is utf8mb4 and the default collation is utf8mb4_unicode_ci. Both values can be adjusted at the server level in services.yaml: db:type:mariadb:10.4disk:2048configuration:properties:default_charset:utf8mb4default_collation:utf8mb4_unicode_ciNote that the effect of this setting is to set the character set and collation of any tables created once those properties are set. Tables created prior to when those settings are changed will be unaffected by changes to the services.yaml configuration. However, you can change your own table’s character set and collation through ALTER TABLE commands. For example: # To change defaults when creating new tables: ALTER DATABASE main CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci; # To change defaults when creating new columns: ALTER TABLE table_name CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci; # To convert existing data: ALTER TABLE table_name CONVERT TO CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci; Consult the MySQL documentation for further details. Storage Engine We recommend using the InnoDB storage engine wherever possible. MyISAM is only properly supported in Grid environments. In dedicated cluster environments there is no replication of MyISAM tables. If MyISAM tables are inadvertently created or imported in a dedicated environment they can be converted to use the InnoDB storage engine using the following procedure: RENAME TABLE \u0026lt;existing\u0026gt; \u0026lt;old\u0026gt;; INSERT INTO \u0026lt;existing\u0026gt; SELECT * from \u0026lt;old\u0026gt;; Access your MariaDB service Assuming your MariaDB relationship is named database, the host name and port number obtained from PLATFORM_RELATIONSHIPS would be database.internal and 3306. Open an SSH session and run the MySQL command line client. mysql -h database.internal -P 3306 -u user main Outside the application container, you can use Platform CLI platform sql. Exporting data The easiest way to download all data in a MariaDB instance is with the Platform.sh CLI. If you have a single SQL database, the following command will export all data using the mysqldump command to a local file: platform db:dump If you have multiple SQL databases it will prompt you which one to export. You can also specify one by relationship name explicitly: platform db:dump --relationship database By default the file will be uncompressed. If you want to compress it, use the --gzip (-z) option: platform db:dump --gzip You can use the --stdout option to pipe the result to another command. For example, if you want to create a bzip2-compressed file, you can run: platform db:dump --stdout | bzip2 \u0026gt; dump.sql.bz2 Importing data The easiest way to load data into a database is to pipe an SQL dump through the platform sql command, like so: platform sql \u0026lt; my_database_backup.sql That will run the database backup against the SQL database on Platform.sh. That will work for any SQL file, so the usual caveats about importing an SQL dump apply (e.g., it’s best to run against an empty database). As with exporting, you can also specify a specific environment to use and a specific database relationship to use, if there are multiple. platform sql --relationship database -e master \u0026lt; my_database_backup.sql Note: Importing a database backup is a destructive operation. It will overwrite data already in your database. Taking a backup or a database export before doing so is strongly recommended. Troubleshooting MySQL lock wait timeout definer/invoker of view lack rights to use them MySQL server has gone away",
        "section": "Configure services",
        "subsections": " Supported versions  Deprecated versions   Relationship Usage example Multiple databases Adjusting database configuration  Packet and connection sizing   Character encoding Storage Engine Access your MariaDB service Exporting data Importing data Troubleshooting  ",
        "image": "",
        "url": "/configuration/services/mysql.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "312d100b69834d64014c0062f45f576e",
        "title": "SimpleSAML",
        "description": "",
        "text": " SimpleSAMLphp is a library for authenticating a PHP-based application against a SAML server, such as Shibboleth. Although Drupal has modules available to authenticate using SimpleSAML some additional setup is required. The following configuration assumes you are building Drupal 8 using Composer. If not, you will need to download the library manually and adjust some paths accordingly. Download the library and Drupal module The easiest way to download SimpleSAMLphp is via Composer. The following command will add both the Drupal module and the PHP library to your composer.json file. composer require simplesamlphp/simplesamlphp drupal/externalauth drupal/simplesamlphp_auth Once that’s run, commit both composer.json and composer.lock to your repository. Include SimpleSAML cookies in the cache key The SimpleSAML client uses additional cookies besides the Drupal session cookie that need to be whitelisted for the cache. To do so, modify your routes.yaml file for the route that points to your Drupal site and add two additional cookies to the cache.cookies line. It should end up looking approximately like this:  https://{default}/ :type:upstreamupstream: app:http cache:enabled:truecookies:[\u0026#39;/^SS?ESS/\u0026#39;,\u0026#39;/^Drupal.visitor/\u0026#39;,\u0026#39;SimpleSAMLSessionID\u0026#39;,\u0026#39;SimpleSAMLAuthToken\u0026#39;]Commit this change to the Git repository. Expose the SimpleSAML endpoint The SimpleSAML library’s www directory needs to be publicly accessible. That can be done by mapping it directly to a path in the Application configuration. Add the following block to the web.locations section of .platform.app.yaml: web:locations:\u0026#39;/simplesaml\u0026#39;:root:\u0026#39;vendor/simplesamlphp/simplesamlphp/www\u0026#39;allow:truescripts:trueindex:- index.phpThat will map all requests to example.com/simplesaml/ to the vendor/simplesamlphp/www directory, allowing static files there to be served, PHP scripts to execute, and defaulting to index.php. Create a configuration directory Your SimpleSAMLphp configuration will need to be outside of the vendor directory. The composer require will download a template configuration file to vendor/simplesamlphp/simplesamlphp/config. Rather than modifying that file in place (as it won’t be included in Git), copy the vendor/simplesamlphp/simplesamlphp/config directory to simplesamlphp/config (in your application root). It should contain two files, config.php and authsources.php. Additionally, create a simplesamlphp/metadata directory. This directory will hold your IdP definitions. Consult the SimpleSAMLphp documentation and see the examples in vendor/simplesamlphp/simplesamlphp/metadata-templates. Next, you need to tell SimpleSAMLphp where to find that directory using an environment variable. The simplest way to set that is to add the following block to your .platform.app.yaml file: variables:env:SIMPLESAMLPHP_CONFIG_DIR:/app/simplesamlphp/configCommit the whole simplesamplphp directory and .platform.app.yaml to Git. Configure SimpleSAML to use the database SimpleSAMLphp is able to store its data either on disk or in the Drupal database. Platform.sh strongly recommends using the database. Open the file simplesamlphp/config/config.php that you created earlier. It contains a number of configuration properties that you can adjust as needed. Some are best edited in-place and the file already includes ample documentation, specifically: auth.adminpassword technicalcontact_name technicalcontact_email Others are a little more involved. In the interest of simplicity we recommend simply pasting the following code snippet at the end of the file, as it will override the default values in the array. \u0026lt;?php // Set SimpleSAML to log using error_log(), which on Platform.sh will // be mapped to the /var/log/app.log file. $config[\u0026#39;logging.handler\u0026#39;] = \u0026#39;errorlog\u0026#39;; // Set SimpleSAML to use the metadata directory in Git, rather than // the empty one in the vendor directory. $config[\u0026#39;metadata.sources\u0026#39;] = [ [\u0026#39;type\u0026#39; =\u0026gt; \u0026#39;flatfile\u0026#39;, \u0026#39;directory\u0026#39; =\u0026gt; dirname(__DIR__) . \u0026#39;/metadata\u0026#39;], ]; // Setup the database connection for all parts of SimpleSAML. if (isset($_ENV[\u0026#39;PLATFORM_RELATIONSHIPS\u0026#39;])) { $relationships = json_decode(base64_decode($_ENV[\u0026#39;PLATFORM_RELATIONSHIPS\u0026#39;]), TRUE); foreach ($relationships[\u0026#39;database\u0026#39;] as $instance) { if (!empty($instance[\u0026#39;query\u0026#39;][\u0026#39;is_master\u0026#39;])) { $dsn = sprintf( %s:host=%s;dbname=%s , $instance[\u0026#39;scheme\u0026#39;], $instance[\u0026#39;host\u0026#39;], $instance[\u0026#39;path\u0026#39;] ); $config[\u0026#39;database.dsn\u0026#39;] = $dsn; $config[\u0026#39;database.username\u0026#39;] = $instance[\u0026#39;username\u0026#39;]; $config[\u0026#39;database.password\u0026#39;] = $instance[\u0026#39;password\u0026#39;]; $config[\u0026#39;store.type\u0026#39;] = \u0026#39;sql\u0026#39;; $config[\u0026#39;store.sql.dsn\u0026#39;] = $dsn; $config[\u0026#39;store.sql.username\u0026#39;] = $instance[\u0026#39;username\u0026#39;]; $config[\u0026#39;store.sql.password\u0026#39;] = $instance[\u0026#39;password\u0026#39;]; $config[\u0026#39;store.sql.prefix\u0026#39;] = \u0026#39;simplesaml\u0026#39;; } } } // Set the salt value from the Platform.sh entropy value, provided for this purpose. if (isset($_ENV[\u0026#39;PLATFORM_PROJECT_ENTROPY\u0026#39;])) { $config[\u0026#39;secretsalt\u0026#39;] = $_ENV[\u0026#39;PLATFORM_PROJECT_ENTROPY\u0026#39;]; } Generate SSL certificates (optional) Depending on your Identity Provider (IdP), you may need to generate an SSL/TLS certificate to connect to the Service Provider (SP). If so, you should generate the certificate locally following the instructions in the SimpleSAMLphp documentation . Whatever your resulting idP file is should be placed in the simplesamlphp/metadata directory. The certificate should be placed in the simplesamlphp/cert directory. (Create it if needed.) Then add the following line to your simplesamlphp/config/config.php file to tell the library where to find the certificate: $config[\u0026#39;certdir\u0026#39;] = dirname(__DIR__) . \u0026#39;/cert\u0026#39;; Deploy Commit all changes and deploy the site, then enable the simplesamlphp_auth module within Drupal (usually by enabling it locally and pushing a config change). Consult the module documentation for further information on how to configure the module itself. Note that you should not check the “Activate authentication via SimpleSAMLphp” checkbox in the module configuration until you have the rest of the configuration completed or you may be locked out of the site.",
        "section": "Getting Started",
        "subsections": " Download the library and Drupal module Include SimpleSAML cookies in the cache key Expose the SimpleSAML endpoint Create a configuration directory Configure SimpleSAML to use the database Generate SSL certificates (optional) Deploy  ",
        "image": "",
        "url": "/frameworks/drupal8/simplesaml.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "bf38b98c1aa49340988e4b6215b11ba5",
        "title": "Storage",
        "description": "",
        "text": " The built file system image that results from your build process is mounted read-only. That means it cannot be edited in production, even by accident. Many applications still require the ability to write and store files, however. For that, applications can specify one or more mount points, that is, directories that will be mounted from a writable network file system cluster. They may be mounted anywhere within the file system of your application. If the specified directory already exists the contents of it will be masked by the writable mount and inaccessible at runtime. Disk Your plan storage size specifies the maximum total space available to all applications and services. When deploying your project, the sum of all disk keys defined in .platform.app.yaml and .platform/services.yaml must be equal or less than the plan storage size. For example, if your plan storage size is 5 GB, you can assign: 2 GB to your application, 3 GB to your database 1 GB to your application, 4 GB to your database 1 GB to your application, 1 GB to your database, 3 GB to your Elasticsearch service etc. If you receive an error on git push mentioning the total disk space configured for the application and its services exceeds the plan storage size, you need to either increase the disk space reserved for your project on the project setup page or lower the storage assigned to each service and the application. The disk key is optional. If set, it defines the size of the persistent disk of the application (in MB). Its minimum value is 256 MB and a validation error will occur if you try to set it lower. Mounts The mounts key is an object whose keys are paths relative to the root of the application (that is, where the .platform.app.yaml file lives), and values are a 2-line mount definition. This section is optional: if your application doesn’t need writable local file storage, you can omit the mounts section and set disk to the minimum value of 256. Note that whether a mounted directory is web-accessible or not depends on the configuration of the web.locations block in .platform.app.yaml. Depending on the application’s needs, it’s possible to publish files on writable mounts, leave them private, or have rules for different paths and file types as desired. Basic mounts The following block defines a single writable directory, web/uploads: mounts:\u0026#39;web/uploads\u0026#39;:source:localsource_path:uploadsThe source specifies where the writable mount is. source_path specifies the subdirectory from within the source that the mount should point at. It is often easiest to have it match the name of the mount point itself but that is not required. local mounts The local source indicates that the mount point will point to a local directory on the application container. The source_path is then a subpath of that. That means they may overlap. local mounts are not shared between different application containers or workers. Be aware that the entire local space for a single app container is a common directory, and the directory is not wiped. That means if you create a mount point with a source_path of “uploads”, then write files there, then remove the mount point, the files will still exist on disk indefinitely until manually removed. Local mounts require that the disk key be set. If it is omitted there will be no storage space available at all. service mounts A service mount refers to a network-storage service, as defined in services.yaml. They function in essentially the same way as a local mount, with two important differences: The disk size of the service mount is controlled in services.yaml; it is separate from the value of the disk key in .platform.app.yaml. Multiple application containers may refer to the same service mount and share files. A service mount works like so: mounts:\u0026#39;web/uploads\u0026#39;:source:serviceservice:filessource_path:uploadsThis assumes that a network-storage service named files has already been defined. See the Network Storage page for more details and examples. Multi-instance disk mounts If you have multiple application instances defined (using both web and workers), each instance will have its own local disk mounts. That’s the case even if they are named the same, and even if there is only a single top-level mounts directive. In that case, every instance will have an identical configuration, but separate, independent file spaces. If you want to have multiple application instances share file storage, you will need to use a service mount. How do I set up both public and private file uploads? The following example sets up two file mounts. One is mounted at /private within the application container, the other at /web/uploads. The two file mounts together have a limit of 1024 MB of storage. disk:1024mounts:\u0026#39;private\u0026#39;:source:localsource_path:private\u0026#39;web/uploads\u0026#39;:source:localsource_path:uploadsThen in the web.locations block, you’d specify that the web/uploads path is accessible. For example, this fragment would specify the /web path as the docroot but provide a more locked-down access to the /web/uploads path. web:locations:\u0026#39;/\u0026#39;:# The public directory of the application relative to its root.root:\u0026#39;web\u0026#39;# The front-controller script which determines where to send# non-static requests.passthru:\u0026#39;/app.php\u0026#39;# Allow uploaded files to be served, but do not run scripts.\u0026#39;/web/uploads\u0026#39;:root:\u0026#39;web/uploads\u0026#39;expires:300sscripts:falseallow:trueSee the web locations documentation for more details. Why can’t I mount a hidden folder? Platform.sh ignores YAML keys that start with a dot. This causes a mount like .myhiddenfolder to be ignored. If you want to mount a hidden folder, you’ll have to prepend it with a /: mounts:\u0026#39;/.myhiddenfolder\u0026#39;:source:localsource_path:\u0026#39;myhiddenfolder\u0026#39;How do I setup overlapping mount paths? While not recommended it is possible to setup multiple mount points whose source paths overlap. Consider the following example: mounts:\u0026#39;private\u0026#39;:source:localsource_path:stuff\u0026#39;secret\u0026#39;:source:localsource_path:stuff/secretIn this configuration, there will be two mount points as seen from the application: ~/private and ~/secret. However, the secret mount will point to a directory that is also under the mount point for private. That is, the secret path and the private/secret path will be the exact same directory. Although this configuration won’t cause any technical issues, it may be quite confusing so is generally not recommended. Checking the size of mounts You can use standard commands such as df -ah to find the total disk usage of mounts (which are usually all on the same filesystem) and du -hs /path/to/dir to check the size of individual directories. The CLI provides a command that combines these checks: $ platform mount:size Checking disk usage for all mounts of the application \u0026#39;app\u0026#39;... \u0026#43;-------------------------\u0026#43;-----------\u0026#43;---------\u0026#43;-----------\u0026#43;-----------\u0026#43;----------\u0026#43; | Mount(s) | Size(s) | Disk | Used | Available | Capacity | \u0026#43;-------------------------\u0026#43;-----------\u0026#43;---------\u0026#43;-----------\u0026#43;-----------\u0026#43;----------\u0026#43; | private | 55.2 MiB | 1.9 GiB | 301.5 MiB | 1.6 GiB | 15.5% | | tmp | 34.1 MiB | | | | | | web/sites/default/files | 212.2 MiB | | | | | \u0026#43;-------------------------\u0026#43;-----------\u0026#43;---------\u0026#43;-----------\u0026#43;-----------\u0026#43;----------\u0026#43;",
        "section": "Configure your application",
        "subsections": " Disk Mounts Basic mounts  local mounts service mounts   Multi-instance disk mounts How do I set up both public and private file uploads? Why can\u0026rsquo;t I mount a hidden folder? How do I setup overlapping mount paths? Checking the size of mounts  ",
        "image": "",
        "url": "/configuration/app/storage.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "aebb6ca80c4742b242be1da5716a356f",
        "title": "Symfony - Getting started",
        "description": "",
        "text": " Prerequisites Composer Composer is a tool for dependency management in PHP. It allows you to declare the dependent libraries your project needs and it will install them in your project for you. Install Composer Configure your app The ideal .platform.app.yaml file will vary from project project, and you are free to customize yours as needed. It also varies somewhat from one Symfony version to the next as Symfony has evolved. See the appropriate repository below for your Symfony version. Symfony 3 Symfony 4",
        "section": "Featured frameworks",
        "subsections": " Prerequisites Configure your app  ",
        "image": "",
        "url": "/frameworks/symfony.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "796231296a641294f370d54bae5213bf",
        "title": "Using Memcached with Drupal 7.x",
        "description": "",
        "text": " Platform.sh recommends using Redis for caching with Drupal 7 over Memcached, as Redis offers better performance when dealing with larger values as Drupal tends to produce. However, Memcached is also available if desired and is fully supported. Requirements Add a Memcached service First you need to create a Memcached service. In your .platform/services.yaml file, add or uncomment the following: cacheservice:type:memcached:1.4That will create a service named cacheservice, of type memcached, specifically version 1.4. Expose the Memcached service to your application In your .platform.app.yaml file, we now need to open a connection to the new Memcached service. Under the relationships section, add the following: relationships:cache:\u0026#39;cacheservice:memcached\u0026#39;The key (left side) is the name that will be exposed to the application in the PLATFORM_RELATIONSHIPS variable . The right hand side is the name of the service we specified above (cacheservice) and the endpoint (memcached). If you named the service something different above, change cacheservice to that. Add the Memcached PHP extension You will need to enable the PHP Memcached extension. In your .platform.app.yaml file, add the following right after the type block: # Additional extensionsruntime:extensions:- memcachedAdd the Drupal module You will need to add the Memcache module to your project. If you are using a Drush Make file, add the following line to your project.make file: projects[memcache][version] = 1.6 Then commit the Note: You must commit and deploy your code before continuing, then enable the module. The memcache module must be enabled before it is configured in the settings.platformsh.php file. Configuration The Drupal Memcache module must be configured via settings.platformsh.php. Place the following at the end of settings.platformsh.php. Note the inline comments, as you may wish to customize it further. Also review the README.txt file that comes with the memcache module, as it has a more information on possible configuration options. For instance, you may want to consider using memcache for locking as well and configuring cache stampede protection. The example below is intended as a “most common case”. \u0026lt;?php if (!empty($_ENV[\u0026#39;PLATFORM_RELATIONSHIPS\u0026#39;]) \u0026amp;\u0026amp; extension_loaded(\u0026#39;memcached\u0026#39;)) { $relationships = json_decode(base64_decode($_ENV[\u0026#39;PLATFORM_RELATIONSHIPS\u0026#39;]), true); // If you named your memcached relationship something other than  cache , set that here. $relationship_name = \u0026#39;cache\u0026#39;; if (!empty($relationships[$relationship_name])) { // These lines tell Drupal to use memcached as a backend. // Comment out just these lines if you need to disable it for some reason and // fall back to the default database cache. $conf[\u0026#39;cache_backends\u0026#39;][] = \u0026#39;sites/all/modules/contrib/memcache/memcache.inc\u0026#39;; $conf[\u0026#39;cache_default_class\u0026#39;] = \u0026#39;MemCacheDrupal\u0026#39;; $conf[\u0026#39;cache_class_cache_form\u0026#39;] = \u0026#39;DrupalDatabaseCache\u0026#39;; // While we\u0026#39;re at it, use Memcache for locking, too. $conf[\u0026#39;lock_inc\u0026#39;] = \u0026#39;sites/all/modules/contrib/memcache/memcache-lock.inc\u0026#39;; foreach ($relationships[$relationship_name] as $endpoint) { $host = sprintf( %s:%d , $endpoint[\u0026#39;host\u0026#39;], $endpoint[\u0026#39;port\u0026#39;]); $conf[\u0026#39;memcache_servers\u0026#39;][$host] = \u0026#39;default\u0026#39;; } // If using a multisite configuration, adapt this line to include a site-unique // value. $conf[\u0026#39;memcache_key_prefix\u0026#39;] = $PLATFORM_ENVIRONMENT; } }",
        "section": "Getting Started",
        "subsections": " Requirements  Add a Memcached service Expose the Memcached service to your application Add the Memcached PHP extension Add the Drupal module   Configuration  ",
        "image": "",
        "url": "/frameworks/drupal7/memcached.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "9718131189e48b804813ec41091b3eda",
        "title": "Using Redis with PHP",
        "description": "",
        "text": " Redis is a popular structured key-value service, supported by Platform.sh. It’s frequently used for caching. For PHP, Redis support is provided through a PECL extension called PhpRedis . Unfortunately, the extension has been known to break its API between versions, even between minor versions. That makes it difficult for Platform.sh to bundle like other PHP extensions . Fortunately, the extension is small enough that it’s reasonable to compile as part of the build step and enable yourself. That makes it possible to install the specific version of the extension that your application code requires. All of the necessary tools to compile PHP extensions are included in our PHP containers, so cloning the source code and compiling it on each build is straightforward. That does entail a few minute additions to each build, however. Alternatively, we have written a shell script that leverages the build cache directory to only compile the extension once, and supports compiling any version of the extension. Using the Redis builder script Copy the following script into a file named install-redis.sh in your application root (as a sibling of your .platform.app.yaml file). run() { # Run the compilation process. cd $PLATFORM_CACHE_DIR || exit 1; if [ ! -f  ${PLATFORM_CACHE_DIR}/phpredis/modules/redis.so  ]; then ensure_source checkout_version  $1  compile_source fi copy_lib enable_lib } enable_lib() { # Tell PHP to enable the extension. echo  Enabling PhpRedis extension.  echo  extension=${PLATFORM_APP_DIR}/redis.so  \u0026gt;\u0026gt; $PLATFORM_APP_DIR/php.ini } copy_lib() { # Copy the compiled library to the application directory. echo  Installing PhpRedis extension.  cp $PLATFORM_CACHE_DIR/phpredis/modules/redis.so $PLATFORM_APP_DIR } checkout_version () { # Check out the specific Git tag that we want to build. git checkout  $1  } ensure_source() { # Ensure that the extension source code is available and up to date. if [ -d  phpredis  ]; then cd phpredis || exit 1; git fetch --all --prune else git clone https://github.com/phpredis/phpredis.git cd phpredis || exit 1; fi } compile_source() { # Compile the extension. phpize ./configure make } ensure_environment() { # If not running in a Platform.sh build environment, do nothing. if [ -z  ${PLATFORM_CACHE_DIR}  ]; then echo  Not running in a Platform.sh build environment. Aborting Redis installation.  exit 0; fi } ensure_arguments() { # If no version was specified, don\u0026#39;t try to guess. if [ -z $1 ]; then echo  No version of the PhpRedis extension specified. You must specify a tagged version on the command line.  exit 1; fi } ensure_environment ensure_arguments  $1  run  $1  Invoke that script from your build hook, specifying a version. Any tagged version of the library is acceptable: hooks:build:| set -ebashinstall-redis.sh5.1.1If you ever wish to change the version of PhpRedis you are using, update the build hook and clear the build cache: platform project:clear-build-cache. The new version will not be used until you clear the build cache. There is no need to declare the extension in the runtime block. That is only for pre-built extensions. What the script does Downloads the PhpRedis source code. Checks out the version specified in the build hook. Compiles the extension. Copies the resulting redis.so file to your application root. Adds a line to the php.ini file in your application root to enable the extension, creating the file if necessary. If the script does not find a $PLATFORM_CACHE_DIR directory defined, it exits silently. That means if you run the build hook locally it will have no effect.",
        "section": "PHP",
        "subsections": " Using the Redis builder script What the script does  ",
        "image": "",
        "url": "/languages/php/redis.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "1a474894f62a15152af0f6ba24ecd6ed",
        "title": "Apache Solr Search",
        "description": "",
        "text": " Using Solr with the module Apache Solr Search on Drupal 7.x This page is about configuring Solr with the module Apache Solr Search . If your project uses Search API then you should follow the instructions Search API . Requirements You will need the module Apache Solr Search If you are using a make file, you can add those lines to your project.make: projects[apachesolr][version] = 1.8 Configuration The Apache Solr Search module allows configuration to be overridden from settings.php. Just add the following to your settings.platformsh.php file: \u0026lt;?php if (isset($_ENV[\u0026#39;PLATFORM_RELATIONSHIPS\u0026#39;])) { $relationships = json_decode(base64_decode($_ENV[\u0026#39;PLATFORM_RELATIONSHIPS\u0026#39;]), TRUE); if (!empty($relationships[\u0026#39;solr\u0026#39;])) { // Override search API server settings fetched from default configuration. foreach ($relationships[\u0026#39;solr\u0026#39;] as $endpoint) { // If your Solr server\u0026#39;s machine name is not  solr , update the following line. $environment_machine_name = \u0026#39;solr\u0026#39;; $environment_url =  http://  . $endpoint[\u0026#39;host\u0026#39;] .  :  . $endpoint[\u0026#39;port\u0026#39;] .  /  . $endpoint[\u0026#39;path\u0026#39;]; $conf[\u0026#39;apachesolr_default_environment\u0026#39;] = $environment_machine_name; $conf[\u0026#39;apachesolr_environments\u0026#39;][$environment_machine_name][\u0026#39;url\u0026#39;] = $environment_url; } } } Note that the Solr server must already be defined in Drupal and ideally exported to a Feature. The most common machine name used is just solr, as above. If you used a different name adjust the code as appropriate. Relationships configuration If you did not name the relationship solr in your .platform.app.yaml file, adjust the name accordingly. Also, if you have multiple Solr cores defined the above foreach() loop will not work. Most likely you will want to name the relationships by the machine name of the Solr server they should map to and then map each one individually. The file .platform.app.yaml must have the Solr relationship enabled, such as this snippet: relationships: solr: \u0026#39;solrsearch:solr\u0026#39;",
        "section": "Getting Started",
        "subsections": " Requirements Configuration Relationships configuration  ",
        "image": "",
        "url": "/frameworks/drupal7/apachesolr-module.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "83e480245f5fa48ef5d8938983abdc11",
        "title": "Configure routes",
        "description": "",
        "text": " The final configuration file you will need to modify in your repository is the .platform/routes.yaml file, which describes how an incoming HTTP request is going to be processed by Platform.sh. . ├── .platform │ ├── routes.yaml │ └── services.yaml ├── .platform.app.yaml └── \u0026lt; application code \u0026gt; A minimal configuration .platform/routes.yaml for all languages will look very similar: # The routes of the project.## Each route describes how an incoming URL is going# to be processed by Platform.sh. https://{default}/ :type:upstreamupstream: app:http  https://www.{default}/ :type:redirectto: https://{default}/  Configuring the routes can be done using either an absolute URL or a URL template as shown in the examples above that have the form http://www.{default}, where {default} will be substituted by either your configured domain or those automatically generated by Platform.sh. If you set up a domain of example.com, the route configuration http://www.{default} will be resolved to http://www.example.com/. Your production (master) environment’s routes will be configured according to these rules, but so will each development environment that you activate. Each route can then be configured with the following properties: type: upstream: serves the application. Takes the form upstream: \u0026lt;application name\u0026gt;:http, using the application name set in your .platform.app.yaml. redirect: configures redirects from http://{default} to your application. cache: controls caching behavior of the route . redirects: controls redirect rules associated with the route. Note: Each language and framework may have additional attributes that you will need to include in .platform/routes.yaml depending on the needs of your application. To find out what else you may need to include to configure your routes, consult The Routes documentation for Platform.sh The documentation goes into far more extensive detail of which attributes can also be included for route configuration, and should be used as your primary reference. Language-specific templates for Platform.sh Projects: Compare the .platform/routes.yaml file from the simple template above to other templates when writing your own. In the next step, you will be able to see how Platform.sh leverages environment variables to make connecting your application to its services simple. Back I\u0026#39;ve configured my routes",
        "section": "Getting started",
        "subsections": "",
        "image": "",
        "url": "/gettingstarted/gettingstarted/own-code/routes-configuration.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "ed15401e7bb55e05a30363df77a8541c",
        "title": "Network Storage",
        "description": "",
        "text": " Platform.sh supports internal “storage as a service” to provide a file store that can be shared between different application containers. The network storage service enables a new kind of mount that refers to a shared service rather than to a local directory. Any application can use both local and/or service mounts, or neither. Supported versions Grid Dedicated 1.0 None available (This is a reference to a version of our network storage implementation, not to a version of a 3rd party application.) Supported regions The Network storage service is available on all regions except: eu.platform.sh us.platform.sh If you are on one of those and require the service we suggest you migrate your project to one of the newer regions (such as eu-2, us-2, ca, au, fr-1 or de-2). Define the service First, declare a new service in the services.yaml file like so: files:type:network-storage:1.0disk:256 This example creates a service named files that is of type network-storage, and gives it 256 MB of storage total. Declare the mount Second, add the following entry to your mounts list: mounts:\u0026#39;my/files\u0026#39;:source:serviceservice:filessource_path:files This block will declare a writeable mount on the application container at the path my/files, which will be provided by the files service defined above. The source_path specifies the path within the network service that the mount points to. It is often easiest to have it match the name of the mount point itself but that is not required. Note that you do not need to add a relationship to point to the files service. That is handled automatically by the system. The application container can now read from and write to the my/files path just as if it were a local writeable mount. Note: There is a small performance hit for using a network mount over a local mount. In most cases it should not be noticeable. However, high-volume sequential file creation (that is, creating a large number of small files in rapid succession) may see a more significant performance hit. If that is something your application does regularly then a local mount will be more effective. Multi-application usage If your project contains more than one application (that is, multiple directories with their own .platform.app.yaml files), they can all use the same network mounts if desired. If the source_path is the same in both .platform.app.yaml files then the files will be shared between both applications, even if the mount location is different. It is also possible to have one application mount a source_path that is a subdirectory of another application’s mount. For example: app1: mounts:\u0026#39;my/files\u0026#39;:source:serviceservice:filessource_path:files app2: mounts:\u0026#39;process\u0026#39;:source:serviceservice:filessource_path:uploads/incoming\u0026#39;done\u0026#39;:source:serviceservice:filessource_path:uploads/doneIn this example, app1 will have access to the entire uploads directory by writing to web/uploads. app2, by contrast, will have two mounts that it can write to: process and done. The process mount will refer to the same directory as the web/uploads/incoming directory does on app1, and the done mount will refer to the same directory as the web/uploads/done directory on app1. Worker instances When defining a Worker instance it is important to keep in mind what mount behavior is desired. Unless the mounts block is defined within the web and workers sections separately, a top level mounts block will apply to both instances. However, local mounts will be a separate storage area for each instance while service mounts will refer to the same file system. For example: name:apptype:php:7.2disk:1024mounts:\u0026#39;network_dir\u0026#39;:source:serviceservice:filessource_path:our_stuff\u0026#39;local_dir\u0026#39;:source:localsource_path:my_stuffweb:locations: / :root: public passthru: /index.php workers:queue:commands:start:| php worker.phpIn this case, both the web instance and the queue worker will have two mount points: network_dir and local_dir. The local_dir mount on each will be independent and not connected to each other at all, and they will each take 1024 MB of space. The network_dir mount on each will point to the same network storage space on the files service. They will both be able to read and write to it simultaneously. The amount of space it has available will depend on the disk key specified in services.yaml. How do I give my workers access to my main application’s files? The most common use case for network-storage is to allow a CMS-driven site to use a worker that has access to the same file mounts as the web-serving application. For that case, all that is needed is to set the necessary file mounts as service mounts. For example, the following .platform.app.yaml file (fragment) will keep Drupal files directories shared between web and worker instances while keeping the Drush backup directory web-only (as it has no need to be shared). (This assumes a service named files has already been defined in services.yaml.) name:\u0026#39;app\u0026#39;type:\u0026#39;php:7.2\u0026#39;relationships:database:\u0026#39;db:mysql\u0026#39;hooks:# ...web:locations:\u0026#39;/\u0026#39;:# ...disk:1024mounts:# The public and private files directories are# network mounts shared by web and workers.\u0026#39;web/sites/default/files\u0026#39;:source:serviceservice:filessource_path:files\u0026#39;private\u0026#39;:source:serviceservice:filessource_path:private# The backup, temp, and cache directories for# Drupal\u0026#39;s CLI tools don\u0026#39;t need to be shared.# It wouldn\u0026#39;t hurt anything to make them network# shares, however.\u0026#39;/.drush\u0026#39;:source:localsource_path:drush\u0026#39;tmp\u0026#39;:source:localsource_path:tmp\u0026#39;drush-backups\u0026#39;:source:localsource_path:drush-backups\u0026#39;/.console\u0026#39;:source:localsource_path:console# Crons run on the web container, so they have the# same mounts as the web container.crons:drupal:spec:\u0026#39;*/20 * * * *\u0026#39;cmd:\u0026#39;cd web ; drush core-cron\u0026#39;# The worker defined here will also have the same 6 mounts;# 2 of them will be shared with the web container,# the other 4 will be local to the worker.workers:queue:commands:start:| cd web \u0026amp;\u0026amp; drush queue-run myqueueHow can I migrate a local storage to a network storage? There is no automated way of transferring data from one storage type to another. However, the process is fundamentally “just” moving files around on disk, so it is reasonably straightforward. Suppose you have this mount configuration: mounts:web/uploads:source:localsource_path:uploadsAnd want to move that to a network storage mount. The following approximate steps will do so with a minimum of service interruption. Add a new network-storage service, named files, that has at least enough space for your existing files with some buffer. You may need to increase your plan’s disk size to accommodate it. Add a new mount to the network storage service on a non-public directory: mounts:new-uploads:source:serviceservice:filessource_path:uploads(Remember the source_path can be the same since they’re on different storage services.) Deploy these changes. Then use rsync to copy all files from the local mount to the network mount. (Be careful of the trailing /.) rsync -avz web/uploads/* new-uploads/ Reverse the mounts. That is, point the web/uploads directory to the network mount instead: mounts:web/uploads:source:serviceservice:filessource_path:uploadsold-uploads:source:localsource_path:uploadsCommit and push that. Test to make sure the network files are accessible. Cleanup. First, run another rsync just to make sure any files uploaded during the transition are not lost. (Note the command is different here.) rsync -avz old-uploads/* web/uploads/ Once you’re confident all the files are accounted for, delete the entire contents of old-uploads. If you do not, the files will remain on disk but inaccessible, just eating up disk space needlessly. Once that’s done you can remove the old-uploads mount and push again to finish the process. You are also free to reduce the disk size in the .platform.app.yaml file if desired, but make sure to leave enough for any remaining local mounts. Why do I get an invalid service type error with network storage? The network-storage service is only available on our newer regions. If you are running on the older us or eu regions and try to create a network-storage service you will receive this error. To make use of network-storage you will need to migrate to the newer us-2 or eu-2 regions. See our tutorial on how to migrate regions for more information.",
        "section": "Configure services",
        "subsections": " Supported versions Supported regions Define the service Declare the mount Multi-application usage Worker instances How do I give my workers access to my main application\u0026rsquo;s files? How can I migrate a local storage to a network storage? Why do I get an invalid service type error with network storage?  ",
        "image": "",
        "url": "/configuration/services/network-storage.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "ddbc4f599821eaf5253c34ccf4987b27",
        "title": "Protective block",
        "description": "",
        "text": " The Platform.sh service has a protective blocking feature that, under certain circumstances, restricts access to web sites with security vulnerabilities. We use this partial blocking method to prevent exploitation of known security vulnerabilities. The protective block is meant for high impact, low complexity attacks. The Platform.sh security block Outdated software often contains known vulnerabilities that can be exploited from the Internet. Sites that can be exploited are protected by Platform.sh. The system partially blocks access to these sites. How the protective block works Platform.sh maintains a database of signatures of known security vulnerabilities in open-source software that are commonly deployed on our infrastructure. The security check only analyze known vulnerabilities in open-source projects like Drupal, Symfony or WordPress. It cannot examine customizations written by Platform.sh customers. We analyze the code of your application: When you push new code to Git Regularly when new vulnerabilities are added to our database If a vulnerability deemed as critical is detected in your application, Platform.sh is going to reject the Git push. We run two types of blocks: For production websites, we run a “partial block” that allows the site to stay mostly online. Depending on the nature of the vulnerability, parts of a request, such as a query string, cookies or any additional headers, may be removed from GET requests. All other requests may be blocked entirely - this could apply to logging in, form submission or product checkout. For development websites, we run complete blocks, and the error message gives you detailed information about the vulnerability. Unblocking is automated upon resolution of the security risk. The block is removed soon after a customer applies a security upgrade and removes the vulnerability. Opting out of the protective block The protective block is there to protect you against known vulnerabilities in the software you deploy on Platform.sh . If nonetheless you want to opt out of the protective block, you simply need to specify it in your .platform.app.yaml like this: preflight:enabled:falseYou can also explicitly opt-out of some specific check like this: preflight:enabled:trueignore_rules:[ drupal:SA-CORE-2014-005 ]",
        "section": "Security and compliance",
        "subsections": " The Platform.sh security block How the protective block works Opting out of the protective block  ",
        "image": "",
        "url": "/security/protective-block.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "f686f84c271008c87faf3fe54b6cb243",
        "title": "Sending E-mail",
        "description": "",
        "text": " By default only the master environment can send emails. For the non-master environments, you can configure outgoing emails via the [management console]({{\u0026lt; relref “/administration/web/configure-environment.md#settings” \u0026gt;}}). Emails from Platform.sh are sent via a SendGrid-based SMTP proxy. Each Platform.sh project is provisioned as a SendGrid sub-account. These SendGrid sub-accounts are capped at 12k emails per month. You can use /usr/sbin/sendmail on your application container to send emails with the assigned SendGrid sub-account. Alternatively, you can use the PLATFORM_SMTP_HOST environment variable to use in your SMTP configuration.” We do not guarantee the deliverability of emails, and we do not support white-labeling them. Our SMTP proxy is intended as a zero-configuration, best effort service. If needed, you can instead use your own SMTP server or email delivery service provider. In that case, please bear in mind that TCP port 25 is blocked for security reasons; use TCP port 465 or 587 instead. Note: You may follow the SPF setup guidelines on SendGrid to improve email deliverability with our SMTP proxy. However, as we do not support white-labeling of emails, DKIM is not provided for our standard email handling (meaning that DMARC can’t be set up either). Thus, for maximum deliverability own mail host must be engaged. Enabling/disabling email Email support can be enabled/disabled per-environment. By default, it is enabled on the master environment and disabled elsewhere. That can be toggled in through the management console or via the command line, like so: platform environment:info enable_smtp true platform environment:info enable_smtp false When SMTP support is enabled the environment variable PLATFORM_SMTP_HOST will be populated with the address of the SMTP host that should be used. When SMTP support is disabled that environment variable will be empty. Note: Changing the SMTP status will not take effect immediately. You will need to issue a new build, not just a new deploy, for the changes to take effect. Sending email in PHP When you send email, you can simply use the built-in mail() function in PHP. The PHP runtime is configured to send email automatically via the assigned SendGrid sub-account. Note that the From header is required; email will not send if that header is missing. Beware of the potential security problems when using the mail() function, which arise when using user-supplied input in the fifth ($additional_parameters) argument. See the PHP mail() documentation for more information. SwiftMailer In Symfony, if you use the default SwiftMailer service, we recommend the following settings in your app/config/parameters.yaml: parameters:mailer_transport:smtpmailer_host: %env(PLATFORM_SMTP_HOST)% mailer_user:nullmailer_password:nullIf you are using a file spool facility, you will probably need to setup a read/write mount for it in .platform.app.yaml, for example: mounts:\u0026#39;app/spool\u0026#39;:source:localsource_path:spoolPorts Port 465 and 587 should be used to send email to your own external email server. Port 25 should be used to send through PLATFORM_SMTP_HOST. (this is the default in most mailers). We proxy your emails through our own smtp host, and encrypt them over port 465 before sending them through to the outside world.",
        "section": "Development",
        "subsections": " Enabling/disabling email Sending email in PHP  SwiftMailer   Ports  ",
        "image": "",
        "url": "/development/email.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "9a8da44c1d094537af4601d1bbfcfb12",
        "title": "TYPO3 - Getting started",
        "description": "",
        "text": " Prerequisites Composer Composer is a tool for dependency management in PHP. It allows you to declare the dependent libraries your project needs and it will install them in your project for you. Install Composer Configure your app The ideal .platform.app.yaml file will vary from project project, and you are free to customize yours as needed. A recommended baseline TYPO3 configuration is listed below, and can also be found in our TYPO3 template project . # This file describes an application. You can have multiple applications# in the same project.# The name of this app. Must be unique within a project.name:app# The type of the application to build.type:php:7.4build:flavor:composer# The relationships of the application with services or other applications.# The left-hand side is the name of the relationship as it will be exposed# to the application in the PLATFORM_RELATIONSHIPS variable. The right-hand# side is in the form `\u003cservice name\u003e:\u003cendpoint name\u003e`.relationships:database:'db:mysql'rediscache:'cache:redis'# The configuration of app when it is exposed to the web.web:locations:'/':# The public directory of the app, relative to its root.root:'public'passthru:'/index.php'index:- 'index.php'allow:falserules:# Allow access to common static The size of the persistent disk of the application (in MB).disk:2048# The mounts that will be performed when the package is deployed.mounts: public/typo3temp :source:localsource_path: typo3temp  public/fileadmin :source:localsource_path: fileadmin  var :source:localsource_path: var # The hooks that will be performed when the package is deployed.hooks:build:| set -e# Install the PhpRedis extensionbashinstall-redis.sh5.1.1phpvendor/bin/typo3cmsinstall:setup--install-steps-config=src/SetupConfiguration.yaml--no-interaction--skip-extension-setupphpvendor/bin/typo3cmsinstall:generatepackagestates# Enable the install tool for 60mins after deploymenttouchpublic/typo3conf/ENABLE_INSTALL_TOOL# Keep the checked-in LocalConfiguration available, but make the actual file writable later-on# by creating a symlink which will be accesible belowif[-fpublic/typo3conf/LocalConfiguration.php];thenmvpublic/typo3conf/LocalConfiguration.phppublic/typo3conf/LocalConfiguration.FromSource.phpln-sf../../var/LocalConfiguration.phppublic/typo3conf/LocalConfiguration.phpfi;# Clean up the FIRST_INSTALL file, that was createdif[-fpublic/FIRST_INSTALL];thenrmpublic/FIRST_INSTALLfi;deploy:| set -eif[!-fvar/platformsh.installed];then# copy the created LocalConfiguration into the writable locationcppublic/typo3conf/LocalConfiguration.FromSource.phpvar/LocalConfiguration.php# This creates the initial admin user with a default password.# *CHANGE THIS VALUE IMMEDIATELY AFTER on * * * * cmd: vendor/bin/typo3 scheduler:run ",
        "section": "Featured frameworks",
        "subsections": " Prerequisites Configure your app  ",
        "image": "",
        "url": "/frameworks/typo3.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "d87b913352af9d10b8d6569a95f1aaa7",
        "title": "Using Lando for local Drupal development",
        "description": "",
        "text": " Lando is a local development platform that works well with Platform.sh. Once installed locally it is a simple matter to create an approximate equivalent of your Platform.sh environment for development. If using Drupal 8 there is a drupal8 recipe available that is a good starting point for your site. Setting up the Lando environment # Download your project. platform get \u0026lt;projectId\u0026gt; cd \u0026lt;projectId\u0026gt; # Create a basic Drupal 8 Lando config file. lando init --recipe drupal8 # Commit the Lando config file to your repository. git add .lando.yml git commit -m  Add Lando configuration  You can now customize the configuration file as needed. In addition to the general recommendations for all Lando-with-Platform.sh sites the following additions are recommended for Drupal 8: # Name the application the same as in your .platform.app.yaml.name:apprecipe:drupal8config:# Enable XDebug for local development.xdebug:true# If you are providing a custom php.ini configuration for Platform.sh, specifying# the same file here will allow the one file to drive both environments.conf:php:php.iniOnce the configuration file is set, you should start Lando running: lando start # To download dependencies. lando composer install Setting up Drupal for Lando The Platform.sh environment variables are not available in Lando, but Lando provides its own alternates. To connect Drupal to the database add the following to your web/sites/default/settings.local.php file: \u0026lt;?php $databases[\u0026#39;default\u0026#39;][\u0026#39;default\u0026#39;] = [ \u0026#39;driver\u0026#39; =\u0026gt; \u0026#39;mysql\u0026#39;, \u0026#39;database\u0026#39; =\u0026gt; getenv(\u0026#39;DB_NAME\u0026#39;), \u0026#39;username\u0026#39; =\u0026gt; getenv(\u0026#39;DB_USER\u0026#39;), \u0026#39;password\u0026#39; =\u0026gt; getenv(\u0026#39;DB_PASSWORD\u0026#39;), \u0026#39;host\u0026#39; =\u0026gt; getenv(\u0026#39;DB_HOST\u0026#39;), \u0026#39;port\u0026#39; =\u0026gt; getenv(\u0026#39;DB_PORT\u0026#39;), ]; If you need to add additional services (Redis cache, Solr, Elasticsearch, etc.) see the Lando documentation . Place any additional configuration necessary for those in the settings.local.php file as well. Downloading data from Platform.sh to Lando Assuming you’re using a standard Drupal 8 configuration, the following commands will fully synchronize the SQL database and uploaded files to your local system. First, make sure the git branch you have checked out is the environment you want to synchronize from. Then run the following from the repository root: # Download a database backup and import it into Lando. platform db:dump --gzip -f database.sql.gz lando db-import database.sql.gz # Download all user files locally, skipping those already found locally. rsync -az `platform ssh --pipe`:/app/web/sites/default/files/ ./web/sites/default/files/ rsync -az `platform ssh --pipe`:/app/private/ ./private/ If you have customized the file mounts in your .platform.app.yaml file then update the rsync commands accordingly.",
        "section": "Getting Started",
        "subsections": " Setting up the Lando environment Setting up Drupal for Lando Downloading data from Platform.sh to Lando  ",
        "image": "",
        "url": "/frameworks/drupal8/lando.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "1bff9011c2335f06ca9790010536d649",
        "title": "Variables",
        "description": "",
        "text": " Platform.sh provides a number of ways to set variables , either globally or specific to a single environment. For values that should be consistent between different environments (because they’re configuring the application or runtime itself, generally) the easiest way to control them is to set them in the .platform.app.yaml file. Only prefixed variables may be set from the .platform.app.yaml file. Some prefixes have specific meaning while others are only significant to a particular application. Nested variables will be automatically converted into a nested array or list structure as appropriate to the language. For example, the following section in .platform.app.yaml will set a single variable named env:AUTHOR to the value Juan. variables:env:AUTHOR:\u0026#39;Juan\u0026#39;That will have the exact same runtime effect as setting a project variable via the CLI as follows, except it will be versioned along with the code: $ platform variable:create env:AUTHOR --level project --value Juan The variable name may itself have punctuation in it. For example, to set a Drupal 8 configuration override (assuming you’re using the recommended settings.platformsh.php file) you can do the following: variables:d8config: system.site:name : \u0026#39;My site rocks\u0026#39;This will create a Platform.sh variable, that is, an item in the $PLATFORM_VARIABLES environment variable, named d8config:system.site:name with value “My site rocks”. Complex values The value for a variable may be more than just a string; it may also be a nested structure. If the variable is in the env namespace, it will be mapped to a Unix environment variable as a JSON string. If not, it will be included in the PLATFORM_VARIABLES environment variable. For example, the following variable definitions: variables:env:BASIC: a string INGREDIENTS:- \u0026#39;peanut butter\u0026#39;- \u0026#39;jelly\u0026#39;QUANTITIES: milk :  1 liter  cookies :  1 kg stuff:STEPS:[\u0026#39;un\u0026#39;,\u0026#39;deux\u0026#39;,\u0026#39;trois\u0026#39;]COLORS:red:\u0026#39;#FF0000\u0026#39;green:\u0026#39;#00FF00\u0026#39;blue:\u0026#39;#0000FF\u0026#39;Would appear this way in various languages: PHP Python \u0026lt;?php var_dump($_ENV[\u0026#39;BASIC\u0026#39;]); // string(8)  a string  var_dump($_ENV[\u0026#39;INGREDIENTS\u0026#39;]); // string(26)  [ peanut butter ,  jelly ]  var_dump($_ENV[\u0026#39;QUANTITIES\u0026#39;]); // string(38)  { milk :  1 liter ,  cookies :  1 kg }  $variables = json_decode(base64_decode($_ENV[\u0026#39;PLATFORM_VARIABLES\u0026#39;]), TRUE); print_r($variables[\u0026#39;stuff:STEPS\u0026#39;]); /* array(3) { [0]=\u0026gt; string(2)  un  [1]=\u0026gt; string(4)  deux  [2]=\u0026gt; string(5)  trois  } */ print_r($variables[\u0026#39;stuff:COLORS\u0026#39;]); /* array(3) { [ red ]=\u0026gt; string(7)  #FF0000  [ green ]=\u0026gt; string(7)  #00FF00  [ blue ]=\u0026gt; string(7)  #0000FF  } */ import os import json import base64 print os.getenv(\u0026#39;BASIC\u0026#39;) // a string print os.getenv(\u0026#39;INGREDIENTS\u0026#39;) // [ peanut butter ,  jelly ] print os.getenv(\u0026#39;QUANTITIES\u0026#39;) // { milk :  1 liter ,  cookies :  1 kg } variables = json.loads(base64.b64decode(os.getenv(\u0026#39;PLATFORM_VARIABLES\u0026#39;)).decode(\u0026#39;utf-8\u0026#39;)) print variables[\u0026#39;stuff:STEPS\u0026#39;] // [u\u0026#39;un\u0026#39;, u\u0026#39;deux\u0026#39;, u\u0026#39;trois\u0026#39;] print variables[\u0026#39;stuff:COLORS\u0026#39;] // {u\u0026#39;blue\u0026#39;: u\u0026#39;#0000FF\u0026#39;, u\u0026#39;green\u0026#39;: u\u0026#39;#00FF00\u0026#39;, u\u0026#39;red\u0026#39;: u\u0026#39;#FF0000\u0026#39;}",
        "section": "Configure your application",
        "subsections": " Complex values  ",
        "image": "",
        "url": "/configuration/app/variables.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "af1d5514d9f13903877a1105ec5ace41",
        "title": "Accessing logs",
        "description": "",
        "text": " Logs for various tasks on an application container are available in the /var/log directory. They can be accessed on the normal shell after logging in with platform ssh. Alternatively, they may also be accessed remotely using the platform logcommand. The CLI lets you specify which log file to access (the name of the file below minus the .log extension, as well as view the entire file in a pager, only the most recent lines, and so forth. Run platform log --help for complete documentation. A number of different log files are available depending on the application container in use. Although the files in /var/log are writable, they should not be written to directly. Only write to it via standard logging mechanisms, such as your application’s logging facility. If your application has its own logging mechanism that should be written to a dedicated logs mount in your application. All log files are trimmed to 100 MB automatically. But if you need to have complete logs, you can set up cron which will upload them to third-party storage. Contextual Code made a simple and well-described example how to achieve it. access.log This is the raw access log for the nginx instance running on the application container. That is, it does not include any requests that return a redirect or cache hit from the router . app.log Any log messages generated by the application will be sent to this file. That includes language errors such as PHP Errors, Warnings, and Notices, as well as uncaught exceptions. cron.log The cron log contains the output of all recent cron executions. If there is no cron hook specified in the container configuration then this file will be absent. It also will not exist until the first time cron has run. deploy.log The deploy log contains the output of the most recent run of the deploy hook for the container. If there is no deploy hook then this file will be absent. nginx/error.log nginx startup log messages will be recorded in this file. It is rarely needed except when debugging possible nginx configuration errors. This file is not currently available using the platform log command. error.log nginx-level errors that occur once nginx has fully started will be recorded here. This will include HTTP 500 errors for missing directories, file types that are excluded based on the .platform.app.yaml file, etc. php.access.log On a PHP container, the php.access.log contains a record of all requests to the PHP service. post_deploy.log The post_deploy log contains the output of the most recent run of the post_deploy hook for the container. If there is no post_deploy hook then this file will be absent.",
        "section": "Development",
        "subsections": " access.log app.log cron.log deploy.log nginx/error.log error.log php.access.log post_deploy.log  ",
        "image": "",
        "url": "/development/logs.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "f434ce14b76096738eaa23d09a26ab65",
        "title": "Build and deploy",
        "description": "",
        "text": " The .platform.app.yaml file provides a number of ways to control how an application gets turned from a directory in Git into a running application. There are three blocks that control different parts of the process: the build flavor, dependencies, and hooks. The build process will run the build flavor, then install dependencies, then run the user-provided build hook. The deploy process will run the deploy hook. Build The build defines what happens when building the application. Its only property is flavor, which specifies a default set of build tasks to run. Flavors are language-specific. PHP (composer by default) composer will run composer --no-ansi --no-interaction install --no-progress --prefer-dist --optimize-autoloader if a composer.json file is detected. drupal will run drush make automatically in one of a few different ways. See the Drupal 7 documentation for more details. We recommend only using this build mode for Drupal 7. Node.js (default by default) default will run npm prune --userconfig .npmrc \u0026amp;\u0026amp; npm install --userconfig .npmrc if a package.json file is detected. Note that this also allows you to provide a custom .npmrc file in the root of your application (as a sibling of the .platform.app.yaml file.) In all languages you can also specify a flavor of none (which is the default for any language other than PHP and Node.js); as the name suggests it will take no action at all. That is useful when you want complete control over your build steps, such as to run a custom Composer command or use an alternate Node.js package manager. build:flavor:composerBuild dependencies It is also possible to install additional system-level dependencies as part of the build process. These can be installed before the build hook runs using the native package manager for several web-focused languages. Platform.sh supports pulling any dependencies for the following languages: PHP (via Composer ) Python 2 and 3 (via Pip ) Ruby (via Bundler ) Node.js (via NPM ) Java (via Apache Maven , Gradle , or Apache Ant ) Python dependencies Applications can have both Python 2 and Python 3 dependencies, using the version of each that is packaged with the most recent Debian distribution. The format of Python dependencies complies with PEP 394 . That is, specifying a dependency in a python or python2 block will use pip2 and Python 2, while specifying a dependency in a python3 block will use pip3 and Python 3. We suggest that you specify your dependencies with the specific version of Python you wish to use (i.e. either python2 or python3), rather than with the generic python declaration, to ensure your application will function normally in the future if Debian’s default version of Python changes. Specifying dependencies Build dependencies are independent of the eventual dependencies of your application and are available in the PATH during the build process and in the runtime environment of your application. Note that in many cases a given package can be installed either as a global dependency or as part of your application’s own dependencies. In such cases it’s up to you which one to use. You can specify those dependencies as shown below: dependencies:php:# Specify one Composer package per line.drush/drush:\u0026#39;8.0.0\u0026#39;python:# Specify one Python 2 package per line.behave:\u0026#39;*\u0026#39;python2:# Specify one Python 2 package per line.requests:\u0026#39;*\u0026#39;python3:# Specify one Python 3 package per line.numpy:\u0026#39;*\u0026#39;ruby:# Specify one Bundler package per line.sass:\u0026#39;3.4.7\u0026#39;nodejs:# Specify one NPM package per line.grunt-cli:\u0026#39;~0.1.13\u0026#39;Note that the package name format for each language is defined by the package manager used; similarly, the version constraint string will be interpreted by the package manager. Consult the appropriate package manager’s documentation for the supported formats. Hooks Platform.sh supports three “hooks”, or points in the deployment of a new version of an application that you can inject a custom script into. Each runs at a different stage of the process. Each hook is executed as a single script, so they will be considered failed only if the final command in them fails. To cause them to fail on the first failed command, add set -e to the beginning of the hook. If a build hook fails for any reason then the build is aborted and the deploy will not happen. The “home” directory for each hook is the application root. If your scripts need to be run from the doc root of your application, you will need to cd to it first; e.g.: cd web. hooks:build:| set -ecdwebcpsome_file.phpsome_other_file.phpdeploy:| update_schema.shpost_deploy:| set -eimport_new_content.shclear_cache.shThe | character tells YAML that the lines that follow should be interpreted literally as a newline-containing string rather than as multiple lines of YAML properties. Hooks are executed using the dash shell, not the bash shell used by normal SSH logins. In most cases that makes no difference but may impact some more involved scripts. Build hook The build hook is run after the build flavor (if any). At this point no services (such as a database) are available nor any persistent file mounts, as the application has not yet been deployed. Environment variables that exist only at runtime such as PLATFORM_BRANCH, PLATFORM_DOCUMENT_ROOT etc. are not available during this phase. The full list of build time and runtime variables is available on the variables page . There are three writeable directories at this time: $PLATFORM_APP_DIR - This is where your code is checked out, and is the working directory when the build hook starts. The contents of this directory after the build hook is what will be “the application” that gets deployed. (This directory is always /app, but it’s better to use the variable or rely on the working directory than to hard code that.) Most of the time, this is the only directory you use. $PLATFORM_CACHE_DIR - This directory persists between builds, but is NOT deployed as part of your application. It’s a good place for temporary build artifacts, such as downloaded .tar.gz or .zip files, that can be reused between builds. Note that it is shared by all builds on all branches, so if using the cache directory make sure your build code accounts for that. /tmp - The temp directory is also useful for writing files that are not needed in the final application, but it will be wiped between each build. There are no constraints on what can be downloaded during your build hook except for the amount of disk available at that time. Independent of the mounted disk size you have allocated for deployment, build environments (the application plus the cache directory) and therefore application images are limited to 4 GB during the build phase. If you exceed this limit you will receive a No space left on device error. It is possible to increase this limit in certain situations, but it will be necessary to open a support ticket in order to do so. Consult the Troubleshooting guide for more information on this topic. Deploy hook The deploy hook is run after the application container has been started, but before it has started accepting requests. You can access other services at this stage (MySQL, Solr, Redis, etc.). The disk where the application lives is read-only at this point. Note that the deploy hook will only run on a web instance, not on a worker instance. Be aware: The deploy hook blocks the site accepting new requests. If your deploy hook is only a few seconds then incoming requests in that time are paused and will continue when the hook completes, effectively appearing as the site just took a few extra seconds to respond. If it takes too long, however, requests cannot be held and will appear as dropped connections. Only run tasks in your deploy hook that have to be run exclusively, such as database schema updates or some types of cache clear. A post-deploy task that can safely run concurrently with new incoming requests should be run as a post_deploy hook instead. After a Git push, in addition to the log shown in the activity log, you can see the results of the deploy hook in the /var/log/deploy.log file when logged in to the environment via SSH. It contains the log of the execution of the deployment hook. For example: [2014-07-03 10:03:51.100476] Launching hook \u0026#39;cd public ; drush -y updatedb\u0026#39;. My_custom_profile 7001 Update 7001: Enable the Platform module. Do you wish to run all pending updates? (y/n): y Performed update: my_custom_profile_update_7001 \u0026#39;all\u0026#39; cache was cleared. Finished performing updates. Post-Deploy hook The post_deploy hook functions exactly the same as the deploy hook, but after the container is accepting connections. That is, it will run concurrently with normal incoming traffic. That makes it well suited to any updates that do not require exclusive database access. What is “safe” to run in a post_deploy hook vs. in a deploy hook will vary by the application. Often times content imports, some types of cache warmups, and other such tasks are good candidates for a post_deploy hook. The post_deploy hook logs to its own file in addition to the activity log, /var/log/post-deploy.log. How do I compile Sass files as part of a build? As a good example of combining dependencies and hooks, you can compile your SASS files using Grunt. Let’s assume that your application has Sass source files (Sass being a Ruby tool) in the web/styles directory. That directory also contains a package.json file for npm and Gruntfile.js for Grunt (a Node.js tool). The following blocks will download a specific version of Sass and Grunt pre-build, then during the build step will use them to install any Grunt dependencies and then run the grunt command. This assumes that your Grunt command includes the Sass compile command. dependencies:ruby:sass:\u0026#39;3.4.7\u0026#39;nodejs:grunt-cli:\u0026#39;~0.1.13\u0026#39;hooks:build:| cd web/stylesnpminstallgruntHow can I run certain commands only on certain environments? The deploy and post_deploy hooks have access to all of the same environment variables as the application does normally, which makes it possible to vary those hooks based on the environment. A common example is to enable certain modules only in non-production environments. Because the hook is simply a shell script we have full access to all shell scripting capabilities, such as if/then directives. The following Drupal example checks the $PLATFORM_BRANCH variable to see if we’re in a production environment (the master branch) or not. If so, it forces the devel module to be disabled. If not, it forces the devel module to be enabled, and also uses the drush Drupal command line tool to strip user-specific information from the database. hooks:deploy:| if [  $PLATFORM_BRANCH  = master ]; then# Use Drush to disable the Devel module on the Master environment.drushdisdevel-yelse# Use Drush to enable the Devel module on other environments.drushendevel-y# Sanitize your database and get rid of sensitive information from Master environment.drush-ysql-sanitize--sanitize-email=user_%uid@example.com--sanitize-password=custompasswordfidrush-yupdatedb",
        "section": "Configure your application",
        "subsections": " Build  PHP (composer by default) Node.js (default by default)   Build dependencies  Python dependencies Specifying dependencies   Hooks  Build hook Deploy hook Post-Deploy hook   How do I compile Sass files as part of a build? How can I run certain commands only on certain environments?  ",
        "image": "",
        "url": "/configuration/app/build.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "c98c87467c55692189a11832c170de1d",
        "title": "Compliance guidance",
        "description": "",
        "text": " Platform.sh has many PCI and SOC 2 certified customers using our services. Some requirements are the responsibility of the host and others are the responsibility of the application developer. Basic compliance questions can be handled by our support team via a ticket. For more advanced questions or walk-through of a full audit please contact your Platform.sh Account Manager. Overview Platform.sh provides a Platform as a Service (PaaS) solution which our customers may use for applications requiring PCI compliance. Note: Cardholder processing activity is discouraged. Please use a third party processor. Security \u0026amp; Compensating Controls For a list of security measures, please see our Security page . Please take note that customer environments are deployed in a read-only instance, segregated with GRE and IPSEC tunnels, which often permits compensating controls to be claimed for several PCI requirements. Because customers can use our PaaS in a variety of ways, the best approach with auditors is to focus is on “What do I, the customer, control/configure and how is it managed in a compliant manner?” Responsibility Platform.sh and customers have shared responsibility for ensuring an up to date and secure system. Compliance is ultimately the responsibility of the customer, however. Platform.sh is responsible for: Physical and Environmental controls - We use third party hosting and thus these requirements are passed through to those providers (e.g. AWS). Patch Management - Platform.sh is responsible for patching and fixing underlying system software, management software, and environment images. Configuration Management - Platform.sh maintains the configuration of its infrastructure and devices. Awareness and Training - Platform.sh trains its own employees in secure software development and management. Capacity Management - Platform.sh is responsible for capacity management of the infrastructure, such as server allocation and bandwidth management. Access Control - Platform.sh is responsible for providing access control mechanisms to customers and for vetting all Platform.sh personnel access. Backups - Platform.sh is responsible for backing up the infrastructure and management components of the system. On Platform.sh Dedicated Enterprise (only), Platform.sh will also backup application code and databases on behalf of customers. Customers are responsible for: Patch Management - Customers are responsible for maintaining and patching application code uploaded to Platform.sh, either written by them or by a third party. Configuration Management - Customers are responsible for the secure configuration of their application, including Platform.sh configuration and routes managed through YAML files. Awareness and Training - Customers are responsible for training their own employees and users in secure software practices. Capacity Management - Customers are responsible for ensuring their application containers have sufficient resources for their selected tasks. Access Control - Customers are responsible for effectively leveraging available access control mechanisms, including proper access control settings, secrets management, ssh keys management, and the use of two-factor authentication. Backups - On Platform.sh Professional customers are responsible for all application and database backups. The Platform.sh PCI Responsibility Matrix (Excel) provides guidance on shared responsibilities to achieve PCI DSS compliance using PCI DSS 3.2 as a reference. This document was prepared by Platform.sh for informational purposes only. It is provided as a courtesy to facilitate customers’ consideration and review of the Platform.sh PCI Responsibility Summary, but the spreadsheet does not replace and is separate from the Platform.sh PCI AOC (available upon request). Customers may use the spreadsheet in conjunction with the Platform.sh PCI AOC solely to facilitate understanding elements of the report. The spreadsheet does not create any warranties, representations, contractual commitments, conditions, or assurances from Platform.sh, its affiliates, vendors, or licensors. By opening the linked document you accept and agree to these Terms of Use. If you do not wish to adhere to these Terms of Use, do not open, download, save, or otherwise access the linked document.",
        "section": "Security and compliance",
        "subsections": " Overview Security \u0026amp; Compensating Controls Responsibility  ",
        "image": "",
        "url": "/security/compliance-guidance.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "9be9559b8490787614597b1a7e0c9060",
        "title": "Connect to services",
        "description": "",
        "text": " At this point you have configured your application’s services as well as its build and deploy process in your .platform/services.yaml and .platform.app.yaml files. As an example, in your .platform.app.yaml file you may have defined a relationship called database: relationships:database: mysqldb:mysql which was configured in .platform/services.yaml with mysqldb:type:mysql:10.4disk:1024 Note: If your application does not require access to services and you left .platform/services.yaml blank, feel free to proceed to the next step. In order to connect to this service and use it in your application, Platform.sh exposes its credentials in the application container within a base64-encoded JSON PLATFORM_RELATIONSHIPS environment variable. To access this variable you can install a Platform.sh configuration reader library Go Node.js Java (Gradle) Java (Maven) PHP Python go mod edit -require=github.com/platformsh/config-reader-go/v2 npm install platformsh-config --save compile group: \u0026#39;sh.platform\u0026#39;, name: \u0026#39;config\u0026#39;, version: 2.2.0\u0026#39; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;sh.platform\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;config\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.2.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; composer install platformsh/config-reader pip install platformshconfig and access the credentials of database PHP Python Node.js Java Go \u0026lt;?php use $config = new Config(); $credentials = $config-\u0026gt;credentials(\u0026#39;database\u0026#39;); from platformshconfig import Config config = Config() credentials = config.credentials(\u0026#39;database\u0026#39;) const config = require( platformsh-config ).config(); const credentials = config.credentials(\u0026#39;database\u0026#39;); import Config; Config config = new Config(); Credential cred = config.getCredential(\u0026#39;database\u0026#39;) import psh  github.com/platformsh/config-reader-go/v2  config, err := psh.NewRuntimeConfig() // Handle err credentials, err := config.Credentials( database ) // Handle err or read and decode the environment variable directly. Node.js PHP Python relationships = JSON.parse(new Buffer(process.env[\u0026#39;PLATFORM_RELATIONSHIPS\u0026#39;], \u0026#39;base64\u0026#39;).toString()); credentials = relationships[\u0026#39;database\u0026#39;]; \u0026lt;?php $relationships = json_decode(base64_decode(getenv(\u0026#39;PLATFORM_RELATIONSHIPS\u0026#39;)), TRUE); $credentials = $relationships[\u0026#39;database\u0026#39;]; import os, json, base64 relationships = json.loads(base64.b64decode(os.environ[ PLATFORM_RELATIONSHIPS ])) credentials = relationships[\u0026#39;database\u0026#39;] In either case, credentials can now be used to connect to database: {  username :  user ,  scheme :  mysql ,  service :  mysql ,  fragment : null,  ip :  169.254.197.253 ,  hostname :  czwb2d7zzunu67lh77infwkm6i.mysql.service._.eu-3.platformsh.site ,  public : false,  cluster :  rjify4yjcwxaa-master-7rqtwti ,  host :  mysql.internal ,  rel :  mysql ,  query : {  is_master : true },  path :  main ,  password :   ,  type :  mysql:10.2 ,  port : 3306 } You can find out more information about Platform.sh Config Reader libraries on GitHub: PHP Config Reader Python Config Reader Node.js Config Reader Java Config Reader Go Config Reader You can also find examples of how to connect to each of Platform.sh managed services in multiple languages in the Services Documentation . Project configured, services connected - time to commit the changes and push your repository onto your project. Back I\u0026#39;ve connected to my services",
        "section": "Getting started",
        "subsections": "",
        "image": "",
        "url": "/gettingstarted/gettingstarted/own-code/connect-services.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "453b61a907646f08dfbdb9fe1509879b",
        "title": "Multiple Drupal sites in a single Project",
        "description": "",
        "text": " Platform.sh supports running multiple applications multiple applications in the same project, and these can be two or more Drupal sites. But they would be separate Drupal instances: they will have their assets separate and live their lives apart, and it would be much better for them not to share the same database (though they could). Drupal “Multisite” and Platform.sh Platform.sh actively discourages running Drupal in “multisite” mode. Doing so eliminates many of the advantages Platform.sh offers, such as isolation and safe testing. Additionally, because of the dynamic nature of the domain names that are created for the different environments, the multisite configuration would be complex and fragile. We recommend running separate projects for separate Drupal sites, or using one of the various “single instance” options available such as Domain Access , Organic Groups , or Workbench Access . The only reason to use Drupal Multisite would be to manage a series of nearly-identical sites with separate databases. For that case we have built a template repository that uses a unified lookup key for a subdomain, database name, and file paths. Note that it will likely require modification for your specific setup and some configurations may require a different approach. In particular, this example: Defines two MySQL databases. Uses a modified settings.platformsh.php that accepts a key variable from settings.php to specify which database and file system paths to use. Extracts the the sites directory to use from the domain.",
        "section": "Getting Started",
        "subsections": " Drupal \u0026ldquo;Multisite\u0026rdquo; and Platform.sh  ",
        "image": "",
        "url": "/frameworks/drupal8/multi-site.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "7ecc1f3cb2b5e091e2684a865be23584",
        "title": "PostgreSQL (Database service)",
        "description": "",
        "text": " PostgreSQL is a high-performance, standards-compliant relational SQL database. See the PostgreSQL documentation for more information. Supported versions Grid Dedicated 9.6 10 11 12 None available Note: Upgrading to PostgreSQL 12 using the postgis extension is not currently supported. Attempting to upgrade with this extension enabled will result in a failed deployment that will require support intervention to fix. See the Upgrading to PostgreSQL 12 with postgis section below for more details. Deprecated versions The following versions are available but are not receiving security updates from upstream, so their use is not recommended. They will be removed at some point in the future. Grid Dedicated 9.3 None available Relationship The format exposed in the $PLATFORM_RELATIONSHIPS environment variable : {  cluster :  rjify4yjcwxaa-master-7rqtwti ,  host :  postgresql.internal ,  hostname :  2tcu2y75zg2ub6wzufaz2nxlcm.postgresql.service._.eu-3.platformsh.site ,  ip :  169.254.58.227 ,  password :  main ,  path :  main ,  port : 5432,  query : {  is_master : true },  rel :  postgresql ,  scheme :  pgsql ,  service :  postgresql ,  type :  postgresql:11 ,  username :  main  } Usage example In your .platform/services.yaml add: dbpostgres:type:postgresql:12disk:256 Add a relationship to the service in your .platform.app.yaml: relationships:postgresdatabase: dbpostgres:postgresql  Note: You will need to use the postgresql type when defining the service # .platform/services.yamlservice_name:type:postgresql:versiondisk:256 and the endpoint postgresql when defining the relationship # .platform.app.yamlrelationships:relationship_name:“service_name:postgresql” Your service_name and relationship_name are defined by you, but we recommend making them distinct from each other. For PHP, in your .platform.app.yaml add: runtime:extensions:- pdo_pgsqlYou can then use the service in a configuration file of your application with something like: Go Java Node.js PHP Python package examples import (  database/sql   fmt  _  github.com/lib/pq  psh  github.com/platformsh/config-reader-go/v2  libpq  github.com/platformsh/config-reader-go/v2/libpq  ) func UsageExamplePostgreSQL() string { // Create a NewRuntimeConfig object to ease reading the Platform.sh environment variables. // You can alternatively use os.Getenv() yourself. config, err := psh.NewRuntimeConfig() checkErr(err) // The \u0026#39;database\u0026#39; relationship is generally the name of the primary SQL database of an application. // It could be anything, though, as in the case here where it\u0026#39;s called  postgresql . credentials, err := config.Credentials( postgresql ) checkErr(err) // Retrieve the formatted credentials. formatted, err := libpq.FormattedCredentials(credentials) checkErr(err) // Connect. db, err := sql.Open( postgres , formatted) checkErr(err) defer db.Close() // Creating a table. sqlCreate := CREATE TABLE IF NOT EXISTS PeopleGo ( id SERIAL PRIMARY KEY, name VARCHAR(30) NOT NULL, city VARCHAR(30) NOT _, err = db.Exec(sqlCreate) checkErr(err) // Insert data. sqlInsert := INSERT INTO PeopleGo(name, city) VALUES (\u0026#39;Neil Armstrong\u0026#39;, \u0026#39;Moon\u0026#39;), (\u0026#39;Buzz Aldrin\u0026#39;, \u0026#39;Glen Ridge\u0026#39;), (\u0026#39;Sally Ride\u0026#39;, \u0026#39;La _, err = db.Exec(sqlInsert) checkErr(err) table := \u0026lt;thead\u0026gt; \u0026lt;tr\u0026gt;\u0026lt;th\u0026gt;Name\u0026lt;/th\u0026gt;\u0026lt;th\u0026gt;City\u0026lt;/th\u0026gt;\u0026lt;/tr\u0026gt; \u0026lt;/thead\u0026gt; var id int var name string var city string // Read it back. rows, err := db.Query( SELECT * FROM PeopleGo ) if err != nil { panic(err) } else { for rows.Next() { err = rows.Scan(\u0026amp;id, \u0026amp;name, \u0026amp;city) checkErr(err) table \u0026#43;= name, city) } table \u0026#43;= } _, err = db.Exec( DROP TABLE PeopleGo; ) checkErr(err) return table } package sh.platform.languages.sample; import sh.platform.config.Config; import sh.platform.config.MySQL; import sh.platform.config.PostgreSQL; import javax.sql.DataSource; import java.sql.Connection; import java.sql.ResultSet; import java.sql.SQLException; import java.sql.Statement; import java.util.function.Supplier; public class PostgreSQLSample implements Supplier\u0026lt;String\u0026gt; { @Override public String get() { StringBuilder logger = new StringBuilder(); // Create a new config object to ease reading the Platform.sh environment variables. // You can alternatively use getenv() yourself. Config config = new Config(); // The \u0026#39;database\u0026#39; relationship is generally the name of primary SQL database of an application. // It could be anything, though, as in the case here here where it\u0026#39;s called  postgresql . PostgreSQL database = config.getCredential( postgresql , PostgreSQL::new); DataSource dataSource = database.get(); // Connect to the database try (Connection connection = dataSource.getConnection()) { // Creating a table. String sql =  CREATE TABLE JAVA_FRAMEWORKS (  \u0026#43;   id SERIAL PRIMARY KEY,  \u0026#43;  name VARCHAR(30) NOT NULL) ; final Statement statement = connection.createStatement(); statement.execute(sql); // Insert data. sql =  INSERT INTO JAVA_FRAMEWORKS (name) VALUES  \u0026#43;  (\u0026#39;Spring\u0026#39;),  \u0026#43;  (\u0026#39;Jakarta EE\u0026#39;),  \u0026#43;  (\u0026#39;Eclipse JNoSQL\u0026#39;) ; statement.execute(sql); // Show table. sql =  SELECT * FROM JAVA_FRAMEWORKS ; final ResultSet resultSet = statement.executeQuery(sql); while (resultSet.next()) { int id = resultSet.getInt( id ); String name = resultSet.getString( name ); logger.append(String.format( the JAVA_FRAMEWORKS id %d the name %s  , id, name)); } statement.execute( DROP TABLE JAVA_FRAMEWORKS ); return logger.toString(); } catch (SQLException exp) { throw new RuntimeException( An error when execute PostgreSQL , exp); } } } const pg = require(\u0026#39;pg\u0026#39;); const config = require( platformsh-config ).config(); exports.usageExample = async function() { const credentials = config.credentials(\u0026#39;postgresql\u0026#39;); const client = new pg.Client({ host: credentials.host, port: credentials.port, user: credentials.username, password: credentials.password, database: credentials.path, }); client.connect(); let sql = \u0026#39;\u0026#39;; // Creating a table. sql = `CREATE TABLE IF NOT EXISTS People ( id SERIAL PRIMARY KEY, name VARCHAR(30) NOT NULL, city VARCHAR(30) NOT NULL )`; await client.query(sql); // Insert data. sql = `INSERT INTO People (name, city) VALUES (\u0026#39;Neil Armstrong\u0026#39;, \u0026#39;Moon\u0026#39;), (\u0026#39;Buzz Aldrin\u0026#39;, \u0026#39;Glen Ridge\u0026#39;), (\u0026#39;Sally Ride\u0026#39;, \u0026#39;La Jolla\u0026#39;);`; await client.query(sql); // Show table. sql = `SELECT * FROM People`; let result = await client.query(sql); let output = \u0026#39;\u0026#39;; if (result.rows.length \u0026gt; 0) { output \u0026#43;=`\u0026lt;table\u0026gt; \u0026lt;thead\u0026gt; \u0026lt;tr\u0026gt;\u0026lt;th\u0026gt;Name\u0026lt;/th\u0026gt;\u0026lt;th\u0026gt;City\u0026lt;/th\u0026gt;\u0026lt;/tr\u0026gt; \u0026lt;/thead\u0026gt; \u0026lt;tbody\u0026gt;`; result.rows.forEach((row) =\u0026gt; { output \u0026#43;= }); output \u0026#43;= } // Drop table. sql = `DROP TABLE People`; await client.query(sql); return output; }; \u0026lt;?php declare(strict_types=1); use // Create a new config object to ease reading the Platform.sh environment variables. // You can alternatively use getenv() yourself. $config = new Config(); // The \u0026#39;database\u0026#39; relationship is generally the name of primary SQL database of an application. // It could be anything, though, as in the case here here where it\u0026#39;s called  postgresql . $credentials = $config-\u0026gt;credentials(\u0026#39;postgresql\u0026#39;); try { // Connect to the database using PDO. If using some other abstraction layer you would // inject the values from $database into whatever your abstraction layer asks for. $dsn = sprintf(\u0026#39;pgsql:host=%s;port=%d;dbname=%s\u0026#39;, $credentials[\u0026#39;host\u0026#39;], $credentials[\u0026#39;port\u0026#39;], $credentials[\u0026#39;path\u0026#39;]); $conn = new $credentials[\u0026#39;username\u0026#39;], $credentials[\u0026#39;password\u0026#39;], [ // Always use Exception error mode with PDO, as it\u0026#39;s more reliable. =\u0026gt; // So we don\u0026#39;t have to mess around with cursors and unbuffered queries by default. ]); $conn-\u0026gt;query( DROP TABLE IF EXISTS People ); // Creating a table. $sql =  CREATE TABLE IF NOT EXISTS People ( id SERIAL PRIMARY KEY, name VARCHAR(30) NOT NULL, city VARCHAR(30) NOT NULL ) ; $conn-\u0026gt;query($sql); // Insert data. $sql =  INSERT INTO People (name, city) VALUES (\u0026#39;Neil Armstrong\u0026#39;, \u0026#39;Moon\u0026#39;), (\u0026#39;Buzz Aldrin\u0026#39;, \u0026#39;Glen Ridge\u0026#39;), (\u0026#39;Sally Ride\u0026#39;, \u0026#39;La Jolla\u0026#39;); ; $conn-\u0026gt;query($sql); // Show table. $sql =  SELECT * FROM People ; $result = $conn-\u0026gt;query($sql); if ($result) { print \u0026lt;\u0026lt;\u0026lt;TABLE\u0026lt;table\u0026gt; \u0026lt;thead\u0026gt; \u0026lt;tr\u0026gt;\u0026lt;th\u0026gt;Name\u0026lt;/th\u0026gt;\u0026lt;th\u0026gt;City\u0026lt;/th\u0026gt;\u0026lt;/tr\u0026gt; \u0026lt;/thead\u0026gt; \u0026lt;tbody\u0026gt; TABLE; foreach ($result as $record) { $record-\u0026gt;name, $record-\u0026gt;city); } print } // Drop table. $sql =  DROP TABLE People ; $conn-\u0026gt;query($sql); } catch $e) { print $e-\u0026gt;getMessage(); } import psycopg2 from platformshconfig import Config def usage_example(): # Create a new Config object to ease reading the Platform.sh environment variables. # You can alternatively use os.environ yourself. config = Config() # The \u0026#39;database\u0026#39; relationship is generally the name of primary SQL database of an application. # That\u0026#39;s not required, but much of our default automation code assumes it.\u0026#39; database = config.credentials(\u0026#39;postgresql\u0026#39;) try: # Connect to the database. conn_params = { \u0026#39;host\u0026#39;: database[\u0026#39;host\u0026#39;], \u0026#39;port\u0026#39;: database[\u0026#39;port\u0026#39;], \u0026#39;dbname\u0026#39;: database[\u0026#39;path\u0026#39;], \u0026#39;user\u0026#39;: database[\u0026#39;username\u0026#39;], \u0026#39;password\u0026#39;: database[\u0026#39;password\u0026#39;] } conn = psycopg2.connect(**conn_params) # Open a cursor to perform database operations. cur = conn.cursor() cur.execute( DROP TABLE IF EXISTS People ) # Creating a table. sql = \u0026#39;\u0026#39;\u0026#39; CREATE TABLE IF NOT EXISTS People ( id SERIAL PRIMARY KEY, name VARCHAR(30) NOT NULL, city VARCHAR(30) NOT NULL ) \u0026#39;\u0026#39;\u0026#39; cur.execute(sql) # Insert data. sql = \u0026#39;\u0026#39;\u0026#39; INSERT INTO People (name, city) VALUES (\u0026#39;Neil Armstrong\u0026#39;, \u0026#39;Moon\u0026#39;), (\u0026#39;Buzz Aldrin\u0026#39;, \u0026#39;Glen Ridge\u0026#39;), (\u0026#39;Sally Ride\u0026#39;, \u0026#39;La Jolla\u0026#39;); \u0026#39;\u0026#39;\u0026#39; cur.execute(sql) # Show table. sql = \u0026#39;\u0026#39;\u0026#39;SELECT * FROM People\u0026#39;\u0026#39;\u0026#39; cur.execute(sql) result = cur.fetchall() table = \u0026#39;\u0026#39;\u0026#39;\u0026lt;table\u0026gt; \u0026lt;thead\u0026gt; \u0026lt;tr\u0026gt;\u0026lt;th\u0026gt;Name\u0026lt;/th\u0026gt;\u0026lt;th\u0026gt;City\u0026lt;/th\u0026gt;\u0026lt;/tr\u0026gt; \u0026lt;/thead\u0026gt; \u0026lt;tbody\u0026gt;\u0026#39;\u0026#39;\u0026#39; if result: for record in result: table \u0026#43;= record[2]) table \u0026#43;= # Drop table sql =  DROP TABLE People  cur.execute(sql) # Close communication with the database cur.close() conn.close() return table except Exception as e: return e Exporting data The easiest way to download all data in a PostgreSQL instance is with the Platform CLI. If you have a single SQL database, the following command will export all data using the pg_dump command to a local file: platform db:dump If you have multiple SQL databases it will prompt you which one to export. You can also specify one by relationship name explicitly: platform db:dump --relationship database By default the file will be uncompressed. If you want to compress it, use the --gzip (-z) option: platform db:dump --gzip You can use the --stdout option to pipe the result to another command. For example, if you want to create a bzip2-compressed file, you can run: platform db:dump --stdout | bzip2 \u0026gt; dump.sql.bz2 Importing data The easiest way to load data into a database is to pipe an SQL dump through the platform sql command, like so: platform sql \u0026lt; my_database_backup.sql That will run the database backup against the SQL database on Platform.sh. That will work for any SQL file, so the usual caveats about importing an SQL dump apply (e.g., it’s best to run against an empty database). As with exporting, you can also specify a specific environment to use and a specific database relationship to use, if there are multiple. platform sql --relationship database -e master \u0026lt; my_database_backup.sql Note: Importing a database backup is a destructive operation. It will overwrite data already in your database. Taking a backup or a database export before doing so is strongly recommended. Extensions Platform.sh supports a number of PostgreSQL extensions. To enable them, list them under the configuration.extensions key in your services.yaml file, like so: db:type:postgresql:12disk:1025configuration:extensions:- pg_trgm- hstoreIn this case you will have pg_trgm installed, providing functions to determine the similarity of text based on trigram matching, and hstore providing a key-value store. Available extensions The following is the extensive list of supported extensions. Note that you cannot currently add custom extensions not listed here. address_standardizer - Used to parse an address into constituent elements. Generally used to support geocoding address normalization step. address_standardizer_data_us - Address Standardizer US dataset example adminpack - administrative functions for PostgreSQL autoinc - functions for autoincrementing fields bloom - bloom access method - signature file based index (requires 9.6 or higher) btree_gin - support for indexing common datatypes in GIN btree_gist - support for indexing common datatypes in GiST chkpass - data type for auto-encrypted passwords citext - data type for case-insensitive character strings cube - data type for multidimensional cubes dblink - connect to other PostgreSQL databases from within a database dict_int - text search dictionary template for integers dict_xsyn - text search dictionary template for extended synonym processing earthdistance - calculate great-circle distances on the surface of the Earth file_fdw - foreign-data wrapper for flat file access fuzzystrmatch - determine similarities and distance between strings hstore - data type for storing sets of (key, value) pairs insert_username - functions for tracking who changed a table intagg - integer aggregator and enumerator (obsolete) intarray - functions, operators, and index support for 1-D arrays of integers isn - data types for international product numbering standards lo - Large Object maintenance ltree - data type for hierarchical tree-like structures moddatetime - functions for tracking last modification time pageinspect - inspect the contents of database pages at a low level pg_buffercache - examine the shared buffer cache pg_freespacemap - examine the free space map (FSM) pg_prewarm - prewarm relation data (requires 9.6 or higher) pg_stat_statements - track execution statistics of all SQL statements executed pg_trgm - text similarity measurement and index searching based on trigrams pg_visibility - examine the visibility map (VM) and page-level visibility info (requires 9.6 or higher) pgcrypto - cryptographic functions pgrouting - pgRouting Extension (requires 9.6 or higher) pgrowlocks - show row-level locking information pgstattuple - show tuple-level statistics plpgsql - PL/pgSQL procedural language postgis - PostGIS geometry, geography, and raster spatial types and functions postgis_sfcgal - PostGIS SFCGAL functions postgis_tiger_geocoder - PostGIS tiger geocoder and reverse geocoder postgis_topology - PostGIS topology spatial types and functions postgres_fdw - foreign-data wrapper for remote PostgreSQL servers refint - functions for implementing referential integrity (obsolete) seg - data type for representing line segments or floating-point intervals sslinfo - information about SSL certificates tablefunc - functions that manipulate whole tables, including crosstab tcn - Triggered change notifications timetravel - functions for implementing time travel tsearch2 - compatibility package for pre-8.3 text search functions (obsolete, only available for 9.6 and 9.3) tsm_system_rows - TABLESAMPLE method which accepts number of rows as a limit (requires 9.6 or higher) tsm_system_time - TABLESAMPLE method which accepts time in milliseconds as a limit (requires 9.6 or higher) unaccent - text search dictionary that removes accents uuid-ossp - generate universally unique identifiers (UUIDs) xml2 - XPath querying and XSLT Note: Upgrading to PostgreSQL 12 using the postgis extension is not currently supported. Attempting to upgrade with this extension enabled will result in a failed deployment that will require support intervention to fix. See the Upgrading to PostgreSQL 12 with postgis section below for more details. Notes Could not find driver If you see this error: Fatal error: Uncaught exception \u0026#39;PDOException\u0026#39; with message \u0026#39;could not find driver\u0026#39;, this means you are missing the pdo_pgsql PHP extension. You simply need to enable it in your .platform.app.yaml (see above). Upgrading PostgreSQL 10 and later include an upgrade utility that can convert databases from previous versions to version 10 or 11. If you upgrade your service from a previous version of PostgreSQL to version 10 or above (by modifying the services.yaml file) the upgrader will run automatically. The upgrader does not work to upgrade to PostgreSQL 9 versions, so upgrades from PostgreSQL 9.3 to 9.6 are not supported. Upgrade straight to version 11 instead. Warning: Make sure you first test your migration on a separate branch. Warning: Be sure to take a backup of your master environment before you merge this change. Downgrading is not supported. If you want, for whatever reason, to downgrade you should dump to SQL, remove the service, recreate the service, and import your dump. Upgrading to PostgreSQL 12 with the postgis extension Upgrading to PostgreSQL 12 using the postgis extension is not currently supported. Attempting to upgrade with this extension enabled will result in a failed deployment that will require support intervention to fix. If you need to upgrade, you should follow the same steps recommended for performing downgrades: dump the database, remove the service, recreate the service with PostgreSQL 12, and then import the dump to that service.",
        "section": "Configure services",
        "subsections": " Supported versions  Deprecated versions   Relationship Usage example Exporting data Importing data Extensions  Available extensions   Notes  Could not find driver   Upgrading  Upgrading to PostgreSQL 12 with the postgis extension    ",
        "image": "",
        "url": "/configuration/services/postgresql.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "e59885d50d221542f252c65644027fdf",
        "title": "Using Solr with the module Search API on Drupal 7.x",
        "description": "",
        "text": " This page is about configuring Solr with the module Search API . If your project uses Apache Solr Search then you should follow the instructions Apache Solr Search . Requirements You will need to add the Search API and Search API Solr modules to your project. The Search API Override module is strongly recommended in order to allow the Solr configuration to be populated from settings.php. If you are using a make file, you can add those lines to your project.make: projects[entity][version] = 1.8 projects[search_api][version] = 1.20 projects[search_api_solr][version] = 1.11 projects[search_api_override][version] = 1.0-rc1 Configuration The Search API module includes recommended configuration files to use with Drupal. See the Solr configuration page for details of how to configure your Solr server to use the Drupal configuration files. Note that the Drupal 7 version of Search API Solr does not include configuration files for Solr 6. The Drupal 8 version of the module does, however, and should work acceptably. It can also be customized as desired. The Search API Override module (listed above) allows Search API configuration to be overridden from settings.php. Once it has been enabled, add the following to your settings.platformsh.php file: \u0026lt;?php if (isset($_ENV[\u0026#39;PLATFORM_RELATIONSHIPS\u0026#39;])) { $relationships = json_decode(base64_decode($_ENV[\u0026#39;PLATFORM_RELATIONSHIPS\u0026#39;]), TRUE); if (!empty($relationships[\u0026#39;solr\u0026#39;])) { // Override search API server settings fetched from default configuration. $conf[\u0026#39;search_api_override_mode\u0026#39;] = \u0026#39;load\u0026#39;; foreach ($relationships[\u0026#39;solr\u0026#39;] as $endpoint) { $conf[\u0026#39;search_api_override_servers\u0026#39;] = array( \u0026#39;MACHINE_NAME_OF_SOLR_SERVER\u0026#39; =\u0026gt; array( \u0026#39;options\u0026#39; =\u0026gt; array( \u0026#39;host\u0026#39; =\u0026gt; $endpoint[\u0026#39;host\u0026#39;], \u0026#39;port\u0026#39; =\u0026gt; $endpoint[\u0026#39;port\u0026#39;], \u0026#39;path\u0026#39; =\u0026gt; \u0026#39;/\u0026#39; . $endpoint[\u0026#39;path\u0026#39;], \u0026#39;http_method\u0026#39; =\u0026gt; \u0026#39;POST\u0026#39;, ), ), ); } } } Replace MACHINE_NAME_OF_SOLR_SERVER with the Drupal machine name of the server you want to override. The solr server must already be defined in Drupal and ideally exported to a Feature. Relationships configuration If you did not name the relationship solr in your .platform.app.yaml file, adjust the name accordingly. Also, if you have multiple Solr cores defined the above foreach() loop will not work. Most likely you will want to name the relationships by the machine name of the Solr server they should map to and then map each one individually. The file .platform.app.yaml must have the Solr relationship enabled, such as this snippet: relationships: solr: \u0026#39;solrsearch:solr\u0026#39;",
        "section": "Getting Started",
        "subsections": " Requirements Configuration Relationships configuration  ",
        "image": "",
        "url": "/frameworks/drupal7/search-api-module.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "faab687d8873a9ac8a02d1d2a9d26f0a",
        "title": "Wordpress",
        "description": "",
        "text": " The recommended way to deploy WordPress on Platform.sh is using Composer. The most popular and supported way to do so is with the John Bloch script. Platform.sh strongly recommends starting new WordPress projects from our WordPress Template , which is built using Composer and includes the WP-CLI by default. It also includes modifications to the configuration files necessary to connect to a database on Platform.sh automatically. Plugin compatibility Platform.sh does not blacklist any WordPress plugins. However, some plugins are known to require write access to their own file system as part of their setup process. That is not possible on Platform.sh. The file system where code lives is read-only for security reasons and cannot be written to from the application, only during the build hook . In some cases that can be worked around by copying a file as part of the build hook process. In other cases the plugin is simply incompatible with Platform.sh. If you find a plugin that tries to write to its own directory we recommend filing an issue with that plugin, as such behavior should be viewed as a security bug.",
        "section": "Featured frameworks",
        "subsections": " Plugin compatibility  ",
        "image": "",
        "url": "/frameworks/wordpress.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "cfba97e470c8fcf1b79b618ed376c603",
        "title": "Build, Deploy, Done!",
        "description": "",
        "text": " With your configuration files complete, all that’s left is to commit the changes and push to Platform.sh. Commit and push Run the commands git add . git commit -m  Add config files.  git push -u platform master Platform.sh will detect the presence of your configuration files and use them to build the application. Verify When the build is completed, you can verify the deployment by typing the command platform url This will return a list of your routes. Pick the primary route 0 and click Enter, which will open your application in a browser window. Alternatively, you can also log back into the management console in your new project. Select the Master environment in the Environments list and click the link below the Overview box on the left side of the page. That’s it! Using the Platform.sh CLI and a few properly configured files, pushing your application to run on Platform.sh takes only a few minutes. Now that your code is on Platform.sh, check out some of the Next Steps to get started developing. Back I\u0026#39;ve deployed my application",
        "section": "Getting started",
        "subsections": "",
        "image": "",
        "url": "/gettingstarted/gettingstarted/own-code/push-project.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "19ddeafc0f577a4cd1f561813af16b90",
        "title": "Drupal Frequently Asked Questions (FAQ)",
        "description": "",
        "text": " How can I import configuration on production? If you don’t want to do so manually, include the following lines in your deploy hook in .platformsh.app.yaml: drush-yupdatedbdrush-yconfig-importThat will automatically run update.php and import configuration on every new deploy. The above configuration is included by default if you used our Drupal 8 example repository or created a project through the management console. I’m getting a PDO Exception ‘MySQL server has gone away’ Normally, this means there is a problem with the MySQL server container and you may need to increase the storage available to MySQL to resolve the issue. Ballooning MySQL storage can be caused by a number of items: A large number of watchdog entries being captured. Fix the errors being generated or disable database logging. Cron should run at regular intervals to ensure cache tables get cleared out. Why do I get “MySQL cannot connect to the database server”? If you are having a problem connecting to the database server, you will need force a re-deployment of the database container. To do so, you can edit the service definition to add or remove a small amount of storage and then push. Can I use the name of the session cookie for caching? For Drupal sites, the name of the session cookie is based on a hash of the domain name. This means that it will actually be consistent for a specific website and can safely be used as a fixed value.",
        "section": "Getting Started",
        "subsections": " How can I import configuration on production? I\u0026rsquo;m getting a PDO Exception \u0026lsquo;MySQL server has gone away\u0026rsquo; Why do I get \u0026ldquo;MySQL cannot connect to the database server\u0026rdquo;? Can I use the name of the session cookie for caching?  ",
        "image": "",
        "url": "/frameworks/drupal8/faq.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "cc7908825606a195061bc05118b5bf6a",
        "title": "RabbitMQ (Message queue service)",
        "description": "",
        "text": " RabbitMQ is an open source message broker software (sometimes called message-oriented middleware) that implements the Advanced Message Queuing Protocol (AMQP). See the RabbitMQ documentation for more information.” Supported versions Grid Dedicated 3.5 3.6 3.7 3.8 None available Relationship The format exposed in the $PLATFORM_RELATIONSHIPS environment variable : {  cluster :  rjify4yjcwxaa-master-7rqtwti ,  host :  rabbitmq.internal ,  hostname :  cefpddpigx4xs4alwihkji65fe.rabbitmq.service._.eu-3.platformsh.site ,  ip :  169.254.178.95 ,  password :  guest ,  port : 5672,  rel :  rabbitmq ,  scheme :  amqp ,  service :  rabbitmq ,  type :  rabbitmq:3.7 ,  username :  guest  } Usage example In your .platform/services.yaml: queuerabbit:type:rabbitmq:3.8disk:256 In your .platform.app.yaml: relationships:rabbitmqqueue: queuerabbit:rabbitmq  Note: You will need to use the rabbitmq type when defining the service # .platform/services.yamlservice_name:type:rabbitmq:versiondisk:256 and the endpoint rabbitmq when defining the relationship # .platform.app.yamlrelationships:relationship_name:“service_name:rabbitmq” Your service_name and relationship_name are defined by you, but we recommend making them distinct from each other. You can then use the service in a configuration file of your application with something like: Go Java PHP Python package examples import (  fmt  psh  github.com/platformsh/config-reader-go/v2  amqpPsh  github.com/platformsh/config-reader-go/v2/amqp   github.com/streadway/amqp   sync  ) func UsageExampleRabbitMQ() string { // Create a NewRuntimeConfig object to ease reading the Platform.sh environment variables. // You can alternatively use os.Getenv() yourself. config, err := psh.NewRuntimeConfig() checkErr(err) // Get the credentials to connect to RabbitMQ. credentials, err := config.Credentials( rabbitmq ) checkErr(err) // Use the amqp formatted credentials package. formatted, err := amqpPsh.FormattedCredentials(credentials) checkErr(err) // Connect to the RabbitMQ server. connection, err := amqp.Dial(formatted) checkErr(err) defer connection.Close() // Make a channel. channel, err := connection.Channel() checkErr(err) defer channel.Close() // Create a queue. q, err := channel.QueueDeclare(  deploy_days , // name false, // durable false, // delete when unused false, // exclusive false, // no-wait nil, // arguments ) body :=  Friday  msg := fmt.Sprintf( Deploying on %s , body) // Publish a message. err = channel.Publish(   , // exchange q.Name, // routing key false, // mandatory false, // immediate amqp.Publishing{ ContentType:  text/plain , Body: []byte(msg), }) checkErr(err) outputMSG := fmt.Sprintf( [x] Sent \u0026#39;%s\u0026#39; \u0026lt;br\u0026gt; , body) // Consume the message. msgs, err := channel.Consume( q.Name, // queue   , // consumer true, // auto-ack false, // exclusive false, // no-local false, // no-wait nil, // args ) checkErr(err) var received string var wg sync.WaitGroup wg.Add(1) go func() { for d := range msgs { received = fmt.Sprintf( [x] Received message: \u0026#39;%s\u0026#39; \u0026lt;br\u0026gt; , d.Body) wg.Done() } }() wg.Wait() outputMSG \u0026#43;= received return outputMSG } package sh.platform.languages.sample; import sh.platform.config.Config; import sh.platform.config.RabbitMQ; import javax.jms.Connection; import javax.jms.ConnectionFactory; import javax.jms.MessageConsumer; import javax.jms.MessageProducer; import javax.jms.Queue; import javax.jms.Session; import javax.jms.TextMessage; import java.util.function.Supplier; public class RabbitMQSample implements Supplier\u0026lt;String\u0026gt; { @Override public String get() { StringBuilder logger = new StringBuilder(); // Create a new config object to ease reading the Platform.sh environment variables. // You can alternatively use getenv() yourself. Config config = new Config(); try { // Get the credentials to connect to the RabbitMQ service. final RabbitMQ credential = config.getCredential( rabbitmq , RabbitMQ::new); final ConnectionFactory connectionFactory = credential.get(); // Connect to the RabbitMQ server. final Connection connection = connectionFactory.createConnection(); connection.start(); final Session session = connection.createSession(false, Session.AUTO_ACKNOWLEDGE); Queue queue = session.createQueue( cloud ); MessageConsumer consumer = session.createConsumer(queue); // Sending a message into the queue. TextMessage textMessage = session.createTextMessage( Platform.sh ); textMessage.setJMSReplyTo(queue); MessageProducer producer = session.createProducer(queue); producer.send(textMessage); // Receive the message. TextMessage replyMsg = (TextMessage) consumer.receive(100); logger.append( Message:  ).append(replyMsg.getText()); // close connections. producer.close(); consumer.close(); session.close(); connection.close(); return logger.toString(); } catch (Exception exp) { throw new RuntimeException( An error when execute RabbitMQ , exp); } } } \u0026lt;?php declare(strict_types=1); use use use // Create a new config object to ease reading the Platform.sh environment variables. // You can alternatively use getenv() yourself. $config = new Config(); // Get the credentials to connect to the RabbitMQ service. $credentials = $config-\u0026gt;credentials(\u0026#39;rabbitmq\u0026#39;); try { $queueName = \u0026#39;deploy_days\u0026#39;; // Connect to the RabbitMQ server. $connection = new AMQPStreamConnection($credentials[\u0026#39;host\u0026#39;], $credentials[\u0026#39;port\u0026#39;], $credentials[\u0026#39;username\u0026#39;], $credentials[\u0026#39;password\u0026#39;]); $channel = $connection-\u0026gt;channel(); $channel-\u0026gt;queue_declare($queueName, false, false, false, false); $msg = new AMQPMessage(\u0026#39;Friday\u0026#39;); $channel-\u0026gt;basic_publish($msg, \u0026#39;\u0026#39;, \u0026#39;hello\u0026#39;); echo  [x] Sent // In a real application you\u0026#39;t put the following in a separate script in a loop. $callback = function ($msg) { printf( [x] Deploying on %s\u0026lt;br $msg-\u0026gt;body); }; $channel-\u0026gt;basic_consume($queueName, \u0026#39;\u0026#39;, false, true, false, false, $callback); // This blocks on waiting for an item from the queue, so comment it out in this demo script. //$channel-\u0026gt;wait(); $channel-\u0026gt;close(); $connection-\u0026gt;close(); } catch (Exception $e) { print $e-\u0026gt;getMessage(); } import pika from platformshconfig import Config def usage_example(): # Create a new Config object to ease reading the Platform.sh environment variables. # You can alternatively use os.environ yourself. config = Config() # Get the credentials to connect to the RabbitMQ service. credentials = config.credentials(\u0026#39;rabbitmq\u0026#39;) try: # Connect to the RabbitMQ server creds = pika.PlainCredentials(credentials[\u0026#39;username\u0026#39;], credentials[\u0026#39;password\u0026#39;]) parameters = pika.ConnectionParameters(credentials[\u0026#39;host\u0026#39;], credentials[\u0026#39;port\u0026#39;], credentials=creds) connection = pika.BlockingConnection(parameters) channel = connection.channel() # Check to make sure that the recipient queue exists channel.queue_declare(queue=\u0026#39;deploy_days\u0026#39;) # Try sending a message over the channel channel.basic_publish(exchange=\u0026#39;\u0026#39;, routing_key=\u0026#39;deploy_days\u0026#39;, body=\u0026#39;Friday!\u0026#39;) # Receive the message def callback(ch, method, properties, body): print(  [x] Received {} .format(body)) # Tell RabbitMQ that this particular function should receive messages from our \u0026#39;hello\u0026#39; queue channel.basic_consume(\u0026#39;deploy_days\u0026#39;, callback, auto_ack=False) # This blocks on waiting for an item from the queue, so comment it out in this demo script. # print(\u0026#39; [*] Waiting for messages. To exit press CTRL\u0026#43;C\u0026#39;) # channel.start_consuming() connection.close() return   [x] Sent \u0026#39;Friday!\u0026#39;\u0026lt;br/\u0026gt;  except Exception as e: return e (The specific way to inject configuration into your application will vary. Consult your application or framework’s documentation.) Connecting to RabbitMQ From your local development environment For debugging purposes, it’s sometimes useful to be able to directly connect to a service instance. You can do this using SSH tunneling. To open a tunnel, log into your application container like usual, but with an extra flag to enable local port forwarding: ssh -L 5672:rabbitmqqueue.internal:5672 \u0026lt;projectid\u0026gt;-\u0026lt;branch_ID\u0026gt;@ssh.eu.platform.sh Within that SSH session, use the following command to pretty-print your relationships. This lets you see which username and password to use, and you can double check that the remote service’s port is 5672. php -r \u0026#39;print_r(json_decode(base64_decode($_ENV[ PLATFORM_RELATIONSHIPS ])));\u0026#39; If your service is running on a different port, you can re-open your SSH session with the correct port by modifying your -L flag: -L 5672:rabbitmqqueue.internal:\u0026lt;remote port\u0026gt;. Finally, while the session is open, you can launch a RabbitMQ client of your choice from your local workstation, configured to connect to localhost:5672 using the username and password you found in the relationship variable. Access the management plugin (Web UI) In case you want to access the browser-based UI, you have to use an SSH tunnel. To open a tunnel, log into your application container like usual, but with an extra flag to enable local port forwarding: ssh -L 15672:rabbitmqqueue.internal:15672 \u0026lt;projectid\u0026gt;-\u0026lt;branch_ID\u0026gt;@ssh.eu.platform.sh After you successfully established a connection, you should be able to open http://localhost:15672 in your browser. You’ll find the credentials like mentioned above. From the application container The application container currently doesn’t include any useful utilities to connect to RabbitMQ with. However, you can install your own by adding a client as a dependency in your .platform.app.yaml file. For example, you can use amqp-utils by adding this: dependencies:ruby:amqp-utils: 0.5.1 Then, when you SSH into your container, you can simply type any amqp- command available to manage your queues. Configuration Virtual hosts You can configure additional virtual hosts to a RabbitMQ service, which can be useful for separating resources, such as exchanges, queues, and bindings, to their own namespace. In your .platform/services.yaml file define the names of the virtual hosts under the configuration.vhosts attribute: rabbitmq:type:rabbitmq:3.8disk:512configuration:vhosts:- foo- bar",
        "section": "Configure services",
        "subsections": " Supported versions Relationship Usage example Connecting to RabbitMQ  From your local development environment Access the management plugin (Web UI) From the application container   Configuration  Virtual hosts    ",
        "image": "",
        "url": "/configuration/services/rabbitmq.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "6443a42b289ac42d9412648e0a5068e5",
        "title": "SimpleSAML",
        "description": "",
        "text": " SimpleSAMLphp is a library for authenticating a PHP-based application against a SAML server, such as Shibboleth. Although Drupal has modules available to authenticate using SimpleSAML some additional setup is required. The following setup assumes you’re using the drupal build flavor and building your site with Drush Make. If not, you may need to adjust some paths in the configuration but the basics are the same. Download the library First, download the 3rd party SimpleSAMLphp library . When you unpack the tar.gz file it will contain a directory named simplesamplephp-???, where the ??? is the version number of the library. Place that directory at the root of your application, as a sibling of your .platform.app.yaml file, named simplesamplephp. (The directory name doesn’t really matter but removing the version number means that it won’t change in future updates.) The drupal build flavor will move that directory to the public/sites/default/ directory during build. The rest of the configuration is based on that behavior. Include SimpleSAML cookies in the cache key The SimpleSAML client uses additional cookies besides the Drupal session cookie that need to be whitelisted for the cache. To do so, modify your routes.yaml file for the route that points to your Drupal site and add two additional cookies to the cache.cookies line. It should end up looking approximately like this:  https://{default}/ :type:upstreamupstream: app:http cache:enabled:truecookies:[\u0026#39;/^SS?ESS/\u0026#39;,\u0026#39;/^Drupal.visitor/\u0026#39;,\u0026#39;SimpleSAMLSessionID\u0026#39;,\u0026#39;SimpleSAMLAuthToken\u0026#39;]Commit this change to the Git repository. Expose the SimpleSAML endpoint The SimpleSAML library’s www directory needs to be publicly accessible. That can be done by mapping it directly to a path in the Application configuration. Add the following block to the web.locations section of .platform.app.yaml: web:locations:\u0026#39;/simplesaml\u0026#39;:root:\u0026#39;public/sites/default/simplesamlphp/www\u0026#39;allow:truescripts:trueindex:- index.phpThat will map all requests to example.com/simplesaml/ to the simplesamlphp/www directory, allowing static files there to be served, PHP scripts to execute, and defaulting to index.php. Install the simpleSAMLphp Authentication module You will need to install the simpleSAMLphp Authentication module. If using Drush Make then the easiest way to do so is simply to add the following line to your project.make file: projects[simplesamlphp_auth][version] = 2.0-alpha2 (Adjust the version to whatever is current.) Much of the module configuration will depend on your Identity Provider (IdP). However, the module also need to know the location of your simplesamlphp_auth module. The easiest way to set it is to include the following at the end of your settings.platformsh.php file: \u0026lt;?php // Set the path for the SimpleSAMLphp library dynamically. $conf[\u0026#39;simplesamlphp_auth_installdir\u0026#39;] = __DIR__ . \u0026#39;/simplesamlphp\u0026#39;; Deploy the site and enable the simplesamlphp_auth module. Consult the module documentation for further information on how to configure the module itself. Note that you should not check the “Activate authentication via SimpleSAMLphp” checkbox in the module configuration until you have the rest of the configuration completed or you may be locked out of the site. Configure SimpleSAML to use the database SimpleSAMLphp is able to store its data either on disk or in the Drupal database. Platform.sh strongly recommends using the database. Open the file simplesamlphp/config/config.php. It contains a number of configuration properties that you can adjust as needed. Some are best edited in-place and the file already includes ample documentation, specifically: auth.adminpassword technicalcontact_name technicalcontact_email Others are a little more involved. In the interest of simplicity we recommend simply pasting the following code snippet at the end of the file, as it will override the default values in the array. \u0026lt;?php // Set SimpleSAML to log using error_log(), which on Platform.sh will // be mapped to the /var/log/app.log file. $config[\u0026#39;logging.handler\u0026#39;] = \u0026#39;errorlog\u0026#39;; // Set SimpleSAML to use the metadata directory in Git, rather than // the empty one in the vendor directory. $config[\u0026#39;metadata.sources\u0026#39;] = [ [\u0026#39;type\u0026#39; =\u0026gt; \u0026#39;flatfile\u0026#39;, \u0026#39;directory\u0026#39; =\u0026gt; dirname(__DIR__) . \u0026#39;/metadata\u0026#39;], ]; // Setup the database connection for all parts of SimpleSAML. if (isset($_ENV[\u0026#39;PLATFORM_RELATIONSHIPS\u0026#39;])) { $relationships = json_decode(base64_decode($_ENV[\u0026#39;PLATFORM_RELATIONSHIPS\u0026#39;]), TRUE); foreach ($relationships[\u0026#39;database\u0026#39;] as $instance) { if (!empty($instance[\u0026#39;query\u0026#39;][\u0026#39;is_master\u0026#39;])) { $dsn = sprintf( %s:host=%s;dbname=%s , $instance[\u0026#39;scheme\u0026#39;], $instance[\u0026#39;host\u0026#39;], $instance[\u0026#39;path\u0026#39;] ); $config[\u0026#39;database.dsn\u0026#39;] = $dsn; $config[\u0026#39;database.username\u0026#39;] = $instance[\u0026#39;username\u0026#39;]; $config[\u0026#39;database.password\u0026#39;] = $instance[\u0026#39;password\u0026#39;]; $config[\u0026#39;store.type\u0026#39;] = \u0026#39;sql\u0026#39;; $config[\u0026#39;store.sql.dsn\u0026#39;] = $dsn; $config[\u0026#39;store.sql.username\u0026#39;] = $instance[\u0026#39;username\u0026#39;]; $config[\u0026#39;store.sql.password\u0026#39;] = $instance[\u0026#39;password\u0026#39;]; $config[\u0026#39;store.sql.prefix\u0026#39;] = \u0026#39;simplesaml\u0026#39;; } } } // Set the salt value from the Platform.sh entropy value, provided for this purpose. if (isset($_ENV[\u0026#39;PLATFORM_PROJECT_ENTROPY\u0026#39;])) { $config[\u0026#39;secretsalt\u0026#39;] = $_ENV[\u0026#39;PLATFORM_PROJECT_ENTROPY\u0026#39;]; } Generate SSL certs (optional) You may need to generate an SSL/TLS certificate, depending on your Identity Provider (IdP). If so, you should generate the certificate locally following the instructions in the SimpleSAMLphp documentation . Your resulting IdP file should be placed in the simplesamlphp/metadata directory. The certificate should be placed in the simplesamlphp/cert directory. (Create it if needed.) Then add the following line to your simplesamlphp/config/config.php file to tell the library where to find the certificate: \u0026lt;?php $config[\u0026#39;certdir\u0026#39;] = dirname(__DIR__) . \u0026#39;/cert\u0026#39;; Deploy Commit all changes and deploy the site, then enable the simplesamlphp_auth module within Drupal. Consult the module documentation for further information on how to configure the module itself. Note that you should not check the “Activate authentication via SimpleSAMLphp” checkbox in the module configuration until you have the rest of the configuration completed or you may be locked out of the site. Recovering from a locked site If SimpleSAML is misconfigured it is possible to find yourself locked out of the site, as it will try to authenticate against a SimpleSAML server, fail, and then disallow other logins. If that happens, the easiest way to recover it is to disable the SimpleSAML login. That can be done with the following command: platform ssh  cd public \u0026amp;\u0026amp; drush vset simplesamlphp_auth_activate 0  Alternatively you could log into the server and run the drush command there yourself. If that doesn’t work it is likely that the configuration is “pinned” using Features or via settings.php. Instead disable the module entirely, then remove the “pin” and re-enable it. platform ssh  cd public \u0026amp;\u0026amp; drush pm-disable simplesamlphp_auth -y ",
        "section": "Getting Started",
        "subsections": " Download the library Include SimpleSAML cookies in the cache key Expose the SimpleSAML endpoint Install the simpleSAMLphp Authentication module Configure SimpleSAML to use the database Generate SSL certs (optional) Deploy Recovering from a locked site  ",
        "image": "",
        "url": "/frameworks/drupal7/simplesaml.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "ec3c067f73cfba8ac0cf97bc59ee5d90",
        "title": "Use a private Git repository",
        "description": "",
        "text": " Pull code from a private Git repository Let’s say you’re building a module (or theme, library…) which is stored in a private Git repository that you have access to, and you want to use it on your project. Platform.sh allows you to include code dependencies that are stored in external private Git repositories (e.g. from a Drupal .make file, a PHP composer.json file). To grant Platform.sh access to your private Git repository, you need to add the project public SSH key to the deploy keys of your Git repository. You can copy your project’s public key by going to the Settings tab on the management console and then clicking the Deploy Key tab on the left hand side. If your private repository is on GitHub, go to the target repository’s settings page. Go to Deploy Keys and click Add deploy key. Paste the public SSH key in and submit. By default, on github, deploy keys are read only, so you don’t need to worry about the system pushing code to the private repository. If you’re using Drupal for example, you can now use your private module by adding it to your make file: ; Add private repository from GitHub projects[module_private][type] = module projects[module_private][subdir] =  contrib  projects[module_private][download][type] = git projects[module_private][download][branch] = dev projects[module_private][download][url] =  git@github.com:guguss/module_private.git  Note: In the make file use the \u0026lt;user\u0026gt;@\u0026lt;host\u0026gt;:\u0026lt;path\u0026gt;.git format, or ssh://\u0026lt;user\u0026gt;@\u0026lt;host\u0026gt;:\u0026lt;port\u0026gt;/\u0026lt;path\u0026gt;.git if using a non-standard port. Using multiple private Git repositories More complex projects may have many repositories that they want to include, but GitHub only allows you to associate a deploy key with a single repository. If your project needs to access multiple repositories, you can choose to attach an SSH key to an automated user account. Since this account won’t be used by a human, it’s called a machine user. You can then add the machine account as collaborator or add the machine user to a team with access to the repositories it needs to manipulate. More information about this is available on GitHub .",
        "section": "Development",
        "subsections": " Pull code from a private Git repository Using multiple private Git repositories  ",
        "image": "",
        "url": "/development/private-repository.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "c722a57676dea0d653a458d18a4f43c3",
        "title": "Vulnerability Scanning and Penetration Testing",
        "description": "",
        "text": " Platform.sh understands the need for application owners to ensure the integrity, and standards compliance, of their applications. Because there could be adverse impacts to other clients which would violate our terms of service, we only permit certain types of tests. Approved Activities Vulnerability scanning of your web application. You are free to perform this as often as required without approval from Platform.sh. Web application penetration tests that do not result in high network load. You are free to perform this as often as required without approval from Platform.sh. Approved Activities by Prior Arrangement For Platform.sh Enterprise-Dedicated customers we do permit infrastructure penetration testing (but not load testing) by prior arrangement. This requires special advanced preparation. You must submit a support ticket request a minimum of three (3) weeks in advance for us to coordinate this on your behalf. Prohibited Activities Vulnerability scanning of web applications which you do not own. Denial of Service tests and any other type of testing which results in heavy network load. Social engineering tests of Platform.sh services including falsely representing yourself as a Platform.sh employee. Infrastructure penetration tests for non-Dedicated-Enterprise customers. This includes SSH and database testing. Rate Limits Please limit scans to a maximum of 20 Mbps and 50 requests per second in order to prevent triggering denial of service bans. Troubleshooting If your vulnerability scanning suggests there may be an issue with Platform.sh’s service, please ensure your container is updated and retest. If the problem remains, please contact support .",
        "section": "Security and compliance",
        "subsections": " Approved Activities Approved Activities by Prior Arrangement Prohibited Activities Rate Limits Troubleshooting  ",
        "image": "",
        "url": "/security/pen-test.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "1f80afed50618a7f5be17adaaf31af33",
        "title": "Web",
        "description": "",
        "text": " The web key defines a single web instance container running a single web server process (currently Nginx), behind which runs your application. The web key configures the web server, including what requests should be served directly (such as static files) and which should be passed to your application. The server is extremely flexible, which means that some configurations will be more involved than others. Additionally, defaults may vary somewhat between different language base images (specified by the type key of .platform.app.yaml). The first section on this page explains the various options the file supports. If you prefer, the later sections include various example configurations to demonstrate common patterns and configurations. You can also examine the .platform.app.yaml files of the provided project templates for various common Free Software applications. See the various language pages for an index of available examples. The web key defines how the application is exposed to the web (in HTTP). Here we tell the web application how to serve content, including static files, front-controller scripts, index files, index scripts, and so on. We support any directory structure, so the static files can be in a subdirectory and the index.php file can be further down. Commands The commands key defines the command to launch the application. For now there is only a single command, start, but more will be added in the future. The start key specifies the command to use to launch your application. That could be running a uwsgi command for a Python application or a unicorn command for a Ruby application, or simply running your compiled Go application. If the command specified by the start key terminates it will be restarted automatically. web:commands:start:\u0026#39;uwsgi --ini conf/server.ini\u0026#39; Note: Never “background” a start process using \u0026amp;. That will be interpreted as the command terminating and the supervisor process will start a second copy, creating an infinite loop until the container crashes. Just run it as normal and allow the Platform.sh supervisor to manage it. On PHP containers this value is optional and will default to starting PHP-FPM (i.e. /usr/sbin/php-fpm7.0 on PHP7 and /usr/sbin/php5-fpm on PHP5). On all other containers it should be treated as required. It can also be set explicitly on a PHP container in order to run a dedicated process such as React PHP or Amp . Upstream upstream specifies how the front server will connect to your application (the process started by commands.start above). It has two keys: socket_family: Default: tcp. Describes whether your application will listen on a Unix socket (unix) or a TCP socket (tcp). protocol: Specifies whether your application is going to receive incoming requests over HTTP (http) or FastCGI (fastcgi). The default varies depending on which application runtime you’re using. Other values will be supported in the future. On a PHP container with FPM there is almost never a reason to set the upstream explicitly, as the defaults are already configured properly for PHP-FPM. On all other containers the default is tcp and http. web:upstream:socket_family:tcpprotocol:httpThe above configuration (which is the default on non-PHP containers) will forward connections to the process started by commands.start as a raw HTTP request to a TCP port, as though the process were listening to the incoming request directly. Socket family If the socket_family is set to tcp, then your application should listen on the port specified by the PORT environment variable. (In practice it is almost always 8888, but checking the variable is preferred.) If the socket_family is set to unix, then your application should open the unix socket file specified by the SOCKET environment variable. If your application isn’t listening at the same place that the runtime is sending requests, you’ll see 502 Bad Gateway errors when you try to connect to your web site. Locations The locations block is the most powerful, and potentially most involved, section of the .platform.app.yaml file. It allows you to control how the application container responds to incoming requests at a very fine-grained level. Common patterns also vary between language containers due to the way PHP-FPM handles incoming requests. Each entry of the locations block is an absolute URI path (with leading /) and its value includes the configuration directives for how the web server should handle matching requests. That is, if your domain is example.com then \u0026#39;/\u0026#39; means “requests for example.com/”, while \u0026#39;/admin\u0026#39; means “requests for example.com/admin”. If multiple blocks could match an incoming request then the most-specific will apply. web:locations:\u0026#39;/\u0026#39;:# Rules for all requests that don\u0026#39;t otherwise match....\u0026#39;/sites/default/files\u0026#39;:# Rules for any requests that begin with /sites/default/files....The simplest possible locations configuration is one that simply passes all requests on to your application unconditionally: web:locations:\u0026#39;/\u0026#39;:passthru:trueThat is, all requests to /* should be forwarded to the process started by web.commands.start above. Note that for PHP containers the passthru key must specify what PHP file the request should be forwarded to, and must also specify a docroot under which the file lives. For example: web:locations:\u0026#39;/\u0026#39;:root:\u0026#39;web\u0026#39;passthru:\u0026#39;/app.php\u0026#39;This block will serve requests to / from the web directory in the application, and if a file doesn’t exist on disk then the request will be forwarded to the /app.php script. A full list of the possible subkeys for locations is below. root: The folder from which to serve static assets for this location relative to the application root. The application root is the directory in which the .platform.app.yaml file is located. Typical values for this property include public or web. Setting it to \u0026#39;\u0026#39; is not recommended, and its behavior may vary depending on the type of application. Absolute paths are not supported. passthru: Whether to forward disallowed and missing resources from this location to the application and can be true, false or an absolute URI path (with leading /). The default value is false. For non-PHP applications it will generally be just true or false. In a PHP application this will typically be the front controller such as /index.php or /app.php. This entry works similar to mod_rewrite under Apache. Note: If the value of passthru does not begin with the same value as the location key it is under, the passthru may evaluate to another entry. That may be useful when you want different cache settings for different paths, for instance, but want missing files in all of them to map back to the same front controller. See the example block below. index: The files to consider when serving a request for a directory: an array of file names or null. (typically [\u0026#39;index.html\u0026#39;]). Note that in order for this to work, access to the static files named must be allowed by the allow or rules keys for this location. expires: How long to allow static assets from this location to be cached (this enables the Cache-Control and Expires headers) and can be a time or -1 for no caching (default). Times can be suffixed with “ms” (milliseconds), “s” (seconds), “m” (minutes), “h” (hours), “d” (days), “w” (weeks), “M” (months, 30d) or “y” (years, 365d). scripts: Whether to allow loading scripts in that location (true or false). This directive is only meaningful on PHP. allow: Whether to allow serving files which don’t match a rule (true or false, default: true). headers: Any additional headers to apply to static assets. This section is a mapping of header names to header values. Responses from the application aren’t affected, to avoid overlap with the application’s own ability to include custom headers in the response. rules: Specific overrides for a specific location. The key is a PCRE (regular expression) that is matched against the full request path. request_buffering: Most application servers do not support chunked requests (e.g. fpm, uwsgi), so Platform.sh enables request_buffering by default to handle them. That default configuration would look like this if it was present in .platform.app.yaml: web:locations:\u0026#39;/\u0026#39;:passthru:truerequest_buffering:enabled:truemax_request_size:250mIf the application server can already efficiently handle chunked requests, the request_buffering subkey can be modified to disable it entirely (enabled: false). Additionally, applications that frequently deal with uploads greater than 250MB in size can update the max_request_size key to the application’s needs. Note that modifications to request_buffering will need to be specified at each location where it is desired. Rules The rules block warrants its own discussion as it allows overriding most other keys according to a regular expression. The key of each item under the rules block is a regular expression matching paths more specifically than the locations block entries. If an incoming request matches the rule, then its handling will be overridden by the properties under the rule. Note that it will override the entire rule in the case of a compound rule like headers. (See example below.) For example, the following file will serve dynamic requests from index.php in the public directory and disallow requests for static files anywhere. Then it sets a rule to explicitly whitelist common image file formats, and sets a cache lifetime for them of 5 minutes. web:locations:\u0026#39;/\u0026#39;:root:\u0026#39;public\u0026#39;passthru:\u0026#39;/index.php\u0026#39;allow:falserules:# Allow common image files you can imagine the locations and rules blocks can be used to create highly involved and powerful configurations, but obeys Parker’s Law. (With great power comes great responsibility.) The examples below demonstrate various common configurations and recommended defaults. How do I setup a basic PHP application with front-controller? The following web block is a reasonable starting point for a custom PHP application. It sets the directory public as the docroot, and any missing files will get mapped to the /index.php file. mp4 files are forbidden entirely. Image files from the images URL (which will be served from the /public/images directory) will have an expiration time set, but non-image files will be disallowed. web:locations:\u0026#39;/\u0026#39;:root:\u0026#39;public\u0026#39;passthru:\u0026#39;/index.php\u0026#39;index:- index.php# No caching for static files.# (Dynamic pages use whatever cache headers are generated by the program.)expires:-1scripts:trueallow:truerules:# Disallow .mp4 files Set a 5 min expiration time for static files here; a missing URL# will passthru to the \u0026#39;/\u0026#39; location above and hit the application# front-controller.\u0026#39;/images\u0026#39;:expires:300passthru:trueallow:falserules:# Only allow static image files from the images can I serve a static-only site? Although most websites today have some dynamic component, static site generators are a valid way to build a site. This documentation is built using a tool called Hugo, and served by Platform.sh as a static site. You can see the entire repository on GitHub. The .platform.app.yaml file it uses is listed below. Note in particular the web.commands.start directive. There needs to be some background process so it’s set to the sleep shell command, which will simply block forever (or some really long time, as computers don’t know about forever) and restart if needed. The file also runs the Hugo build process, and then whitelists the files to serve. # .platform.app.yaml# The name of this application, which must be unique within a project.name:\u0026#39;docs\u0026#39;# The type key specifies the language and version for your application.type:\u0026#39;nodejs:12\u0026#39;# The hooks that will be triggered when the package is deployed.hooks:# Build hooks can modify the application files on disk but not access any services like databases.build:!includetype:stringpath:build.shdeploy:| cp data/templates.yaml public/scripts/xss/dist/config/templates.yamlcppublic/index.jsonpublic/scripts/xss/dist/config/index.json# The configuration of the application when it is exposed to the web.web:commands:# Run a no-op process that uses no CPU resources, since this is a static site.start:sleepinfinitylocations:\u0026#39;/\u0026#39;:# The public directory of the application relative to its How can I control the headers sent with my files? There are many use cases for setting custom headers on static content, such as custom content type headers, limiting cross-origin usage, etc. Consider the following example: web:locations: / :root: public passthru: /index.php index:- the headers directive sets the X-Frame-Options header to SAMEORIGIN for all static files. That directive is then overriden by the two rules blocks. For *.mp4 files, two custom headers will be sent: X-Frame-Options and Content-Type. The repeated X-Frame-Options is necessary as the headers directive in the rule overrides the parent, rather than extending it. Therefore, the rule for *.mp3 files will add only an X-Specialness header, and no X-Frame-Options header. This example also demonstrates an effective way to set custom Content-Type headers for unusual file types using rules. Note that the headers directive applies only to static content. Headers for responses generated by your application are unaffected. If custom headers for certain file types or frame control are needed, set them from within the application. How can I rewrite an incoming request without a redirect? Rules blocks support regular expression capture groups that can be referenced in a passthru command. For example, the following configuration will result in requests to /project/123 being seen by the application as a request to /index.php?projectid=123 without causing an HTTP redirect. Note that query parameters present in the request are unaffected and will, unconditionally, appear in the request as seen by the application. web:locations:\u0026#39;/\u0026#39;:root:\u0026#39;public\u0026#39;passthru:\u0026#39;/index.php\u0026#39;index:- index.phpscripts:trueallow:truerules:\u0026#39;^/project/(?\u0026lt;projectid\u0026gt;.*)$\u0026#39;:passthru:\u0026#39;/index.php?projectid=$projectid\u0026#39;How can I serve directories at different paths than in my application? Although it’s common for the directories on disk to be served directly by the web server, that’s not actually a requirement. If desired it is quite possible to create a web URL structure that does not map 1:1 to the structure on disk. Consider the following example. The git repository is structured like so: .platform/ services.yaml routes.yaml .platform.app.yaml application/ conf/ server.ini application.py gitbook-src/ old-docs/ The application directory contains a Python application. The gitbook-src directory contains a GitBook project that is the public documentation for the application. The old-docs directory contains a static HTML backup of legacy documentation for an older version of the application that is still needed. Assume that the GitBook source is compiled by the build process into the _book directory, as in the example above. The following web block will: Start your Python application using uwsgi. Route all requests to ‘/’ to the Python application unconditionally, unless one of the following two rules apply. Route requests to the /docs path to the _book directory, which contains our generated documentation, with a short cache lifetime. Route requests to the /docs/legacy path to the old-docs directory, which contains plain old HTML, with a very long cache lifetime since those files should never change. web:commands:start:\u0026#39;uwsgi --ini application/conf/server.ini\u0026#39;locations:\u0026#39;/\u0026#39;:passthru:true\u0026#39;/docs\u0026#39;:root:\u0026#39;_book\u0026#39;index:-  index.html expires:300sscripts:falseallow:true\u0026#39;/docs/legacy\u0026#39;:root:\u0026#39;old-docs\u0026#39;index:-  index.html expires:4wscripts:falseallow:trueEven though the URL structure doesn’t match the directory names or hierarchy on disk, that’s no issue. It also means the application can safely coexist with static files as if it were a single site hierarchy without the need to mix the static pages in with your Python code.",
        "section": "Configure your application",
        "subsections": " Commands Upstream  Socket family   Locations  Rules   How do I setup a basic PHP application with front-controller? How can I serve a static-only site? How can I control the headers sent with my files? How can I rewrite an incoming request without a redirect? How can I serve directories at different paths than in my application?  ",
        "image": "",
        "url": "/configuration/app/web.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "0691da9cff24fc173e25a67f96a292f2",
        "title": "Multiple Drupal  sites in a single Project",
        "description": "",
        "text": " Platform.sh supports running multiple applications in the same project and these can be two or more Drupal site. But, they would be separate Drupal instances , they will have their assets separate and live their lives apart and it would be much better for them not to share the same database (though they could). Note, that the same Drupal instance can also use multiple databases (just add multiple instances to services.yaml and use db_select) you will need to override settings.php as described here and add the other databases you could then use db_select to switch between those. Old Style “Mutlisite” and Platform.sh Platform.sh actively discourages running Drupal in “multisite” mode. Doing so eliminates many of the advantages Platform.sh offers, such as isolation, safe testing, and so forth. Additionally, because of the dynamic nature of the domain names that are created for the different environments the multisite configuration would likely be complex and fragile. We recommend running separate projects for separate Drupal sites, or using one of the various “single instance” options available such as Domain Access , Organic Groups , or Workbench Access . Using Domain Access Of course Platform.sh supports the Domain Access module, as it supports anything Drupal. If the multiple sites are part of the same project this makes sense. Because of the dynamic nature of routes in Platform.sh you will need to implement some logic (here you would replace MYMODULE with a convenient name of your own and include it in your custom modules for your Drupal installation). \u0026lt;?php /** * Implements hook_domain_default_domains(). */ function MYMODULE_domain_default_domains() { $domains = array(); $domains[\u0026#39;wipe-domain-tables\u0026#39;] = \u0026#39;wipe-domain-tables\u0026#39;; $routes = (array) json_decode(base64_decode(getenv(\u0026#39;PLATFORM_ROUTES\u0026#39;))); if (!empty($routes) \u0026amp;\u0026amp; is_array($routes)) { $weight = -1; foreach ($routes as $url =\u0026gt; $route) { if ( $route-\u0026gt;upstream == \u0026#39;drupal\u0026#39; \u0026amp;\u0026amp; $url, $matches) \u0026amp;\u0026amp; $route-\u0026gt;original_url, $matches2) ) { $scheme = $matches[1]; $domain_name = $matches[2]; $machine_name = $matches2[1]; $domains[$machine_name] = array( \u0026#39;subdomain\u0026#39; =\u0026gt; $domain_name, \u0026#39;sitename\u0026#39; =\u0026gt; MYMODULE_get_sitename($machine_name), \u0026#39;scheme\u0026#39; =\u0026gt; $scheme, \u0026#39;valid\u0026#39; =\u0026gt; 1, \u0026#39;weight\u0026#39; =\u0026gt; $weight\u0026#43;\u0026#43;, \u0026#39;is_default\u0026#39; =\u0026gt; ($machine_name == \u0026#39;www\u0026#39; ? 1 : 0), \u0026#39;machine_name\u0026#39; =\u0026gt; $machine_name, ); } } } return $domains; }",
        "section": "Getting Started",
        "subsections": " Using Domain Access  ",
        "image": "",
        "url": "/frameworks/drupal7/multi-site.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "5b08bd32e99bce47ad55d0af03caa38b",
        "title": "Next steps",
        "description": "",
        "text": " In this guide you created a project using the CLI and configured your project to run on Platform.sh using a few simple configuration files. Don’t stop now! There are far more features that make Platform.sh profoundly helpful to developers that you have left to explore. Developing on Platform.sh Once an application has been migrated to Platform.sh, there’s plenty more features that will help improve your development life cycle. Local development Remotely connect to services and build your application locally during development. Development environments Activate development branches and test new features before merging into production. Additional Resources External Integrations Configure Platform.sh to mirror every push and pull request on GitHub, Gitlab, and Bitbucket. Going Live Set up your site for production, configure domains, and go live! Platform.sh Community Portal Check out how-tos, tutorials, and get help for your questions about Platform.sh. Platform.sh Blog Read news and how-to posts all about working with Platform.sh. Back",
        "section": "Getting started",
        "subsections": "   Developing on Platform.sh Additional Resources    ",
        "image": "",
        "url": "/gettingstarted/gettingstarted/own-code/next-steps.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "64b63a3d122539ad6dad284ea666ba7c",
        "title": "Redis (Object cache)",
        "description": "",
        "text": " Redis is a high-performance in-memory object store, well-suited for application level caching. See the Redis documentation for more information. Platform.sh supports two different Redis configurations: One persistent (useful for key-value application data) and one ephemeral (in-memory only, useful for application caching). Aside from that distinction they are identical. Supported versions Grid Dedicated 3.2 4.0 5.0 5.0 Deprecated versions The following versions are available but are not receiving security updates from upstream, so their use is not recommended. They will be removed at some point in the future. Grid Dedicated 2.8 3.0 3.2 Note: Versions 3.0 and higher support up to 64 different databases per instance of the service, but Redis 2.8 is configured to support only a single database. Ephemeral Redis The redis service type is configured to serve as a LRU cache; its storage is not persistent. It is not suitable for use except as a disposable cache. To add an Ephemeral Redis service, specify it in your .platform/services.yaml file like so: cacheredis:type:redis:5.0 Data in an Ephemeral Redis instance is stored only in memory, and thus requires no disk space. When the service hits its memory limit it will automatically evict old cache items according to the configured eviction rule to make room for new ones. Persistent Redis The redis-persistent service type is configured for persistent storage. That makes it a good choice for fast application-level key-value storage. To add a Persistent Redis service, specify it in your .platform/services.yaml file like so: data:type:redis-persistent:5.0disk:256 The disk key is required for redis-persistent to tell Platform.sh how much disk space to reserve for Redis’ persistent data. Note: Switching a service from Persistent to Ephemeral configuration is not supported at this time. To switch between modes, use a different service with a different name. Relationship The format exposed in the $PLATFORM_RELATIONSHIPS environment variable : {  cluster :  rjify4yjcwxaa-master-7rqtwti ,  fragment : null,  host :  redis.internal ,  host_mapped : false,  hostname :  ftfs74bvoizcu4ua5eisskh7re.redis.service._.eu-3.platformsh.site ,  ip :  169.254.0.86 ,  password : null,  path : null,  port : 6379,  public : false,  query : [],  rel :  redis ,  scheme :  redis ,  service :  redis ,  type :  redis:5.0 ,  username : null } The format is identical regardless of whether it’s a persistent or ephemeral service. Usage example In your .platform/services.yaml: cacheredis:type:redis:5.0 If you are using PHP, configure a relationship and enable the PHP redis extension in your .platform.app.yaml. runtime:extensions:- redisrelationships:rediscache: cache:redis You can then use the service in a configuration file of your application with something like: Java Node.js PHP Python package sh.platform.languages.sample; import redis.clients.jedis.Jedis; import redis.clients.jedis.JedisPool; import sh.platform.config.Config; import sh.platform.config.Redis; import java.util.Set; import java.util.function.Supplier; public class RedisSample implements Supplier\u0026lt;String\u0026gt; { @Override public String get() { StringBuilder logger = new StringBuilder(); // Create a new config object to ease reading the Platform.sh environment variables. // You can alternatively use getenv() yourself. Config config = new Config(); // The \u0026#39;database\u0026#39; relationship is generally the name of primary database of an application. // It could be anything, though, as in the case here here where it\u0026#39;s called  redis . Redis database = config.getCredential( redis , Redis::new); JedisPool dataSource = database.get(); // Get a Redis Client final Jedis jedis = dataSource.getResource(); // Set a values jedis.sadd( cities ,  Salvador ); jedis.sadd( cities ,  London ); jedis.sadd( cities ,  São Paulo ); // Read it back. Set\u0026lt;String\u0026gt; cities = jedis.smembers( cities ); logger.append( cities:   \u0026#43; cities); jedis.del( cities ); return logger.toString(); } } const redis = require(\u0026#39;redis\u0026#39;); const config = require( platformsh-config ).config(); const { promisify } = require(\u0026#39;util\u0026#39;); exports.usageExample = async function() { const credentials = config.credentials(\u0026#39;redis\u0026#39;); var client = redis.createClient(credentials.port, credentials.host); // The Redis client is not Promise-aware, so make it so. const redisGet = promisify(client.get).bind(client); const redisSet = promisify(client.set).bind(client); let key = \u0026#39;Deploy day\u0026#39;; let value = \u0026#39;Friday\u0026#39;; // Set a value. await redisSet(key, value); // Read it back. let test = await redisGet(key); let output = `Found value \u0026lt;strong\u0026gt;${test}\u0026lt;/strong\u0026gt; for key \u0026lt;strong\u0026gt;${key}\u0026lt;/strong\u0026gt;.`; return output; }; \u0026lt;?php declare(strict_types=1); use // Create a new config object to ease reading the Platform.sh environment variables. // You can alternatively use getenv() yourself. $config = new Config(); // Get the credentials to connect to the Redis service. $credentials = $config-\u0026gt;credentials(\u0026#39;redis\u0026#39;); try { // Connecting to Redis server. $redis = new Redis(); $redis-\u0026gt;connect($credentials[\u0026#39;host\u0026#39;], $credentials[\u0026#39;port\u0026#39;]); $key =  Deploy day ; $value =  Friday ; // Set a value. $redis-\u0026gt;set($key, $value); // Read it back. $test = $redis-\u0026gt;get($key); printf(\u0026#39;Found value \u0026lt;strong\u0026gt;%s\u0026lt;/strong\u0026gt; for key \u0026lt;strong\u0026gt;%s\u0026lt;/strong\u0026gt;.\u0026#39;, $test, $key); } catch (Exception $e) { print $e-\u0026gt;getMessage(); } from redis import Redis from platformshconfig import Config def usage_example(): # Create a new config object to ease reading the Platform.sh environment variables. # You can alternatively use os.environ yourself. config = Config() # Get the credentials to connect to the Redis service. credentials = config.credentials(\u0026#39;redis\u0026#39;) try: redis = Redis(credentials[\u0026#39;host\u0026#39;], credentials[\u0026#39;port\u0026#39;]) key =  Deploy day  value =  Friday  # Set a value redis.set(key, value) # Read it back test = redis.get(key) return \u0026#39;Found value \u0026lt;strong\u0026gt;{0}\u0026lt;/strong\u0026gt; for key \u0026lt;strong\u0026gt;{1}\u0026lt;/strong\u0026gt;.\u0026#39;.format(test.decode( utf-8 ), key) except Exception as e: return e Multiple databases Redis 3.0 and above are configured to support up to 64 databases. Redis does not support distinct users for different databases so the same relationship connection gives access to all databases. To use a particular database, use the Redis select command through your API library. For instance, in PHP you could write: $redis-\u0026gt;select(0); // switch to DB 0 $redis-\u0026gt;set(\u0026#39;x\u0026#39;, \u0026#39;42\u0026#39;); // write 42 to x $redis-\u0026gt;move(\u0026#39;x\u0026#39;, 1); // move to DB 1 $redis-\u0026gt;select(1); // switch to DB 1 $redis-\u0026gt;get(\u0026#39;x\u0026#39;); // will return 42 Consult the documentation for your connection library and Redis itself for further details. Eviction policy On the Ephemeral redis service it is also possible to select the key eviction policy. That will control how Redis behaves when it runs out of memory for cached items and needs to clear old items to make room. cache:type:redis:5.0configuration:maxmemory_policy:allkeys-lruThe default value if not specified is allkeys-lru, which will simply remove the oldest cache item first. Legal values are: noeviction allkeys-lru volatile-lru allkeys-random volatile-random volatile-ttl See the Redis documentation for a description of each option. Using redis-cli to access your Redis service Assuming a Redis relationship named applicationcache defined in .platform.app.yaml relationships:rediscache: cacheredis:redis  and services.yaml cacheredis:type:redis:5.0 The host name and port number obtained from PLATFORM_RELATIONSHIPS would be applicationcache.internal and 6379. Open an SSH session and access the Redis server using the redis-cli tool as follows: redis-cli -h applicationcache.internal -p 6379 Using Redis as handler for native PHP sessions Using the same configuration but with your Redis relationship named sessionstorage: .platform/services.yaml cacheredis:type:redis:5.0 .platform.app.yaml relationships:sessionstorage: cache:redis variables:php:session.save_handler:redissession.save_path: tcp://sessionstorage.internal:6379 ",
        "section": "Configure services",
        "subsections": " Supported versions  Deprecated versions   Ephemeral Redis Persistent Redis Relationship Usage example Multiple databases Eviction policy Using redis-cli to access your Redis service  Using Redis as handler for native PHP sessions    ",
        "image": "",
        "url": "/configuration/services/redis.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "1550b99deff1aae06d4fec82b5986ac1",
        "title": "Update all the things",
        "description": "",
        "text": " The Platform.sh Rule: Update Early, Update Often Platform.sh periodically updates its container images for the latest security updates from upstream providers. (PHP versions, Ruby versions, MariaDB versions, etc.). These do not always happen immediately but when a security vulnerability is identified and released it tends to be fairly soon after. However, these updates are not automatically propagated to individual projects as that would involve potential customer downtime. Instead, the latest available version of every requested container is loaded on each deploy to a given environment. After a deploy you are always guaranteed to be running the latest Platform.sh-provided version of a container. If you have regular redeploys scheduled for Let’s Encrypt SSL certificates then that will also ensure your container versions are up to date at the same time. For that reason we recommend all customers setup the appropriate cron task to redeploy every two weeks or so.",
        "section": "Security and compliance",
        "subsections": "",
        "image": "",
        "url": "/security/updates.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "5d80ca831390a154bc72c43f1efaae35",
        "title": "Using Git submodules",
        "description": "",
        "text": " Clone submodules during deployment Platform.sh allows you to use submodules in your Git repository. They are usually listed in a .gitmodules file at the root of your Git repository. When you push via Git, Platform.sh will try to clone them automatically. Here is an example of a .gitmodules file: [submodule  app/Oro ] path = src/Oro url = https://github.com/orocrm/platform.git [submodule  src/OroPackages/src/Oro/Bundle/EntitySerializedFieldsBundle ] path = src/OroPackages/src/Oro/Bundle/EntitySerializedFieldsBundle url = https://github.com/orocrm/OroEntitySerializedFieldsBundle.git [submodule  src/OroB2B ] path = src/OroB2B url = https://github.com/orocommerce/orocommerce.git When you run git push, you can see the output of the log: Validating submodules. Updated submodule git://github.com/orocommerce/orocommerce: 4 references updated. Updated submodule git://github.com/orocrm/platform: 229 references updated. Updated submodule git://github.com/orocrm/OroEntitySerializedFieldsBundle: 11 references updated. Error when validating submodules If you see the following error: Validating submodules. Found unresolvable links, updating submodules. E: Error validating submodules in tree: - /src/Oro: Exception: commit 03567c6 not found. This might be due to the following errors fetching submodules: - git@github.com:orocommerce/orocommerce.git: HangupException: The remote server unexpectedly closed the connection. Since the Platform.sh Git server cannot connect to Github via SSH without being granted an SSH key to do so, you should not be using an SSH URL: git@github.com:..., but you should use an HTTPS URL instead: https://github.com/.... Use of private git repositories When using Git submodules that are hosted on private repositories, using the https protocol will fail with errors like: GitProtocolError: unexpected http resp 401 for https://bitbucket.org/myusername/mymodule.git/info/refs?service=git-upload-pack To fix this, you need to: Change your .gitmodules file from the HTTPS syntax to the SSH syntax, e.g. from: [submodule  support/mymodule ] path = support/mymodule url = https://bitbucket.org/myusername/mymodule.git to: [submodule  support/mymodule ] path = support/mymodule url=git@bitbucket.org:myusername/mymodule.git Add the SSH public key in the Platform.sh project settings “Deploy Key” tab in the Web UI as per the Private Repository documentation page, which will allow our Git service to pull the module from the remote git service. This assumes you have configured the remote git repository to allow this by generating a private/public key pair. For example, see the Bitbucket documentation .",
        "section": "Development",
        "subsections": " Clone submodules during deployment Error when validating submodules Use of private git repositories  ",
        "image": "",
        "url": "/development/submodules.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "e73050e3b22ec50821dd721acbd10923",
        "title": "Workers",
        "description": "",
        "text": " Every application may also define zero or more worker instances. A worker instance runs as its own container independently of the web instance and has no Nginx instance running. The router service cannot direct public requests to it, either, so running your own web server on a worker (using Node.js or Go) is not useful. A worker instance is the exact same code and compilation output as a web instance. The container image is built only once, and then deployed multiple times if needed. That is, the build hook and dependencies may not vary from one instance to another. What may vary is how the container is then configured and how resources are allocated. Worker instances are well suited for background tasks such as queue workers, updating indexes, or for running periodic reporting tasks that are too long to make sense as a cron job. (Although those should often be broken up into queue tasks.) A basic, common worker configuration could look like this: workers:queue:size:Scommands:start:| php worker.phpThat defines a single worker named queue, which will be a “small” container, and wil run the command php worker.php on startup. If worker.php ever exits it will be automatically restarted. Any number of workers may be defined with their own distinct name, subject to available resources on your plan. For resource allocation reasons, using workers in your project requires a Medium plan or larger. Accessing the Worker Container Like with any other application container Platform.sh allows you to connect to the worker instance through SSH to inspect logs and interact with it. Using the Platform CLI you would use the --worker switch, like so: platform ssh --worker=queue To output the SSH command you can use: platform ssh --worker=queue --pipe You will see the url is the name of the worker added to the user name after the application name part of the SSH url preceded by a double dash (--). For example given a project with id 3seb7f2j6ogbm you would connect to its master environment for an app called app with a url such as: ssh 3seb7f2j6ogbm-master-7rqtwti--app@ssh.us-2.platform.sh To connect to a worker called queue (as in the example above) you would use an SSH url that would look as follows: ssh 3seb7f2j6ogbm-master-7rqtwti--app--queue@ssh.us-2.platform.sh Workers vs Cron Both worker instances and cron tasks address similar use cases: They both address out-of-band work that an application needs to do but that should not or cannot be done as part of a normal web request. They do so in different ways, however, and so are fit for different use cases. A Cron job is well suited for tasks when: They need to happen on a fixed schedule, not continually. The task itself is not especially long, as a running cron job will block a new deployment. Or it is long, but can be easily divided into many small queued tasks. A delay between when a task is registered and when it actually happens is acceptable. A dedicated worker instance is a better fit if: Tasks should happen “now”, but not block a web request. Tasks are large enough that they risk blocking a deploy, even if they are subdivided. The task in question is a continually running process rather than a stream of discrete units of work. The appropriateness of one approach over the other also varies by language; single-threaded languages would benefit more from either cron or workers than a language with native multi-threading, for instance. If a given task seems like it would run equally well as a worker or as a cron, cron will generally be more efficient as it does not require its own container. Commands The commands key defines the command to launch the worker application. For now there is only a single command, start, but more will be added in the future. The commands.start property is required. The start key specifies the command to use to launch your worker application. It may be any valid shell command, although most often it will run a command in your application in the language of your application. If the command specified by the start key terminates it will be restarted automatically. Note that deploy and post_deploy hooks , as well as cron commands , will run only on the web container, not on workers. Inheritance Any top-level definitions for size , relationships , access , disk and mount , and variables will be inherited by every worker, unless overridden explicitly. That means, for example, that the following two .platform.app.yaml definitions produce identical workers. name:apptype:python:3.5disk:256mounts:test:source:localsource_path:testrelationships:database:\u0026#39;mysqldb:mysql\u0026#39;workers:queue:commands:start:| python queue-worker.pymail:commands:start:| python mail-worker.pyname:apptype:python:3.5workers:queue:commands:start:| python queue-worker.pydisk:256mounts:test:source:localsource_path:testrelationships:database:\u0026#39;mysqldb:mysql\u0026#39;mail:commands:start:| python mail-worker.pydisk:256mounts:test:source:localsource_path:testrelationships:database:\u0026#39;mysqldb:mysql\u0026#39;In both cases, there will be two worker instances named queue and mail. Both will have access to a MySQL/MariaDB service defined in services.yaml named mysqldb through the database relationship. Both will also have their own separate, independent local disk mount at /app/test with 256 MB of allowed space. Customizing a worker The most common properties to set in a worker to override the top-level settings are size and variables. size lets you allocate fewer resources to a container that will be running only a single background process (unlike the web site which will be handling many requests at once), while variables lets you instruct the application to run differently as a worker than as a web site. For example, consider this .platform.app.yaml: name:apptype: python:3.7 disk:2048hooks:build:| pip install -r requirements.txtpipinstall-e.pipinstallgunicornrelationships:database:\u0026#39;mysqldb:mysql\u0026#39;messages:\u0026#39;rabbitqueue:rabbitmq\u0026#39;variables:env:type:\u0026#39;none\u0026#39;web:commands:start: gunicorn -b $PORT project.wsgi:application variables:env:type:\u0026#39;web\u0026#39;mounts:uploads:source:localsource_path:uploadslocations: / :root:  passthru:trueallow:false /static :root: static/ allow:trueworkers:queue:size:\u0026#39;M\u0026#39;commands:start:| python queue-worker.pyvariables:env:type:\u0026#39;worker\u0026#39;disk:512mounts:scratch:source:localsource_path:scratchmail:size:\u0026#39;S\u0026#39;commands:start:| python mail-worker.pyvariables:env:type:\u0026#39;worker\u0026#39;disk:256mounts:{}relationships:emails:\u0026#39;rabbitqueue:rabbitmq\u0026#39;There’s a lot going on here, but it’s all reasonably straightforward. This configuration will take a single Python 3.7 code base from your repository, download all dependencies in requirements.txt, and the install Gunicorn. That artifact (your code plus the downloaded dependencies) will be deployed as three separate container instances, all running Python 3.7. The web instance will start a gunicorn process to serve a web application. It will run the gunicorn process to serve web requests, defined by the project/wsgi.py file which contains an application definition. It will have an environment variable named TYPE with value web. It will have a writable mount at /app/uploads with a maximum space of 2048 MB. It will have access to both a MySQL database and a RabbitMQ server, both of which are defined in services.yaml. Platform.sh will automatically allocate resources to it as available on the plan, once all fixed-size containers are allocated. The queue instance will be a worker that is not web-accessible. It will run the queue-worker.py script, and restart it automatically if it ever terminates. It will have an environment variable named TYPE with value worker. It will have a writable mount at /app/scratch with a maximum space of 512 MB. It will have access to both a MySQL database and a RabbitMQ server, both of which are defined in services.yaml (because it doesn’t specify otherwise). It will have “Medium” levels of CPU and RAM allocated to it, always. The mail instance will be a worker that is not web-accessible. It will run the mail-worker.py script, and restart it automatically if it ever terminates. It will have an environment variable named TYPE with value worker. It will have no writable file mounts at all. It will have access only to the RabbitMQ server, through a different relationship name than on the web instance. It will have no access to MySQL whatsoever. It will have “Small” levels of CPU and RAM allocated to it, always. This way, the web instance has a large upload space, the queue instance has a small amount of scratch space for temporary files, and the mail instance has no persistent writable disk space at all as it doesn’t need it. The mail instance also does not need any access to the SQL database so for security reasons it has none. The workers have known fixed sizes, while web can scale to as large as the plan allows. Each instance can also check the TYPE environment variable to detect how it’s running and, if appropriate, vary its behavior accordingly.",
        "section": "Configure your application",
        "subsections": " Accessing the Worker Container Workers vs Cron Commands Inheritance Customizing a worker  ",
        "image": "",
        "url": "/configuration/app/workers.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "553e2358ea3742bbf22cc783b7621238",
        "title": "Cron jobs",
        "description": "",
        "text": " Cron jobs allow you to run scheduled tasks at specified times or intervals. The crons section of .platform.app.yaml describes these tasks and the schedule when they are triggered. Each item in the list is a unique name identifying a separate cron job. Crons are started right after build phase. It has a few subkeys listed below: spec: The cron specification . For example: */19 * * * * to run every 19 minutes. cmd: The command that is executed, for example cd public ; drush core-cron The minimum interval between cron runs is 5 minutes, even if specified as less. Additionally, a variable delay is added to each cron job in each project in order to prevent host overloading should every project try to run their nightly tasks at the same time. Your crons will not run exactly at the time that you specify, but will be delayed by 0-300 seconds. A single application container may have any number of cron tasks configured, but only one may be running at a time. That is, if a cron task fires while another cron task is still running it will pause and then continue normally once the first has completed. Cron runs are executed using the dash shell, not the bash shell used by normal SSH logins. In most cases that makes no difference but may impact some more involved cron scripts. If an application defines both a web instance and a worker instance, cron tasks will be run only on the web instance. Note: Cron log output is captured in the at /var/log/cron.log. See the Log page for more information on logging. How do I setup Cron for a typical Drupal site? The following example runs Drupal’s normal cron hook every 19 minutes, using Drush. It also sets up a second cron task to run Drupal’s queue runner on the aggregator_feeds queue every 7 minutes. crons:# Run Drupal\u0026#39;s cron tasks every 19 minutes.drupal:spec:\u0026#39;*/19 * * * *\u0026#39;cmd:\u0026#39;cd web ; drush core-cron\u0026#39;# But also run pending queue tasks every 7 minutes.# Use an odd number to avoid running at the same time as the `drupal` cron.drush-queue:spec:\u0026#39;*/7 * * * *\u0026#39;cmd:\u0026#39;cd web ; drush queue-run aggregator_feeds\u0026#39;",
        "section": "Configure your application",
        "subsections": " How do I setup Cron for a typical Drupal site?  ",
        "image": "",
        "url": "/configuration/app/cron.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "406dc89b82bd7f55dd6ea9a56a731831",
        "title": "Drupal Frequently Asked Questions (FAQ)",
        "description": "",
        "text": " How should I name my make files? In order for Platform to automatically detect your make file, you need to call it project.make. You can also have a specific make file for Drupal core called project-core.make When I push changes to a make file, does Platform.sh run the update? After a push, Platform.sh will rebuild your environment and download all the modules that are in your make file. If an update function (hook_update) needs to run, you’ll have to manually trigger it by going to /update.php or use the deployment hooks to automatically run the updates. How can I provide a robots.txt file in production? If using the drupal build mode with a Drush make file, place your robots.txt file in your application root, as a sibling of .platform.app.yaml. It will get moved to the appropriate location automatically by the build process. For all other cases just include the file in your web root normally. On non-production environments Platform.sh automatically blocks web crawlers using the X-Robots-Tag header . You can disable that per-environment if needed. I’m getting a PDO Exception ‘MySQL server has gone away’ Normally, this means there is a problem with the MySQL server container and you may need to increase the storage available to MySQL to resolve the issue. Ballooning MySQL storage can be caused by a number of items: A large number of watchdog entries being captured. Fix the errors being generated or disable database logging. Cron should run at regular intervals to ensure cache tables get cleared out. If you’re using Drupal Commerce Core \u0026lt; 1.10, you may have an extremely large cache_form table . Upgrade to Commerce Core 1.10 to resolve. Why do I get “MySQL cannot connect to the database server”? If you are having a problem connecting to the database server, you will need force a re-deployment of the database container. To do so, you can edit the service definition to add or remove a small amount of storage and then push. Can I use the name of the session cookie for caching? For Drupal sites, the name of the session cookie is based on a hash of the domain name. This means that it will actually be consistent for a specific website and can safely be used as a fixed value. How can I rebuild the site registry? During the migration process, one or more modules may have changed location. This could result in a WSOD (white screen of death), any number of errors (fatal or otherwise), or just a plain broken site. To remedy this situation, the registry will need to be rebuilt . To rebuild the Drupal registry on your Platform.sh instance, you will need to do the following: First, SSH into your web container. $ ssh [SSH-URL] Second, execute the following commands to download, tweak, and run the registry rebuild. $ drush dl registry_rebuild-7.x-2.3 --destination=/app/tmp $ sed -i \u0026#39;s/, define_drupal_root()/, /app/tmp/registry_rebuild/registry_rebuild.php $ cd /app/public $ php ../tmp/registry_rebuild/registry_rebuild.php Can I use Backup \u0026amp; Migrate? The Backup \u0026amp; Migrate module is a Drupal module that provides automated scheduled dumps of a Drupal site’s content. It does so in the form of an SQL dump and/or tar.gz archived copy of your site’s file directory, which can then be optionally uploaded to a remote storage service. In general B\u0026amp;M is not necessary when running on Platform.sh. Platform.sh’s Backup functionality offers a faster, more robust and easier to restore backup, and for exporting data using the Platform.sh CLI is just as effective. If, however, you find it necessary to still run B\u0026amp;M be aware that its resource requirements can be quite high. B\u0026amp;M requires a great deal of memory in order to create a backup, over and above Drupal’s memory requirements. It is possible for B\u0026amp;M to create a backup in the system’s temp folder, then PHP runs out of memory before it can complete sending the backup to a 3rd party or cleaning up the temp file. In the latter case, a full temp disk can result in other, seemingly unrelated issues such as an inability to upload files. If you find B\u0026amp;M failing or the temp directory filling up mysteriously, try increasing the PHP memory limit to account for B\u0026amp;M’s needs. For example, add the following to .platform.app.yaml: variables:php:memory_limit:512MIf that is still insufficient, your site may simply be too large to work effectively with B\u0026amp;M. We recommend setting up automated scheduled backups instead.",
        "section": "Getting Started",
        "subsections": " How should I name my make files? When I push changes to a make file, does Platform.sh run the update? How can I provide a robots.txt file in production? I\u0026rsquo;m getting a PDO Exception \u0026lsquo;MySQL server has gone away\u0026rsquo; Why do I get \u0026ldquo;MySQL cannot connect to the database server\u0026rdquo;? Can I use the name of the session cookie for caching? How can I rebuild the site registry? Can I use Backup \u0026amp; Migrate?  ",
        "image": "",
        "url": "/frameworks/drupal7/faq.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "5df2bf6b6b667b9475acc639cf1a4233",
        "title": "Solr (Search service)",
        "description": "",
        "text": " Apache Solr is a scalable and fault-tolerant search index. Solr search with generic schemas provided, and a custom schema is also supported. See the Solr documentation for more information.” Supported versions Grid Dedicated 3.6 4.1 6.3 6.6 7.6 7.7 8.0 8.4 4.10 6.3 6.6 8.0 Deprecated versions The following versions are available but are not receiving security updates from upstream, so their use is not recommended. They will be removed at some point in the future. Grid Dedicated None available Relationship The format exposed in the $PLATFORM_RELATIONSHIPS environment variable : {  cluster :  rjify4yjcwxaa-master-7rqtwti ,  host :  solr.internal ,  hostname :  7iug3likvszuk2vnf4y3dafara.solr.service._.eu-3.platformsh.site ,  ip :  169.254.202.136 ,  path :  solr/maincore ,  port : 8080,  rel :  solr ,  scheme :  solr ,  service :  solr ,  type :  solr:8.0  } Usage example In your .platform/services.yaml: searchsolr:type:solr:8.4disk:256 In your .platform.app.yaml: relationships:solrsearch: searchsolr:solr  Note: You will need to use the solr type when defining the service # .platform/services.yamlservice_name:type:solr:versiondisk:256 and the endpoint solr when defining the relationship # .platform.app.yamlrelationships:relationship_name:“service_name:solr” Your service_name and relationship_name are defined by you, but we recommend making them distinct from each other. Exception: This pattern will be the case unless you explictly set additional endpoints for multiple cores, as shown in the section below. You can then use the service in a configuration file of your application with something like: Go Java Node.js PHP Python package examples import (  fmt  psh  github.com/platformsh/config-reader-go/v2  gosolr  github.com/platformsh/config-reader-go/v2/gosolr  solr  github.com/rtt/Go-Solr  ) func UsageExampleSolr() string { // Create a NewRuntimeConfig object to ease reading the Platform.sh environment variables. // You can alternatively use os.Getenv() yourself. config, err := psh.NewRuntimeConfig() checkErr(err) // Get the credentials to connect to the Solr service. credentials, err := config.Credentials( solr ) checkErr(err) // Retrieve Solr formatted credentials. formatted, err := gosolr.FormattedCredentials(credentials) checkErr(err) // Connect to Solr using the formatted credentials. connection := \u0026amp;solr.Connection{URL: formatted} // Add a document and commit the operation. docAdd := map[string]interface{}{  add : []interface{}{ map[string]interface{}{ id : 123,  name :  Valentina Tereshkova }, }, } respAdd, err := connection.Update(docAdd, true) checkErr(err) // Select the document. q := \u0026amp;solr.Query{ Params: solr.URLParamMap{  q : []string{ id:123 }, }, } resSelect, err := connection.CustomSelect(q,  query ) checkErr(err) // Delete the document and commit the operation. docDelete := map[string]interface{}{  delete : map[string]interface{}{  id : 123, }, } resDel, err := connection.Update(docDelete, true) checkErr(err) message := one document - %s\u0026lt;br\u0026gt; Selecting document (1 expected): %d\u0026lt;br\u0026gt; Deleting document - %s\u0026lt;br\u0026gt; respAdd, resSelect.Results.NumFound, resDel) return message } package sh.platform.languages.sample; import org.apache.solr.client.solrj.SolrQuery; import org.apache.solr.client.solrj.SolrServerException; import org.apache.solr.client.solrj.impl.HttpSolrClient; import org.apache.solr.client.solrj.impl.XMLResponseParser; import org.apache.solr.client.solrj.response.QueryResponse; import org.apache.solr.client.solrj.response.UpdateResponse; import org.apache.solr.common.SolrDocumentList; import org.apache.solr.common.SolrInputDocument; import sh.platform.config.Config; import sh.platform.config.Solr; import java.io.IOException; import java.util.function.Supplier; public class SolrSample implements Supplier\u0026lt;String\u0026gt; { @Override public String get() { StringBuilder logger = new StringBuilder(); // Create a new config object to ease reading the Platform.sh environment variables. // You can alternatively use getenv() yourself. Config config = new Config(); Solr solr = config.getCredential( solr , Solr::new); try { final HttpSolrClient solrClient = solr.get(); solrClient.setParser(new XMLResponseParser()); // Add a document SolrInputDocument document = new SolrInputDocument(); final String id =  123456 ; document.addField( id , id); document.addField( name ,  Ada Lovelace ); document.addField( city ,  London ); solrClient.add(document); final UpdateResponse response = solrClient.commit(); logger.append( Adding one document. Status (0 is success):  ) SolrQuery query = new SolrQuery(); query.set( q ,  city:London ); QueryResponse queryResponse = solrClient.query(query); SolrDocumentList results = queryResponse.getResults(); logger.append(String.format( Selecting documents (1 expected): %d results.getNumFound())); // Delete one document solrClient.deleteById(id); logger.append(String.format( Deleting one document. Status (0 is success): %s solrClient.commit().getStatus())); } catch (SolrServerException | IOException exp) { throw new RuntimeException( An error when execute Solr  , exp); } return logger.toString(); } } const solr = require(\u0026#39;solr-node\u0026#39;); const config = require( platformsh-config ).config(); exports.usageExample = async function() { let client = new solr(config.formattedCredentials(\u0026#39;solr\u0026#39;, \u0026#39;solr-node\u0026#39;)); let output = \u0026#39;\u0026#39;; // Add a document. let addResult = await client.update({ id: 123, name: \u0026#39;Valentina Tereshkova\u0026#39;, }); output \u0026#43;=  Adding one document. Status (0 is success):   \u0026#43; addResult.responseHeader.status \u0026#43;  \u0026lt;br // Flush writes so that we can query against them. await client.softCommit(); // Select one document: let strQuery = client.query().q(); let writeResult = await client.search(strQuery); output \u0026#43;=  Selecting documents (1 expected):   \u0026#43; writeResult.response.numFound \u0026#43;  \u0026lt;br // Delete one document. let deleteResult = await client.delete({id: 123}); output \u0026#43;=  Deleting one document. Status (0 is success):   \u0026#43; deleteResult.responseHeader.status \u0026#43;  \u0026lt;br return output; }; \u0026lt;?php declare(strict_types=1); use use // Create a new config object to ease reading the Platform.sh environment variables. // You can alternatively use getenv() yourself. $config = new Config(); // Get the credentials to connect to the Solr service. $credentials = $config-\u0026gt;credentials(\u0026#39;solr\u0026#39;); try { $config = [ \u0026#39;endpoint\u0026#39; =\u0026gt; [ \u0026#39;localhost\u0026#39; =\u0026gt; [ \u0026#39;host\u0026#39; =\u0026gt; $credentials[\u0026#39;host\u0026#39;], \u0026#39;port\u0026#39; =\u0026gt; $credentials[\u0026#39;port\u0026#39;], \u0026#39;path\u0026#39; =\u0026gt;  /  . $credentials[\u0026#39;path\u0026#39;], ] ] ]; $client = new Client($config); // Add a document $update = $client-\u0026gt;createUpdate(); $doc1 = $update-\u0026gt;createDocument(); $doc1-\u0026gt;id = 123; $doc1-\u0026gt;name = \u0026#39;Valentina Tereshkova\u0026#39;; $update-\u0026gt;addDocuments(array($doc1)); $update-\u0026gt;addCommit(); $result = $client-\u0026gt;update($update); print  Adding one document. Status (0 is success):   .$result-\u0026gt;getStatus().  \u0026lt;br // Select one document $query = $client-\u0026gt;createQuery($client::QUERY_SELECT); $resultset = $client-\u0026gt;execute($query); print  Selecting documents (1 expected):   .$resultset-\u0026gt;getNumFound() .  \u0026lt;br // Delete one document $update = $client-\u0026gt;createUpdate(); $update-\u0026gt;addDeleteById(123); $update-\u0026gt;addCommit(); $result = $client-\u0026gt;update($update); print  Deleting one document. Status (0 is success):   .$result-\u0026gt;getStatus().  \u0026lt;br } catch (Exception $e) { print $e-\u0026gt;getMessage(); } import pysolr from xml.etree import ElementTree as et from platformshconfig import Config def usage_example(): # Create a new Config object to ease reading the Platform.sh environment variables. # You can alternatively use os.environ yourself. config = Config() # Get the credentials to connect to the Solr service. credentials = config.credentials(\u0026#39;solr\u0026#39;) try: formatted_url = config.formatted_credentials(\u0026#39;solr\u0026#39;, \u0026#39;pysolr\u0026#39;) # Create a new Solr Client using config variables client = pysolr.Solr(formatted_url) # Add a document message = \u0026#39;\u0026#39; doc_1 = {  id : 123,  name :  Valentina Tereshkova  } result0 = client.add([doc_1]) client.commit() message \u0026#43;= \u0026#39;Adding one document. Status (0 is success): {0} \u0026lt;br /\u0026gt;\u0026#39;.format(et.fromstring(result0)[0][0].text) # Select one document query = client.search(\u0026#39;*:*\u0026#39;) message \u0026#43;= documents (1 expected): {0} \u0026lt;br /\u0026gt;\u0026#39;.format(str(query.hits)) # Delete one document result1 = client.delete(doc_1[\u0026#39;id\u0026#39;]) client.commit() message \u0026#43;= one document. Status (0 is success): {0}\u0026#39;.format(et.fromstring(result1)[0][0].text) return message except Exception as e: return e Solr 4 For Solr 4, Platform.sh supports only a single core per server called collection1. You must provide your own Solr configuration via a core_config key in your .platform/services.yaml: search:type:solr:4.10disk:1024configuration:core_config:!archive \u0026lt;directory\u0026gt; The directory parameter points to a directory in the Git repository, in or below the .platform/ folder. This directory needs to contain everything that Solr needs to start a core. At the minimum, solrconfig.xml and schema.xml. For example, place them in .platform/solr/conf/ such that the schema.xml file is located at .platform/solr/conf/schema.xml. You can then reference that path like this - search:type:solr:4.10disk:1024configuration:core_config:!archive solr/conf/ Solr 6 and later For Solr 6 and later Platform.sh supports multiple cores via different endpoints. Cores and endpoints are defined separately, with endpoints referencing cores. Each core may have its own configuration or share a configuration. It is best illustrated with an example. search:type:solr:8.4disk:1024configuration:cores:mainindex:conf_dir:!archive core1-conf extraindex:conf_dir:!archive core2-conf endpoints:main:core:mainindexextra:core:extraindexThe above definition defines a single Solr 8.0 server. That server has 2 cores defined: mainindex — the configuration for which is in the .platform/core1-conf directory — and extraindex — the configuration for which is in the .platform/core2-conf directory. It then defines two endpoints: main is connected to the mainindex core while extra is connected to the extraindex core. Two endpoints may be connected to the same core but at this time there would be no reason to do so. Additional options may be defined in the future. Each endpoint is then available in the relationships definition in .platform.app.yaml. For example, to allow an application to talk to both of the cores defined above its .platform.app.yaml file should contain the following: relationships:solrsearch1:\u0026#39;search:main\u0026#39;solrsearch2:\u0026#39;search:extra\u0026#39;That is, the application’s environment would include a solr1 relationship that connects to the main endpoint, which is the mainindex core, and a solr2 relationship that connects to the extra endpoint, which is the extraindex core. The relationships array would then look something like the following: {  solr1 : [ {  path :  solr/mainindex ,  host :  248.0.65.197 ,  scheme :  solr ,  port : 8080 } ],  solr2 : [ {  path :  solr/extraindex ,  host :  248.0.65.197 ,  scheme :  solr ,  port : 8080 } ] } Configsets For even more customizability, it’s also possible to define Solr configsets. For example, the following snippet would define one configset, which would be used by all cores. Specific details can then be overriden by individual cores using core_properties, which is equivalent to the Solr core.properties file. search:type:solr:8.4disk:1024configuration:configsets:mainconfig:!archive configsets/solr8 cores:english_index:core_properties:| configSet=mainconfigschema=english/schema.xmlarabic_index:core_properties:| configSet=mainconfigschema=arabic/schema.xmlendpoints:english:core:english_indexarabic:core:arabic_indexIn this example, the directory .platform/configsets/solr8 contains the configuration definition for multiple cores. There are then two cores created: english_index uses the defined configset, but specifically the .platform/configsets/solr6/english/schema.xml file, while arabic_index is identical except for using the .platform/configsets/solr6/arabic/schema.xml file. Each of those cores is then exposed as its own endpoint. Note that not all core.properties features make sense to specify in the core_properties. Some keys, such as name and dataDir, are not supported, and may result in a solrconfig that fails to work as intended, or at all. Default configuration If no configuration is specified, the default configuration is equivalent to: search:type:solr:8.4configuration:cores:collection1:conf_dir:\u0026#39;{}\u0026#39;# This will pick up the default Drupal 8 configurationendpoints:solr:core:collection1The default configuration is based on an older version of the Drupal 8 Search API Solr module that is no longer in use. While it may work for generic cases defining your own custom configuration, core, and endpoint is strongly recommended. Limitations The recommended maximum size for configuration directories (zipped) is 2MB. These need to be monitored to ensure they don’t grow beyond that. If the zipped configuration directories grow beyond this, performance will decline and deploys will become longer. The directory archives will be compressed and string encoded. You could use this bash pipeline echo $(($(tar czf - . | base64 | wc -c )/(1024*1024))) Megabytes inside the directory to get an idea of the archive size. The configuration directory is a collection of configuration data, like a data dictionary, e.g. small collections of key/value sets. The best way to keep the size small is to restrict the directory context to plain configurations. Including binary data like plugin .jar files will inflate the archive size, and is not recommended. Accessing the Solr server administrative interface Because Solr uses HTTP for both its API and admin interface it’s possible to access the admin interface over an SSH tunnel. platform tunnel:open That will open an SSH tunnel to all services on the current environment, and give an output similar to: SSH tunnel opened on port 30000 to relationship: solr SSH tunnel opened on port 30001 to relationship: database Logs are written to: /home/myuser/.platformsh/tunnels.log List tunnels with: platform tunnels View tunnel details with: platform tunnel:info Close tunnels with: platform tunnel:close In this example, you can now open http://localhost:30000/solr/ in a browser to access the Solr admin interface. Note that you cannot create indexes or users this way, but you can browse the existing indexes and manipulate the stored data. Note: Platform.sh Dedicated users can use ssh -L 8888:localhost:8983 \u0026lt;user\u0026gt;@\u0026lt;cluster-name\u0026gt;.ent.platform.sh to open a tunnel instead, after which the Solr server administrative interface will be available at http://localhost:8888/solr/. Upgrading The Solr data format sometimes changes between versions in incompatible ways. Solr does not include a data upgrade mechanism as it is expected that all indexes can be regenerated from stable data if needed. To upgrade (or downgrade) Solr you will need to use a new service from scratch. There are two ways of doing that. Destructive In your services.yaml file, change the version of your Solr service and its name. Then update the name in the .platform.app.yaml relationships block. When you push that to Platform.sh, the old service will be deleted and a new one with the name name created, with no data. You can then have your application reindex data as appropriate. This approach is simple but has the downside of temporarily having an empty Solr instance, which your application may or may not handle gracefully, and needing to rebuild your index afterward. Depending on the size of your data that could take a while. Transitional For a transitional approach you will temporarily have two Solr services. Add a second Solr service with the new version a new name and give it a new relationship in .platform.app.yaml. You can optionally run in that configuration for a while to allow your application to populate indexes in the new service as well. Once you’re ready to cut over, remove the old Solr service and relationship. You may optionally have the new Solr service use the old relationship name if that’s easier for your application to handle. Your application is now using the new Solr service. This approach has the benefit of never being without a working Solr instance. On the downside, it requires two running Solr servers temporarily, each of which will consume resources and need adequate disk space. Depending on the size of your data that may be a lot of disk space.",
        "section": "Configure services",
        "subsections": " Supported versions  Deprecated versions   Relationship Usage example Solr 4 Solr 6 and later  Configsets Default configuration Limitations   Accessing the Solr server administrative interface Upgrading  Destructive Transitional    ",
        "image": "",
        "url": "/configuration/services/solr.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "dde2996e92ca2fe8488627346a50a929",
        "title": "Using SSH keys",
        "description": "",
        "text": " One of the ways Platform.sh keeps things secure is by using SSH behind the scenes. Users can interact with their environment through a command shell, or push changes to the environment’s Git repository, and both of these features rely on SSH. You can manage SSH keys through the CLI (see below), or through the SSH keys tab under Account Settings. Find your Public-Private keypair If you use Linux, you probably already have keys. The private key is usually in a file named ~/.ssh/id_rsa and the public key in ~/.ssh/id_rsa.pub, Searching for a public key file: Open up a command prompt. Run the following commands: $ cd ~/.ssh $ ls -a id_rsa id_rsa.pub known_hosts authorized_keys If you find a file named id_rsa.pub, you can use it with Platform.sh. If you don’t find an existing key, see the steps to create a new one in the next section . Create a New Public-Private Keypair Note: If you already have a SSH keypair, you can skip this step. Create a public-private keypair: $ ssh-keygen -t rsa -C  your_email_address@example.com  ssh-keygen generates the key pair and will ask you where you want to save the file: Generating public/private rsa key pair. Enter file in which to save the key (/Users/your_username/.ssh/id_rsa): The default location is fine in most cases. Now it’s time to create a passphrase. A good, strong passphrase is highly recommended, to make your key less useful if it falls into the wrong hands. Enter passphrase (empty for no passphrase): [Type a passphrase] Enter same passphrase again: [Type passphrase again] That’s it. Keys generated! Here are the results: Your identification has been saved in /Users/your_username/.ssh/id_rsa. Your public key has been saved in /Users/your_username/.ssh/id_rsa.pub. The key fingerprint is: 55:c5:d7:a9:1f:dc:7a:67:31:70:fd:87:5a:a6:d0:69 your_email_address@example.com Note: Make note of the location of your public key, you’re going to need that in the next section. Add the SSH key to your Platform account First off, you’ll need to copy your public key to the clipboard. Head over to your user account page on the Platform.sh Accounts page and navigate to the Account Settings tab. In the left side-bar, select SSH keys. Click the Add a public key button. Paste the key that you copied earlier into the ‘Key’ text box. You can also add a title if you like, otherwise it will be auto-generated. Click ‘Save’. That’s it! You’re all set. Now you’ll be able to use Git and command shells with any Platform.sh environment that your user account is authorized to work with. Forwarding keys by default It may be helpful to set your SSH client to always forward keys to Platform.sh servers, which can simplify other SSH or Rsync commands. To do so, include a block in your local ~/.ssh/config file like so: Host *.us.platform.sh ForwardAgent yes Host *.eu.platform.sh ForwardAgent yes Include one Host entry for each Platform.sh region you want to connect to, such as us-2 or eu-4. (You can include other configuration as desired.) SSH to your Web Server In the management console header, click on the environment tab and select the environment that you want to SSH into. Then click the SSH dropdown button towards the top right. $ ssh wk5fqz6qoo123-master@ssh.eu.platform.sh ___ _ _ __ | _ |__ _| |_ / _|___ _ _ _ __ | _/ / _` | _| _/ _ \u0026#39;_| \u0026#39; |_| |_|_|_| Welcome to Platform. This is environment master of project wk5fqz6qoo123. web@wk5fqz6qoo123-master--php:~$ Troubleshoot SSH While trying to log in via SSH, this can happen: $ ssh [SSH-URL] Permission denied (publickey). Don’t panic! It’s an issue which can happen for the following reasons: Your environment is inactive You haven’t redeployed (i.e. git push) your environment since adding the new public key You didn’t upload your public key to your user profile Your SSH private key has not been added into your ssh-agent Your SSH key files have incorrect permissions Check your public key Make sure your public key has been uploaded to your user account. Check your ssh-agent Check that your key is properly added to your SSH agent. This is an authentication agent that manages your private key. Check your SSH agent. Run the command ssh-add -l in your terminal: $ ssh-add -l 2048 12:b0:13:83:7f:56:18:9b:78:ca:54:90:a7:ff:12:69 /Users/nick/.ssh/id_rsa (RSA) Check that file name on the right (.ssh/id_rsa in the example above). Does it match your private key file? If you don’t see your private key file, add your private key: $ ssh-add path-to-your-key Try again. Specify your identity file If your identity (SSH key) associated with Platform.sh is not in a default file name (as may be explained in your SSH software manual, for example) you may have to append a specification like the one below so that the SSH software finds the correct key. Host platform.sh IdentityFile ~/.ssh/id_platformsh Be aware that, above, platform.sh stands for a hostname. Each different hostname you connect to Platform.sh at may have to be specified in the host line, separated by spaces. Still having trouble? If you followed all the steps above, you may also notice an error message similar to below while attempting to SSH to platform.sh: Hello Your Name, you successfully connected, but you do not have access to service \u0026#39;xxxxxxxxxxxxxx-master\u0026#39;: check permissions. Received disconnect from 54.210.49.244: 14: No more auth methods available This usually means a deployment has not been committed yet. When a new key is added, it only becomes immediately active for use with Git. For use with SSH, it will not be activated until a deployment is made. An easy way to force this is to create and push an empty commit: $ git commit --allow-empty -m \u0026#39;force redeploy\u0026#39; $ git push origin master If all else fails, generate some SSH debug information If your private key and public key both look OK but you don’t have any luck logging in, print debugging information. These lines often give clues about what is going wrong. Run the SSH command with the -v option, like this: $ ssh -v [SSH-URL] OpenSSH_6.7.8, OpenSSL 1.2.3 1 Sep 2014 debug1: Connecting to ssh.eu.platform.sh [54.32.10.98] port 22. debug1: Connection established. debug1: identity file /Users/nick/.ssh/id_rsa type 1 ...(30 more lines of this light reading)... debug1: Offering RSA public key: /Users/nick/.ssh/id_rsa debug1: Authentications that can continue: publickey debug1: No more authentication methods to try. Permission denied (publickey). or $ GIT_SSH_COMMAND= git -v  git clone [REPO-URL] You can use this information to make one last check of the private key file. If you’re still stuck, don’t hesitate to submit a support ticket, we’ll help you solve your problem.",
        "section": "Development",
        "subsections": " Find your Public-Private keypair Create a New Public-Private Keypair Add the SSH key to your Platform account  Forwarding keys by default   SSH to your Web Server Troubleshoot SSH  Check your public key Check your ssh-agent Specify your identity file Still having trouble? If all else fails, generate some SSH debug information    ",
        "image": "",
        "url": "/development/ssh.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "c6b2b8e5ca4b9e80f09785cf4b8df380",
        "title": "[Beta] Source operations",
        "description": "",
        "text": " An application can define a number of operations that apply to its source code and that can be automated. Note: Source Operations are currently in Beta. While the syntax is not expected to change, some behavior might in the future. A basic, common source operation could be to automatically update Composer dependencies like this: source:operations:update:command:| set -ecomposerupdategitaddcomposer.lockgitcommit-m Update Composer dependencies. The update key is the name of the operation. It is arbitrary, and multiple source operations can be defined. (You may wish to include more robust error handling than this example.) The environment resource gets a new source-operation action which can be triggered by the CLI: platform source-operation:run update The source-operation:run command takes the command name to run. Additional variables can be added to inject into the environment of the source operation. They will be interpreted the same way as any other variable set through the UI or CLI, which means you need an env: prefix to expose them as a Unix environment variable. They can then be referenced by the source operation like any other variable. platform source-operation:run update --variable env:FOO=bar --variable env:BAZ=beep When this operation is triggered: A clean Git checkout of the current environment HEAD commit is created; this checkout doesn’t have any remotes, has all the tags defined in the project, but only has the current environment branch. Sequentially, for each application that has defined this operation, the operation command is launched in the container image of the application. The environment will have all of the variables available during the build phase, optionally overridden by the variables specified in the operation payload. At the end of the process, if any commits were created, the new commits are pushed to the repository and the normal build process of the environment is triggered. Note that these operations run in an isolated container: it is not part of the runtime cluster of the environment, and doesn’t require the environment to be running. Also be aware that if multiple applications in a single project both result in a new commit, that will appear as two distinct commits in the Git history but only a single new build/deploy cycle will occur. Source Operations with an external Git integration Git integration can be configured to send commits made to the Platform.sh Git remote, to the upstream repository instead. This means that if a source operation did generate a new commit, the commit will be pushed to the upstream repository. Note: Currently, this configuration requires the enable_codesource_integration_push setting to be turned on by a Platform.sh staff and is only available to selected Beta customers. Source Operations can only be triggered on environment created by a branch, and not to environment created by a Pull Request on the external upstream (GitHub, Bitbucket, Gitlab). Automated Source Operations using cron You can use cron to automatically run your source operations. Note: Automated source operations using cron requires to get an API token and install the CLI in your application container. Once the CLI is installed in your application container and an API token has been configured, you can add a cron task to run your source operations once a day. We do not recommend triggering source operations on your master production environment, but rather on a dedicated environment which you can use for testing before deployment. The example below synchronizes the update-dependencies environment with its parent before running the update source operation: crons:update:# Run the \u0026#39;update\u0026#39; source operation every day at midnight.spec:\u0026#39;0 0 * * *\u0026#39;cmd:| set -eif[ $PLATFORM_BRANCH =update-dependencies];thenplatformenvironment:synccodedata--no-wait--yesplatformsource-operation:runupdate--no-wait--yesfi",
        "section": "Configure your application",
        "subsections": " Source Operations with an external Git integration Automated Source Operations using cron  ",
        "image": "",
        "url": "/configuration/app/source-operations.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "1e15d230f09f7bd37dc0348cd5a202ef",
        "title": "Multiple applications",
        "description": "",
        "text": " Platform.sh supports building multiple applications per project (for example RESTful web services with a front-end, or a main website and a blog). For resource allocation reasons, however, that is not supported on Standard plan. Note: This page only applies to Grid projects. Contact your sales representative for advanced Dedicated environment configurations. Project structure There are multiple ways to structure such a project, depending on the way your source code is organized and what your goal is. All of these approaches may be used within a single project simultaneously, although it is often easier to maintain if you settle on just one approach for a given project. Discrete code bases If your project consists of a discrete code base for each application, the most straightforward approach is to put both code bases into a single project repository in separate directories. Each will have its own .platform.app.yaml file, which will define how that particular application gets built, using the code in that directory. For example, if you have a Drupal back end with an AngularJS front end you could organize the repository like this: .git/ .platform/ drupal/ .platform.app.yaml ... angular/ .platform.app.yaml Each .platform.app.yaml file will define a single application container, and build code in that directory. The .platform directory is outside of all of them and still defines additional services you require, as well as routes. Note that disk paths in the .platform.app.yaml file are relative to the directory where that file lives by default. This is the recommended approach for most configurations. Explicit source.root As an alternative, you may specify a source.root key in a .platform.app.yaml file to override the “application root is where the file is” logic. The .platform.app.yaml file may then live anywhere in the repository but use code from another directory. Two separate .platform.app.yaml files may refer to the same directory if desired. For example: # .platform.app.yamlsource:root:restapp.platform/ main/ .platform.app.yaml .platform.app.yaml restapp/ # Your code here In this case, the .platform.app.yaml file in main does not specify a source.root, and so will be built from the code in main. The top-level .platform.app.yaml includes the YAML fragment above. It will get built using the code in restapp, as if it were in that directory. Note that disk parameters in the .platform.app.yaml file will be relative to the source.root directory if specified. The source.root path is relative to the repository root. The primary use case for this configuration is if the source code is pulled in as a Git submodule or downloaded during the build phase. applications.yaml It is possible to define an application in a .platform/applications.yaml file in addition to discrete .platform.app.yaml files. The syntax is nearly identical, but the source.root key is required. The applications.yaml file is then a YAML array of application definitions. For example, the following .platform/applications.yaml file defines three applications: # .platform/applications.yaml- name:apitype:golang:1.14source:root:apiapphooks:build:| go build -o bin/appweb:upstream:socket_family:tcpprotocol:httpcommands:start:./bin/applocations:/:allow:falsepassthru:true- name:maintype: php:7.4 source:root:mainappweb:locations: / :root: web passthru: /index.php - name:admintype: php:7.4 size:Ssource:root:mainappweb:locations: / :root: web passthru: /admin.php In this example, the apiapp directory will get built as a Go application while the mainapp directory will get built as two separate PHP applications, even though none of those directories has a .platform.app.yaml file. The two PHP applications will use the same source code, but have different front controllers for the admin and main applications. The admin instance will also be fixed at an S size container, while main will scale freely. The primary use case for this configuration is defining multiple applications with different configuration off of the same source code, or when the source code is downloaded during the build phase. Submodules Platform.sh supports Git submodules, so each application can be in a separate repository. However, there is currently a notable limitation: the .platform.app.yaml files must be in the top-level repository. That means the project must be structured like this: .git/ .platform/ routes.yaml services.yaml app1/ .platform.app.yaml app1-submodule/ index.php app2/ .platform.app.yaml app2-submodule/ index.php This puts your applications’ files at a different path relative to your .platform.app.yaml files. The recommended way to handle that is to specify a source.root key in the .platform.app.yaml file and have it reference the submodule directory. Multi-app Routes Every application, however it is defined, must have a unique name property. The routes.yaml file may then refer to that application by name as an upstream for whatever route is appropriate. For example, assuming this configuration from above: .git/ .platform/ drupal/ .platform.app.yaml ... angular/ .platform.app.yaml The .platform/routes.yaml file can be structured like this:  https://backend.{default}/ :type:upstreamupstream: drupal:http  https://{default}/ :type:upstreamupstream: angular:http (This assumes your Drupal application is named drupal and your Angular front-end is named angular.) Assuming a domain name of example.com, that will result in: https://backend.example.com/ being served by the Drupal instance. https://example.com/ being served by the AngularJS instance. There is no requirement that an application be web-accessible. If it is not specified in routes.yaml then it will not be web-accessible at all. However, if you are building a non-routable application off of the same code base as another application, you should probably consider defining it as a worker instead. The net result is the same but it is much easier to manage. Relationships In a multi-app configuration, applications by default cannot access each other. However, they may declare a relationships block entry that references another application rather than a service. In that case the endpoint is http. However, be aware that circular relationships are not supported. That is, application A cannot have a relationship to application B if application B also has a relationship to application A. Such circular relationships are usually a sign that the applications should be coordinating through a shared data store, like a database, RabbitMQ server , or similar.",
        "section": "Configure your application",
        "subsections": " Project structure  Discrete code bases Explicit source.root applications.yaml   Submodules Multi-app Routes Relationships  ",
        "image": "",
        "url": "/configuration/app/multi-app.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "c42490bfed602e1e255a9e23a03a5ba5",
        "title": "Transferring data to and from a Dedicated cluster",
        "description": "",
        "text": " Backing up staging and production files Platform.sh automatically creates a backup of the staging and production instances on a Dedicated cluster every six hours. However, those are only useful for a full restore of the environment and can only be done by the Platform.sh team. At times you’ll want to make a manual backup yourself. To create a manual ad-hoc backup of all files on the staging or production environment, use the standard rsync command. rsync -avzP \u0026lt;USERNAME\u0026gt;@\u0026lt;CLUSTER_NAME\u0026gt;.ent.platform.sh:pub/static/ pub/static/ That will copy all files from the pub/static directory on the production instance to the pub/static directory, relative to your local directory where you’re running that command. Backing up the staging and production database To backup your database to your local system you’ll need to get the database credentials to use. First, login to the cluster and run the following command: echo $PLATFORM_RELATIONSHIPS | base64 --decode | json_pp Which should give a JSON output containing something like this:  database  : [ {  path  :  main ,  service  :  mysqldb ,  rel  :  mysql ,  host  :  database.internal ,  ip  :  246.0.80.64 ,  scheme  :  mysql ,  cluster  :  jyu7wavyy6n6q-master-7rqtwti ,  username  :  user ,  password  :   ,  query  : {  is_master  : true },  port  : 3306 } ] The part you want is the user, password, and “path”, which means the DB name. Ignore the rest. Now, run the following command on your local computer: ssh \u0026lt;USERNAME\u0026gt;@\u0026lt;CLUSTER_NAME\u0026gt;.ent.platform.sh \u0026#39;mysqldump --single-transaction -u \u0026lt;user\u0026gt; -p\u0026lt;pass\u0026gt; -h localhost \u0026lt;dbname\u0026gt; | gzip\u0026#39; \u0026gt; database.gz That will run a mysqldump command on the server, compress it using gzip, and stream the output to a file named database.gz on your local computer. (If you’d prefer, bzip2 and xz are also available.) Synchronizing files from dev to staging/production To transfer data into either the staging or production environments, you can either download it from your Platform.sh Development environment to your local system first or transfer it directly between environments using SSH based tools (e.g. SCP, Rsync). First, set up SSH forwarding by default for Platform.sh domains. Then run platform ssh with the master branch checked out to connect to the master dev environment. Files are the easier data to transfer, and can be done with rsync. rsync -avzP pub/static/ \u0026lt;USERNAME\u0026gt;@\u0026lt;CLUSTER_NAME\u0026gt;.ent.platform.sh:pub/static/ Replace pub/static with the path to your files on system, such as web/sites/default/files/. Note that rsync is very picky about trailing / characters. Consult the rsync documentation for more that can be done with that command. Synchronizing the database from development to staging/production The database can be copied directly from the development environment to staging or production, but doing so requires noting the appropriate credentials first on both systems. First, login to the production environment over SSH: ssh \u0026lt;USERNAME\u0026gt;@\u0026lt;CLUSTER_NAME\u0026gt;.ent.platform.sh Once there, you can look up database credentials by running: echo $PLATFORM_RELATIONSHIPS | base64 --decode | json_pp Which should give a JSON output containing something like this: {  database  : [ {  password  :  abc123 ,  username  :  projectname ,  path  :  projectname ,  port  :  3306 ,  scheme  :  mysql ,  host  :  127.0.0.1 ,  query  : {  is_master  : true,  compression  : true } } ] } The part we want is the host, user, password, and the “path”, which is the database name. Ignore the rest. Now, in a separate terminal login to the development instance using platform ssh. Run the same echo command as above to get the credentials for the database on the development instance. (The JSON will be slightly different but again we’re only interested in the user, password, host, and “path”/database name). With the credentials from both databases we can construct a command that will export data from the dev server and write it directly to the Dedicated cluster’s server. mysqldump -u \u0026lt;dev_user\u0026gt; -p\u0026lt;dev_password\u0026gt; -h \u0026lt;dev_host\u0026gt; \u0026lt;dev_dbname\u0026gt; --single-transaction | ssh -C \u0026lt;USERNAME\u0026gt;@\u0026lt;CLUSTER_NAME\u0026gt;.ent.platform.sh \u0026#39;mysql -u \u0026lt;prod_user\u0026gt; -p\u0026lt;prod_password\u0026gt; -h \u0026lt;prod_host\u0026gt; \u0026lt;prod_dbname\u0026gt;\u0026#39; That will dump all data from the database as a stream of queries that will get run on the production database without ever having to create an intermediary file. The -C on the SSH command tells SSH to compress the connection to save time. (Be aware, this is a destructive operation that overwrites data. Backup first.)",
        "section": "Development",
        "subsections": " Backing up staging and production files Backing up the staging and production database Synchronizing files from dev to staging/production Synchronizing the database from development to staging/production  ",
        "image": "",
        "url": "/development/transfer-dedicated.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "3cbb1b5f9439dafe374ca7374a5992aa",
        "title": "Varnish",
        "description": "",
        "text": " Varnish is a popular HTTP proxy server, often used for caching. It is usually not needed on Platform.sh, as each project’s router provides an HTTP cache already and most more advanced use cases will use a CDN instead, both of which render Varnish redundant. However, it is possible to configure a Varnish instance as part of an application if Varnish-specific functionality is needed. Supported versions Grid Dedicated 5.6 6.0 None available How it works All incoming requests still go through the environment’s router first. When using Varnish, a Varnish service sits between the router and the application server or servers. web -\u0026gt; router -\u0026gt; varnish -\u0026gt; application -\u0026gt; application2 Configuration Add a Varnish service Add the following to your .platform/services.yaml file: proxy:type:varnish:6.0relationships:application:\u0026#39;app:http\u0026#39;configuration:vcl:!includetype:stringpath:config.vcl In the relationships block, define a relationship (application) to the application container (app) using the http endpoint. That allows Varnish to talk to the application container. The configuration block is required, and must reference a VCL file (here config.vcl). The file name is relative to the .platform directory. Create a VCL template file The VCL file you provide has three specific requirements over and above the VCL syntax itself. You MUST NOT define a vcl_init() function. Platform.sh will auto-generate that function based on the relationships you define. In particular, it will define a “backend” for each relationship defined in services.yaml, named the same as the relationship. You MUST NOT include the preamble at the beginning of the file, specifying the VCL version. That will be auto-generated as well. You CAN add imports, but not std and directors. You MUST specify the backend to use in vcl_recv(). If you have a single app container/relationship/backend, it’s just a single line. If you want to split requests to different relationships/backends based on some rule then the logic for doing so should be incorporated into the vcl_recv() function. The absolute bare minimum VCL file is: sub vcl_recv { set req.backend_hint = application.backend(); } Where application is the name of the relationship defined in services.yaml. (If the relationship was named differently, use that name instead.) If you have multiple applications fronted by the same Varnish instance then you will need to include logic to determine to which application a request is forwarded. For example: varnish:type:varnish:6.0relationships:blog:\u0026#39;blog:http\u0026#39;main:\u0026#39;app:http\u0026#39;configuration:vcl:!includetype:stringpath:config.vcl# config.vcl sub vcl_recv { if (req.url ~  ^/blog/ ) { set req.backend_hint = blog.backend(); } else { set req.backend_hint = main.backend(); } } This configuration will direct all requests to a URL beginning with a /blog/ path to the application on the relationship blog, and all other requests to the application on the relationship main. Besides that, the VCL file, including the vcl_recv() function, can be arbitrarily complex to suit the needs of the project. That includes additional include directives if appropriate. See the Varnish documentation for more details on the functionality offered by Varnish. Note: A misconfigured VCL file can result in incorrect, often mysterious and confusing behavior. Platform.sh does not provide support for VCL configuration options beyond the basic connection logic documented here. Route incoming requests to Varnish To enable Varnish now, edit the .platform/routes.yaml file to point to the Varnish service you just created. You also need to disable the router cache as it is now entirely redundant with Varnish. For example:  https://{default}/ :type:upstreamupstream: varnish:http cache:enabled:false That will map all incoming requests to the Varnish service rather than the application. Varnish will then, based on the VCL file, forward requests to the application as appropriate. Modules Platform.sh supports a number of optional modules you can include in your VCLs, namely: cookie header saintmode softpurge tcp var vsthrottle xkey To use in your VCL, add an import such as: import xkey; Circular relationships At this time Platform.sh does not support circular relationships between services or applications. That means you cannot add a relationship in your .platform.app.yaml that points to the Varnish service. If you do so then one of the relationships will be skipped and the connection will not work. This limitation may be lifted in the future. Stats endpoint The Varnish service also offers an http\u0026#43;stats endpoint, which provides access to some Varnish analysis and debugging tools. To access it, from a dedicated app container add the following to .platform.app.yaml: relationships:varnishstats: proxy:http\u0026#43;stats  You can then access the varnishstats relationship over HTTP at the following paths to get diagnostic information: /: returns the error if generating the VCL failed with an error /config: returns the generated VCL /stats: returns the output of varnishstat /logs: returns a streaming response of varnishlog Note that because of the circular relationship issue noted above this cannot be done on the application that Varnish is forwarding to. It will need to be run on a separate application container.",
        "section": "Configure services",
        "subsections": " Supported versions How it works Configuration  Add a Varnish service Create a VCL template file Route incoming requests to Varnish   Modules Circular relationships Stats endpoint  ",
        "image": "",
        "url": "/configuration/services/varnish.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "2e3b4ae352691be6251ee1a7231ca714",
        "title": "Public IP addresses",
        "description": "",
        "text": " Platform.sh regions reach the outside through a limited number of IP addresses. Use the inbound IP addresses if you have a corporate firewall which blocks outgoing SSH connections. In that case, simply add our IP addresses for inbound traffic below to your whitelist. Note: These IP addresses are stable, but not guaranteed to never change. Prior to any future change, all affected customers will receive ample warning. Europe West (eu.platform.sh) Outbound IPs: 54.72.94.105 54.76.137.67 54.76.137.94 Inbound IPs (gw.eu.platform.sh): 54.76.137.79 54.76.137.151 54.76.136.188 West 2 (eu-2.platform.sh) Outbound IPs: 52.208.123.9 52.214.63.84 52.30.200.164 Inbound IPs (gw.eu-2.platformsh.site): 34.248.104.12 34.241.191.143 52.210.208.94 West 4 (eu-4.platform.sh) Outbound IPs: 18.200.158.188 18.200.157.200 18.200.184.206 Inbound IPs (gw.eu-4.platformsh.site): 52.215.88.119 52.208.179.40 18.200.179.139 Germany 2 (de-2.platform.sh) (Data Location Guarantee) Outbound IPs: 35.246.248.138 35.246.184.45 35.242.229.239 Inbound IP (gw.de-2.platformsh.site): 35.246.248.138 35.246.184.45 35.242.229.239 France 1 (fr-1.platform.sh) Outbound IPs: 90.84.47.148 90.84.46.222 90.84.46.40 Inbound IPs (gw.fr-1.platformsh.site): 90.84.47.148 90.84.46.222 90.84.46.40 France 2 (ovh-fr-2.platform.sh) Outbound IPs: 51.178.62.146 51.178.61.63 51.178.56.77 Inbound IPs (gw.ovh-fr-2.platformsh.site): 51.178.62.146 51.178.61.63 51.178.56.77 United Kingdom 1 (uk-1.platform.sh) Outbound IPs: 35.242.142.110 35.189.126.202 35.242.183.249 Inbound IPs (gw.uk-1.platformsh.site): 35.242.142.110 35.189.126.202 35.242.183.249 United States East (us.platform.sh) Outbound IPs: 54.88.149.31 54.209.114.37 54.210.53.51 Inbound IPs (gw.us.platform.sh): 54.210.49.244 54.210.55.162 54.88.225.116 East 2 (us-2.platform.sh) Outbound IPs: 34.238.64.193 52.4.246.137 54.157.66.30 Inbound IPs (gw.us-2.platformsh.site): 34.226.46.235 34.238.11.122 54.89.106.200 Canada Outbound IPs: 35.182.24.224 52.60.213.255 35.182.220.113 Inbound IPs: 35.182.174.169 35.182.59.77 52.60.219.22 Australia (au.platform.sh) Outbound IPs: 13.55.135.0 13.54.121.225 13.55.215.151 Inbound IPs (gw.au.platformsh.site): 13.54.88.239 13.55.140.143 13.54.222.56",
        "section": "Development",
        "subsections": " Europe  West (eu.platform.sh) West 2 (eu-2.platform.sh) West 4 (eu-4.platform.sh) Germany 2 (de-2.platform.sh) (Data Location Guarantee) France 1 (fr-1.platform.sh) France 2 (ovh-fr-2.platform.sh) United Kingdom 1 (uk-1.platform.sh)   United States  East (us.platform.sh) East 2 (us-2.platform.sh)   Canada Australia (au.platform.sh)  ",
        "image": "",
        "url": "/development/public-ips.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "cdc8658cf343ae74328b018adda933dc",
        "title": "Upgrading",
        "description": "",
        "text": " Changes in version 2019.05 The !archive tag in YAML has been un-deprecated, and is now favored over the !include option. !include is still available for other include types (yaml, binary, and string). Changes in version 2017.11 (2017-11-09) The !archive tag in YAML files is now deprecated in favor of the more generic !include . For example, the following services.yaml snippet: mysearch:type:solr:6.3disk:1024configuration:core_config:!archive myconfdir Can now be written as: mysearch:type:solr:6.3disk:1024configuration:core_config:!includetype:archivepath: myconfdir  The syntax for the mounts key in .platform.app.yaml has changed. Rather than a parsed string, the value of each mount is a multi-key definition . That is, the following example: mounts: tmp :  shared:files/tmp  logs :  shared:files/logs Can now be written as: mounts:tmp:source:localsource_path:tmplogs:source:localsource_path:logs Changes in version 2016.6 (2016-11-18) Application containers now include the latest LTS version of Node.js, 6.9.1. The previously included version was 4.6.1. Composer was briefly called with --no-dev, but as of 2016-11-21 this change has been reverted, because of the unintended effect it had on projects using the Symfony framework. Changes in version 2016.5 As of October 2016, the default behaviour of the expires key, which controls client-side caching of static files, has changed. Previously, if the key was unset, the Expires and Cache-Control HTTP headers were left unset in the response, which meant that client side caching behaviour was left undefined. To ensure consistent behaviour that doesn’t depend on which browser the client is using, the new default behaviour is to set these headers to values that disable client-side caching. This change only affects static files served directly by the web server. Responses served from passthru URLs continue to use whatever caching headers were set by the application.. To enable caching on your static files, make sure you include an expires key in your web configuration , as shown below: web:locations: / :root: public passthru: /index.php index:- in version 2016.4 As of July 2016, we no longer create default configuration files if one is not provided. The defaults we used to provide were tailored specifically for Drupal 7, which is now a legacy-support version with the release of Drupal 8 and not especially useful for non-Drupal or non-PHP sites. They also defaulted to software versions that are no longer current and recommended. Instead, you must provide your own .platform.app.yaml, .platform/routes.yaml, and .platform/services.yaml files. Additionally, a version for a language or service should always be specified as well. That allows you to control when you upgrade from one version to another without relying on a network default. The previous default files, for reference, are: .platform.app.yaml name:phptype: php:5.4 build:flavor: drupal access:ssh:contributorrelationships:database: mysql:mysql solr: solr:solr redis: redis:redis web:document_root: / passthru: /index.php disk:2048mounts: public/sites/default/files :  shared:files/files  tmp :  shared:files/tmp  private :  shared:files/private crons:drupal:spec: */20 * * * * cmd: cd public ; drush core-cron .platform/routes.yaml  http://{default}/ :type:upstreamupstream: php:http cache:enabled:truessi:enabled:false http://www.{default}/ :type:redirectto: http://{default}/ .platform/services.yaml mysql:type:mysql:5.5disk:2048redis:type:redis:2.8solr:type:solr:3.6disk:1024Changes in version 2016.3 As we are aiming to always provide you more control and flexibility on how to deploy your applications, the .platform.app.yaml format has been greatly improved. It is now way more flexible, and also much more explicit to describe what you want to do. The web key is now a set of locations where you can define very precisely the behavior of each URL prefix. Note, we no longer move your application from “/” to “public/” automatically if the new format is adopted. If you are using Drupal, move all of your Drupal files into “public/” in the Git repository. Old format: web:document_root: / passthru: /index.php index_files:-  index.php expires:300whitelist:- format: web:locations: / :root: public passthru: /index.php index:- compatibility Of course, we alway keep backward compatibility with the previous configuration format. Here is what happens if you don’t upgrade your configuration: # The following parameters are automatically moved as a  /  block in the#  locations  object, and are invalid if there is a valid  locations  block.document_root: /public # Converted to [locations][/][root]passthru: /index.php # Converted to [locations][/][passthru]index_files:- index.php# Converted to [locations][/][index]whitelist:[]# Converted to [locations][/][rules]blacklist:[]# Converted to [locations][/][rules]expires:3d# Converted to [locations][/][expires]Changes in version 2015.7 The .platform.app.yaml configuration file now allows for a much clearer syntax, which you can (and should) start using now. The old format had a single string to identify the ‘toolstack’ you use: toolstack: php:drupal The new syntax allows to separate the concerns of what language you are running and the build process that is going to happen on deployment: type:phpbuild:flavor:drupalCurrently we only support php in the ‘type’ key. Current supported build flavors are drupal, composer and symfony. Changes in version 2014.9 This version introduces changes in the configuration files format. Most of the old configuration format is still supported, but customers are invited to move to the new format. For an example upgrade path, see the Drupal 7.x branch of the Platformsh-examples repository on GitHub. Configuration items for PHP that previously was part of .platform/services.yaml are now moved into .platform.app.yaml, which gains the following top-level items: name: should be  php  relationships, access and disk: should be the same as the relationships key of PHP in .platform/services.yaml Note that there is now a sane default for access (SSH access to PHP is granted to all users that have role “collaborator” and above on the environment) so most customers can now just omit this key in .platform.app.yaml. In addition, version 1.7.0 now has consistency checks for configuration files and will reject git push operations that contain configuration files that are invalid. In this case, just fix the issues as they are reported, commit and push again.",
        "section": "Configure your application",
        "subsections": " Changes in version 2019.05 Changes in version 2017.11 (2017-11-09) Changes in version 2016.6 (2016-11-18) Changes in version 2016.5 Changes in version 2016.4  .platform.app.yaml .platform/routes.yaml .platform/services.yaml   Changes in version 2016.3  Backward compatibility   Changes in version 2015.7 Changes in version 2014.9  ",
        "image": "",
        "url": "/configuration/app/upgrading.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "e719452c6e42346a004592fc5e2ce067",
        "title": "[Beta] Outbound firewall",
        "description": "",
        "text": " In some situations, compliance regulations may require you to limit outbound traffic from your application. The firewall property allows you to do so. This setting has no impact on inbound requests to your application. For that, use the environment access control settings in the Management Console. Note: The outbound firewall is currently in Beta. While the syntax is not expected to change, some behavior might in the future. Syntax The firewall property defines one or more whitelist entries for outbound requests. Its basic syntax is as follows: firewall:outbound:- protocol:tcpips:[ 1.1.1.1/32 ]ports:[443]- protocol:tcpips:[ 1.2.3.4/32 ]ports:[443]The above example allows two outbound rules over TCP. All other outbound requests will be blocked and will time out eventually (usually after 30 seconds). If no rules are specified, the default firewall configuration is equivalent to: firewall:outbound:- protocol:tcpips:[ 0.0.0.0/0 ]That is, all outbound TCP traffic is allowed on all ports (aside from port 25, which is always blocked without exception). In the majority of cases the default is sufficient for most applications. Options Each firewall rule has three configuration values. At least one of ips or ports is required, but both may also be specified. protocol The default and only legal value for the protocol is tcp. Outbound UDP ports are not allowed. As a result this property can be omitted in virtually every circumstance. ips This property is an array of IP addresses in CIDR notation . CIDR allows you to specify a range of IP addresses in a compact format, using a bitmask. Most commonly the bitmask is 8, 16, or 32 but that is not required. For example, 1.2.3.4/8 will match any IP address whose first 8 bits match 1.2.3.4, which corresponds to the first segment. Therefore it will allow 1.*.*.*. In comparison, 1.2.3.4/24 will allow 1.2.3.*. A mask of 32 will match only the IP address specified, so to whitelist a single specific IP you must write 1.2.3.4/32. IP Address Guide has a useful CIDR format calculator. If no ports property is specified, requests to any port on the specified IP addresses are permitted. ports This property is an array of ports in the range 1 to 65535 that are allowed. For example, [80, 443] will only allow requests to the specified IPs on ports 80 and 443 (typically HTTP and HTTPS, respectively). Requests to any other port will be blocked. If not specified, requests to a given IP may be to any port. If no ips property is specified, requests to any IP address are permitted on the specified port(s). Multiple rules It is possible to define an arbitrary number of whitelist firewall rules, as in the example above. If multiple rules are specified, a given outbound request will be allowed if it matches ANY of the defined rules. That means that, for this configuration: firewall:outbound:- ips:[ 1.2.3.4/32 ]ports:[443]- ports:[80]Requests to port 80 on any IP will be allowed, and requests to 1.2.3.4 on either port 80 or 443 will be allowed, even though the first rule only lists port 443. Usage considerations Be aware that many services your application may wish to connect to will be using a domain name that is not on a fixed IP address, or is load-balanced between multiple IP addresses. You will need to contact the administrator of that service in order to determine the correct IP addresses to whitelist. Also be aware that many services are behind a Content Delivery Network (CDN). For most CDNs, routing is done via domain name, not IP address, so thousands of domain names may share the same public IP addresses at the CDN. If you whitelist the IP address of a CDN, you will in most cases be whitelisting many or all of the other customers hosted behind that CDN. That has security implications and limits the usefulness of this configuration option.",
        "section": "Configure your application",
        "subsections": " Syntax Options  protocol ips ports   Multiple rules Usage considerations  ",
        "image": "",
        "url": "/configuration/app/firewall.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "0dc6292b6223331a466ea810b8f50743",
        "title": "Frequently Asked Questions (FAQ)",
        "description": "",
        "text": " What is the difference between a Platform, a Project and an Environment? Platform or Platform.sh is the infrastructure which is running all your projects. A project is the site that you’re working on. Each project corresponds to one Git repository. A project may contain multiple applications that will run in their own isolated containers. Each branch of a project may be deployed in its own environment. An environment is a standalone copy of your site, complete with code, data, and running services. The master branch is the production environment, while any other branch can be setup as an otherwise identical testing environment. How can I cancel my subscription? If you want to delete your project and cancel your subscription, simply go to your user profile and click on “Edit plan” on the project you want to delete. Then you can click on the link: “delete your Platform.sh plan”. This will delete your project and stop invoicing for this project. If you have multiple projects, your subscription will continue until you don’t have any projects left. Does branching an environment duplicate services? Yes! Branching an environment creates an exact copy (snapshot) of the parent environment, containing the files, the database, and code. Each environment runs independently of every other, so if you have four active environments then you have four copies of your application, four copies of your database, four copies of your files, etc. Do you have a local writable file-system? Yes! Platform.sh supports non-ephemeral storage. When you configure your application you can tell it what directories you want to be read/write. (These are called mounts .) These will be mounted on a distributed file system (which is transparent for you). When you backup your environment they will be backed up as well. When you create a new staging environment, these mounts will be cloned with the rest of your data. What happens if I push a local branch to my project? If you push a local branch that you created with Git, you create what is called an inactive environment, that is, an environment that is not deployed. This means there won’t be any services attached to this branch. You are able to convert an inactive environment into an active environment and vice versa back from the environment configuration page or using the CLI with platform environment:activate. How does Master (the live site) scale? The master environment gets a pool of resources based on your plan size, which is then split up between the applications and services you have defined. (For example, PHP 40%, MySQL 30%, Redis 10%, Solr 20%, etc). Increasing your plan size will increase the pool of CPU and RAM that gets split between each container. All containers on development plans are “small” containers. See the sizing configuration page for more details. What exactly am I SSHing into? The platform ssh command allows you to log into your application container (where your PHP app or Node app or Java app is running). It is a fully running Linux environment, but almost all of the disk will be read-only, with the exception of mounts you have defined. Can I edit a quick fix on a Platform environment without triggering a rebuild? No. Changes to the code can only be made through deploying new Git commits. That ensures that “hot patches” don’t get lost in the net update, that all changes are auditable, and that if a security break-in happens the attacker still cannot modify your application code. What do I see when I push code? When you git push new code, Platform.sh rebuilds and redeploys the application. What shows on the command line is the output of your build process (composer, pip, bundler, etc. plus your own build hook) followed by the deploy process. It ends with a description of what was just deployed and the URLs that are now active. To supress the output, run platform push -W. The -W means --no-wait, and will disconnect the connection once the commits are pushed so that you can continue to use your local terminal. The exact same output is also available in the Web Management Console. What Linux distribution is Platform.sh using? Platform.sh is built on Debian. If I choose the Development plan, can I use that plan for production? No. The Development plan provides all the tools to build your website. You can create as many development profiles as you wish for yourself and for your team. However, it does not allow for full production-level resources on the master branch and does not allow you to configure a custom domain name. Once your project is complete and ready for production, you can choose another plan to go live. These plans are available on the pricing page . I am getting weird errors when I push (something with paramiko..) Please validate the syntax of your YAML file. Don’t use tabs. If all fails, contact support. Which geographic zones does Platform.sh cover? Platform.sh works with multiple cloud infrastructure providers, including Amazon Web Services, Microsoft Azure, and Orange. We offer public regions in several parts of the world: USA (East Coast), Canada (East Coast), Europe (Dublin), Europe (Germany), and Australia (Sydney). Dedicated projects can deploy production to any public AWS or Azure region. Why did you choose the .sh extension for your domain? ‘sh’ is the short version of shell. According to Wikipedia™, in computing, a shell is a user interface for access to an operating system’s services. Generally, operating system shells use either a command-line interface (CLI) or graphical user interface (GUI). This is exactly what Platform.sh is about: Giving developers tools to build, test, deploy, and run great websites! “.sh” is also the TLD for Saint Helena that looks like a lovely island, and whose motto is: “Loyal and Unshakeable” which we also strive to be. IDE Specific Tips MAMP pro: In order for MAMP to work well with the symlinks created by the Platform.sh CLI , add the following to the section under Hosts \u0026gt; Advanced called “Customized virtual host general settings.” For more details visit MAMP Pro documentation page . \u0026lt;Directory\u0026gt; Options FollowSymLinks AllowOverride All \u0026lt;/Directory\u0026gt; Note: When you specify your document root, MAMP will follow the symlink and substitute the actual build folder path. This means that when you rebuild your project locally, you will need to repoint the docroot to the symlink again so that it will refresh the build path. Do you support two-factor authentication? Yes, and encourage its use. To do so please go to your Account Settings on our Account site . Then click on the left tab called Security which will propose you to enable TFA Application.",
        "section": "Development",
        "subsections": " What is the difference between a Platform, a Project and an Environment? How can I cancel my subscription? Does branching an environment duplicate services? Do you have a local writable file-system? What happens if I push a local branch to my project? How does Master (the live site) scale? What exactly am I SSHing into? Can I edit a quick fix on a Platform environment without triggering a rebuild? What do I see when I push code? What Linux distribution is Platform.sh using? If I choose the Development plan, can I use that plan for production? I am getting weird errors when I push (something with paramiko..) Which geographic zones does Platform.sh cover? Why did you choose the .sh extension for your domain? IDE Specific Tips Do you support two-factor authentication?  ",
        "image": "",
        "url": "/development/faq.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "a8a30a7691b7a83ed28fdbaaa271fdfc",
        "title": "Troubleshooting",
        "description": "",
        "text": " Force a redeploy There are times where you might want to trigger a redeployment of your application. That can be done with the following command: platform redeploy Do not trigger a redeploy if there are builds in a “Pending” state, as these will block deployment. Wait for those builds to complete. Clear the build cache In rare circumstances the build cache, used to speed up the build process, may become corrupted. That may happen if, for example, code is being downloaded from a 3rd party language service like Packagist or NPM while that service is experiencing issues. To flush the build cache entirely run the following command: platform project:clear-build-cache That will wipe the build cache for the current project entirely. Naturally the next build for each environment will likely be longer as the cache rebuilds. HTTP responses 502 Bad Gateway or 503 Service Unavailable These errors indicate your application (or application runner, like PHP-FPM) is crashing or unavailable. Typical causes include: Your .platform.app.yaml configuration has an error and the process is not starting or requests are not able to be forwarded to it correctly. Check your web.commands.start entry or that your passthru configuration is correct. The amount of traffic coming to your site exceeds the processing power of your application. Certain code path(s) in your application are too slow and timing out. A PHP process is crashing because of a segmentation fault (see below). A PHP process is killed by the kernel out-of-memory killer (see below). Error provisioning the new certificate One reason Let’s Encrypt certificates may fail to provision on your environments has to do with the 64 character limit Let’s Encrypt places on URLs. If the names of your branches are too long, the Platform.sh generated environment URL will go over this limit, and the certificate will be rejected. See Let’s Encrypt limits and branch names for a more detailed breakdown of this issue. Total disk usage exceeds project maximum One of the billable parameters in your project’s settings is Storage. This global storage pool is allocated among the various services and application containers in your project via the disk parameter. The sum of all disk parameters in your project’s YAML config files must be less than or equal to the global project storage number. Error: Resources exceeding plan limit; disk: 8192.00MB \u0026gt; 5120.00MB; try removing a service, or add more storage to your plan This means that you have allocated, for example, disk: 4096 in a MySQL service in services.yaml and also disk: 4096 in the .platform.app.yaml for your application, while only having the minimum default of 5GB storage for your project as a whole. The solution is either to lower the disk parameters to within the limits of 5GB of storage, or raise the global storage parameter on your project’s settings to at least 10GB. Because storage is a billable component of your project, only the project’s owner can make this change. Low disk space When you receive a low-disk space notification for your application container: Check your application’s disk space Run platform ssh within your project folder to login to the container’s shell. Then use the df command to check the available writable space for your application. df -h -x tmpfs -x squashfs | grep -v /run/shared This command will show the writable mounts on the system, similar to: Filesystem Size Used Avail Use% Mounted on /dev/mapper/platform-syd7waxqy4n5q--master--7rqtwti----app 2.0G 37M 1.9G 2% /mnt /dev/mapper/platform-tmp--syd7waxqy4n5q--master--7rqtwti----app 3.9G 42M 3.8G 2% /tmp The first line shows the storage device that is shared by all of your persistent disk mounts . All defined mounts use a common storage pool. In this example, the application container has allocated 2 GB of the total disk space. Of those 2GB, 2% (37 MB) is used by all defined mounts. The second line is the operating system temporary directory, which is always the same size. While you can write to the /tmp directory files there are not guaranteed to persist and may be deleted on deploy. Increase the disk space available The sum of all disk keys defined in your project’s .platform.app.yaml and .platform/services.yaml files must be equal or less than the available storage in your plan. Buy extra storage for your project Each project comes with 5GB of Disk Storage available to each environment. To increase the disk space available for your project, click on “Edit Plan” to increase your storage in bulks of 5GB. See Extra Storage for more information. Increase your application and services disk space Once you have enough storage available, you can increase the disk space allocated for your application and services using disk keys in your .platform.app.yaml and .platform/services.yaml. Check the following resources for more details: Application’s disk space Services’ disk space Check your database disk space For a MariaDB database, the command platform db:size will give approximate disk usage as reported by MariaDB. However, be aware that due to the way MySQL/MariaDB store and pack data this number is not always accurate, and may be off by as much as 10 percentage points. \u0026#43;--------------\u0026#43;--------\u0026#43; | Property | Value | \u0026#43;--------------\u0026#43;--------\u0026#43; | max | 2048MB | | used | 189MB | | percent_used | 9% | \u0026#43;--------------\u0026#43;--------\u0026#43; For the most reliable disk usage warnings, we strongly recommend all customers enable Health notifications on all projects. That will provide you with a push-notification through your choice of channel when the available disk space on any service drops too low. No space left on device During the build hook, you may run into the following error depending on the size of your application: W: [Errno 28] No space left on device: ... The cause of this issue has to do with the amount of disk provided to the build container before it is deployed. Application images are restricted to 4 GB during build, no matter how much writable disk has been set aside for the deployed application. Some build tools (yarn/npm) store cache for different versions of their modules. This can cause the build cache to grow over time beyond the maximum of 4GB. Try clearing the build cache and redeploying. In most cases, this will resolve the issue. If for some reason your application requires more than 4 GB during build, you can open a support ticket to have this limit increased. The most disk space available during build still caps off at 8 GB in these cases. MySQL lock wait timeout If you receive MySQL error messages like this: SQLSTATE[HY000]: General error: 1205 Lock wait timeout exceeded; This means a process running in your application acquired a lock from MySQL for a long period of time. That is typically caused by one of the following: There are multiple places acquiring locks in different order. For example, code path 1 first locks record A and then locks record B. Code path 2, in contrast, first locks record B and then locks record A. There is a long running background process executed by your application that holds the lock until it ends. If you’re using MariaDB 10\u0026#43; , you can use the SQL query SHOW FULL PROCESSLIST to list DB queries waiting for locks. Find output like the following, and start debugging. \u0026lt; skipped \u0026gt; Command: Query Time: ... State: Waiting for table metadata lock Info: SELECT ... \u0026lt; skipped \u0026gt; To find active background processes, run ps aufx on your application container. Also, please make sure that locks are acquired in a pre-defined order and released as soon as possible. MySQL: definer/invoker of view lack rights to use them There is a single MySQL user, so you can not use “DEFINER” Access Control mechanism for Stored Programs and Views. When creating a VIEW, you may need to explicitly set the SECURITY parameter to INVOKER: CREATE OR REPLACE SQL SECURITY INVOKER VIEW `view_name` AS SELECT MySQL server has gone away Disk space issues Errors such as “PDO Exception ‘MySQL server has gone away’” are usually simply the result of exhausting your existing diskspace. Be sure you have sufficient space allocated to the service in .platform/services.yaml . The current disk usage can be checked using the CLI command platform db:size. Because of issues with the way InnoDB reports its size, this can out by up to 20%. As table space can grow rapidly, it is usually advisable to make your database mount size twice the size reported by the db:size command. You are encouraged to add a low-disk warning notification to proactively warn of low disk space before it becomes an issue. Worker timeout Another possible cause of “MySQL server has gone away” errors is a server timeout. MySQL has a built-in timeout for idle connections, which defaults to 10 minutes. Most typical web connections end long before that is ever approached, but it’s possible that a long-running worker may idle and not need the database for longer than the timeout. In that case the same “server has gone away” message may appear. If that’s the case, the best way to handle it is to wrap your connection logic in code that detects a “server has gone away” exception and tries to re-establish the connection. Alternatively, if your worker is idle for too long it can self-terminate. Platform.sh will automatically restart the worker process, and the new process can establish its own new database connection. Packet size limitations Another cause of the “MySQL server has gone away” errors can be the size of the database packets. If that is the case, the logs may show warnings like “Error while sending QUERY packet” before the error. One way to resolve the issue is to use the max_allowed_packet parameter described above . ERROR: permission denied to create database The provided user does not have permission to create databases. The database is created for you and can be found in the path field of the $PLATFORM_RELATIONSHIPS environment variable. “Read-only file system” error Everything will be read-only, except the writable mounts you declare. Writable mounts are there for your data: for file uploads, logs and temporary files. Not for your code. In order to change code on Platform.sh you have to go through Git. This is what gives you all of the benefits of having repeatable deployments, consistent backups, traceability, and the magically fast creation of new staging/dev environments. In Platform.sh, you cannot just “hack production”. It is a constraint, but it is a good constraint. During the build phase of your application, the main filesystem is writable. So you can do whatever you want (e.g. compile code or generate anything you need). But during and after the deploy phase , the main filesystem will be read-only. RootNotFoundException from the CLI If you check out a project via Git directly and not using the platform get command, you may end up with the CLI unable to determine what project it’s in. If you run a CLI command from within the project directory you’ve checked out but get an error like this: [RootNotFoundException] Project root not found. This can only be run from inside a project directory. Then the CLI hasn’t been able to determine the project to use. To fix that, run: platform project:set-remote \u0026lt;project_id\u0026gt; where \u0026lt;project_id\u0026gt; is the random-character ID of the project. That can be found by running platform projects from the command line to list all accessible projects. Alternatively, it can be found in the management console after the platform get command shown or in the URL of the management console or project domain. “File not found” in Drupal If you see a bare “File not found” error when accessing your Drupal site with a browser, this means that you’ve pushed your code as a vanilla project but no index.php has been found. Make sure your repository contains an index.php file in the web location root , or that your Drush make files are properly named. PHP-specific error messages server reached max_children You may see a line like the following in the /var/log/app.log file: WARNING: [pool web] server reached max_children setting (2), consider raising it That indicates that the server is receiving more concurrent requests than it has PHP processes allocated, which means some requests will have to wait until another finishes. In this example there are 2 PHP processes that can run concurrently. Platform.sh sets the number of workers based on the available memory of your container and the estimated average memory size of each process. There are two ways to increase the number of workers: Adjust the worker sizing hints for your project. Upgrade your subscription on Platform.sh to get more computing resources. To do so, log into your account and edit the project. Execution timeout If your PHP application is not able to handle the amount of traffic or it is slow, you should see log lines from /var/log/app.log like any of the below: WARNING: [pool web] child 120, script \u0026#39;/app/public/index.php\u0026#39; (request:  GET /index.php ) execution timed out (358.009855 sec), terminating That means your PHP process is running longer than allowed. You can adjust the max_execution_time value in php.ini, but there is still a 5 minute hard cap on any web request that cannot be adjusted. The most common cause of a timeout is either an infinite loop (which is a bug that you should fix) or the work itself requires a long time to complete. For the latter case, you should consider putting the task into a background job. The following command will identify the 20 slowest requests in the last hour, which can provide an indication of what code paths to investigate. grep $(date \u0026#43;%Y-%m-%dT%H --date=\u0026#39;-1 hours\u0026#39;) /var/log/php.access.log | sort -k 4 -r -n | head -20 If you see that the processing time of certain requests is slow (e.g. taking more than 1000ms), you may wish to consider using a profiler like Blackfire to debug the performance issue. Otherwise, you may check if the following options are applicable: Find the most visited pages and see if they can be cached and/or put behind a CDN. You may refer to how caching works . Upgrade your subscription on Platform.sh to get more computing resources. To do so, log into your account and edit the project subscription. PHP process crashed If your PHP process crashed with a segmentation fault, you should see log lines in /var/log/app.log like below: WARNING: [pool web] child 112 exited on signal 11 (SIGSEGV) after 7.405936 seconds from start This is complicated, either a PHP extension is hitting a segmentation fault or your PHP application code is crashing. You should review recent changes in your application and try to find the cause of it, probably with the help of XDebug. PHP process is killed If your PHP process is killed by the kernel, you should see log lines in /var/log/app.log like this: WARNING: [pool web] child 429 exited on signal 9 (SIGKILL) after 50.938617 seconds from start That means the memory usage of your container exceeds the limit allowed on your plan so the kernel kills the offending process. You should try the following: Check if the memory usage of your application is expected and try to optimize it. Use sizing hints to reduce the amount of PHP workers which reduces the memory footprint. Upgrade your subscription on Platform.sh to get more computing resources. To do so, log into your account and edit the project. Stuck build or deployment If you see a build or deployment running longer than expected, that may be one of the following cases: The build is blocked by a process in your build hook. The deployment is blocked by a long running process in your deploy hook. The deployment is blocked by a long running cron job in the environment. The deployment is blocked by a long running cron job in the parent environment. To determine if your environment is being stuck in the build or the deployment, you can look at the build log available in the management console. If you see a line similar to the following: Re-deploying environment w6ikvtghgyuty-drupal8-b3dsina. It means the build has completed successfully and the system is trying to deploy. If that line never appears then it means the build is stuck. For a blocked build (when you don’t find the Re-deployment environment ... line), create a support ticket to have the build killed. In most regions the build will self-terminate after one hour. In older regions (US and EU) the build will need to be killed by our support team. When a deployment is blocked, you should try the following: Use SSH to connect to your environment. Find any long-running cron jobs or deploy hooks on the environment by running ps afx. Once you have identified the long running process on the environment, kill it with kill \u0026lt;PID\u0026gt;. PID stands for the process id shown by ps afx. If you’re performing “Sync” or “Activate” on an environment and the process is stuck, use SSH to connect to the parent environment and identify any long running cron jobs with ps afx. Kill the job(s) if you see any. Slow or failing build or deployment Builds that take long time or fail is a common problem. Most of the time it’s related to an application issue and they can be hard to troubleshoot without guidance. Here are a few tips that can help you solve the issues you are experiencing. Check for errors in the logs Invisible errors during the build and deploy phase can cause increased wait times, failed builds and other problems. Investigating each log and fixing errors is essential. Related documentation: Accessing logs Build and deploy hooks Hooks are frequently the cause of long build time. If they run into problem they can cause the build to fail or hang indefinitely. The build hook can be tested in your local environment. Because the deployed environment on Platform.sh is read-only the build hooks cannot be rerun there. Deploy hooks can be tested either locally or by logging into the application over SSH and running them there. They should execute safely but be aware that depending on what your scripts are doing they may have an adverse impact on the running application (e.g., flushing all caches). Furthermore, you can test your hooks with these Linux commands to help figure out any problems: time $cmd # Print execution time strace -T $cmd # Print a system call report Related documentation: Build and deploy hooks Cron jobs Containers cannot be shutdown while long-running tasks are active. That means long-running cron jobs will block a container from being shut down to make way for a new deploy. For that reason, make sure your custom cron jobs execution times are low and that they are running properly. Be aware that cron jobs may invoke other services in unexpected ways, which can increase execution time. note Drupal’s drush core-cron run installed module’s cron task. Those can be, for example; evicting invalid cache, updating database records, regenerating assets. Be sure to frequently benchmark the drush core-cron command in all your environments, as it is a common source of performance issues. Related documentation: Cron and scheduled tasks",
        "section": "Development",
        "subsections": " Force a redeploy Clear the build cache HTTP responses 502 Bad Gateway or 503 Service Unavailable Error provisioning the new certificate Total disk usage exceeds project maximum Low disk space  Check your application\u0026rsquo;s disk space Increase the disk space available Check your database disk space   No space left on device MySQL lock wait timeout MySQL: definer/invoker of view lack rights to use them MySQL server has gone away  Disk space issues Worker timeout Packet size limitations   ERROR: permission denied to create database \u0026ldquo;Read-only file system\u0026rdquo; error RootNotFoundException from the CLI \u0026ldquo;File not found\u0026rdquo; in Drupal PHP-specific error messages  server reached max_children Execution timeout PHP process crashed PHP process is killed   Stuck build or deployment Slow or failing build or deployment  Check for errors in the logs Build and deploy hooks Cron jobs    ",
        "image": "",
        "url": "/development/troubleshoot.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "eb0a25ed94126292f8adc0b75980807c",
        "title": "Bitbucket",
        "description": "",
        "text": " The Bitbucket integration allows you to manage your Platform.sh environments directly from your Bitbucket repository. Set up an OAuth consumer You can integrate your Bitbucket repositories with Platform.sh by creating an OAuth consumer for your Workspace. Go to your Bitbucket Workspace and click “Settings”. Under “APPS AND FEATURES” click “OAuth Consumers”. Click the “Add consumer” button. Fill out the information for the consumer. In order for the integration to work correctly, it’s required that you include: Name: Give the consumer a recognizable name, like Platform.sh consumer or Platform.sh integration. Callback URL: The URL users will be redirected to after access authorization. It is sufficient to set this value to http://localhost. Set as a private consumer: At the bottom of the “Details” section, select the “This is a private consumer” checkbox. Permissions: Sets the integration permissions for Platform.sh. These permissions will create the webhooks that will enable Platform.sh to mirror actions from the Bitbucket repository. Account - Email, Read Repositories - Read, Write Pull requests - Read Webhooks - Read and write After you have completed the form, Save the consumer. After you have saved, you will see your consumer listed in the “OAuth consumers” section. If you open that item, it will expose two variables that you will need to complete the integration using the Platform.sh CLI: Key and Secret. Local Install the Platform.sh CLI if you have not already done so. Retrieve a PROJECT_ID for an existing project with platform project:list or create a new project with platform project:create. Then run the integration command: platform integration:add --type=bitbucket --project \u0026lt;PLATFORMSH_PROJECT_ID\u0026gt; --key \u0026lt;CONSUMER_KEY\u0026gt; --secret \u0026lt;CONSUMER_SECRET\u0026gt; --repository \u0026lt;USER\u0026gt;/\u0026lt;REPOSITORY\u0026gt; where PLATFORMSH_PROJECT_ID is the project ID for your Platform.sh project. CONSUMER_KEY is the Key variable of the consumer you created. CONSUMER_SECRET is the Secret variable of the consumer you created. USER/REPOSITORY is the location of the repository. Validate the integration In both cases, you can verify that your integration is functioning properly using the CLI command platform integration:validate Optional parameters By default several parameters will be set for the Bitbucket integration. They can be changed using the platform integration:update command. --fetch-branches: Track and deploy branches (true by default) --prune-branches: Delete branches that do not exist in the remote Bitbucket repository (true by default) --build-pull-requests: Track and deploy pull-requests (true by default) --build-pull-requests-post-merge: false to have Platform.sh build the branch specified in a PR. true to build the result of merging the PR. (false by default) --pull-requests-clone-parent-data: Set to false to disable cloning of parent environment data when creating a PR environment, so each PR environment starts with no data. (true by default) For more information see: platform help integration:update Note: The –prune-branches option depends on –fetch-branches being enabled. If –fetch-branches is disabled, –prune-branches will automatically be set to false, even if specifically set to true.",
        "section": "Source Integrations",
        "subsections": " Set up an OAuth consumer  Local   Validate the integration Optional parameters  ",
        "image": "",
        "url": "/integrations/source/bitbucket.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "a062392cca085e24cdadc3a20cd04b8f",
        "title": "Blackfire",
        "description": "",
        "text": " Platform.sh supports Blackfire.io. Blackfire is a PHP profiler and automated performance testing tool that can be used in the development Integration, Staging, and Production environments. It grants details information on your PHP code’s resources consumption across Wall-Time, CPU, I/O, Memory, Network Calls, HTTP requests and SQL queries. In addition, it can profile your code automatically and notify you whenever your code does not comply with best practices for PHP, Symfony, Drupal, eZPlatform, Typo3 \u0026amp; Magento code performance management. For a high level overview and demo of Blackfire, check out the full video tutorial . Version Check the latest versions of the probe and CLI tool on Blackfire’s documentation . On a Grid plan 1. Get your credentials Sign up for the free 15 days Premium trial at Blackfire.io and install the Blackfire Companion web browser extension ( Chrome or Firefox ). Note: Blackfire also offers a perpetually-free edition but it is for local development only and will not run on Platform.sh. Go to your Dashboard and create a new environment under the Environments tab . You will need to store the server credentials for further configuration. You can find them any time under the “Settings” tab of your environment in Blackfire. 2. Enable the Blackfire extension Configure the extension in your .platform.app.yaml as follows: runtime:extensions:- blackfirePush the changes to your Platform environment to enable Blackfire as follows: git add .platform.app.yaml git commit -m  Enable Blackfire.  git push 3. Configure your server credentials Blackfire enables to have a fine grained configuration of server credentials across branches and environments on Platform.sh. Configuring global server credentials Configuring server credentials on your master branch will enable you to make sure you can profile any other branch: platform variable:create -e master env:BLACKFIRE_SERVER_ID --value \u0026lt;insert your Server ID\u0026gt; platform variable:create -e master env:BLACKFIRE_SERVER_TOKEN --value \u0026lt;insert your Server Token\u0026gt; Configuring server credentials per branch A recommendation is to have a Blackfire environment for production, another one for staging, and another one for development/integration. That can be mapped in Platform.sh to one Blackfire environment for the production branch, one for the staging branch, and one for all feature branches. platform variable:create -e=\u0026lt;insert your branch name\u0026gt; env:BLACKFIRE_SERVER_ID \u0026lt;insert your Server ID\u0026gt; platform variable:create -e=\u0026lt;insert your branch name\u0026gt; env:BLACKFIRE_SERVER_TOKEN \u0026lt;insert your Server Token\u0026gt; 4. Confirm it’s running Login via SSH to your container and confirm that Blackfire is running as follows: php --ri blackfire blackfire blackfire =\u0026gt; enabled blackfire =\u0026gt; 1.16.1 Timing measurement =\u0026gt; gtod Num of CPU =\u0026gt; 8 ... On a Dedicated cluster Sign up for the free 15 days Premium trial at blackfire.io and install the Blackfire Companion web browser extension ( Chrome or Firefox ). Then open a support ticket with the Backfire server ID and token. The client ID and token is optional. Our support team will install it for you. Note, Blackfire integration works only on profiling your cluster via the URL to the origin. Do not profile your site going through the CDN. Profiling web requests Access your site via your browser and click Profile in the Blackfire Companion. That’s it! Your site will be profiled and you should get all the results in your Blackfire account. Profiling CLI commands To profile your PHP CLI scripts, use the following command line: blackfire --config /etc/platform/$USER/blackfire.ini \u0026lt;command\u0026gt; Going further with Blackfire Blackfire also enables to: collaborate with the rest of your team write performance tests automate profiling with periodic builds integrate further with Platform.sh by enabling to automate profiling as each code commit integrate with New Relic for combined benefits of monitoring and profiling integrate with GitHub, Bitbucket and GitLab to show the results of Blackfire builds at the commit status level Check Blackfire’s documentation for more information. Note: Those features may require a Premium or an Enterprise subscription. We offer attractive bundles of Platform.sh and Blackfire.io subscriptions. Please contact our sales department to discuss how we can help you. Troubleshooting Bypassing Reverse Proxy, Cache, and Content Delivery Networks (CDN) If you are using one of those, you will need them to let Blackfire access your servers. More information on how to configure a bypass . HTTP Cache configuration If you are using the HTTP cache with cookies , please update in your .platform.app.yaml the cookies that are allowed to go through the cache. You need to allow the __blackfire cookie name. Something like: cache:enabled:truecookies:[ /SESS.*/ , __blackfire ]Reaching out to the Blackfire support If the above didn’t help, collect the following and send it to the Blackfire support : The output of platform ssh -- php -d display_startup_errors=on --ri blackfire command The Blackfire logs Getting the Blackfire logs Please execute the following in the environment where you’re facing the issue: platform variable:create php:blackfire.log_file --value /tmp/blackfire.log platform variable:create php:blackfire.log_level --value 4 start a profile/build again You will get the logs with platform ssh -- cat /tmp/blackfire.log \u0026gt; blackfire.log. Disabling the Blackfire logs Once you are done, please disable logging with: platform variable:delete php:blackfire.log_file platform variable:delete php:blackfire.log_level",
        "section": "Profiling",
        "subsections": " Version On a Grid plan  1. Get your credentials 2. Enable the Blackfire extension 3. Configure your server credentials 4. Confirm it\u0026rsquo;s running   On a Dedicated cluster Profiling web requests Profiling CLI commands Going further with Blackfire Troubleshooting  Bypassing Reverse Proxy, Cache, and Content Delivery Networks (CDN) HTTP Cache configuration   Reaching out to the Blackfire support  Getting the Blackfire logs Disabling the Blackfire logs    ",
        "image": "",
        "url": "/integrations/profiling/blackfire.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "e08fe797431a40542d8f4bfdb7631551",
        "title": "C#/.NET Core",
        "description": "",
        "text": " Platform.sh supports deploying .NET applications by allowing developers to define a build process and pass its variables to the .NET Core build environment. Supported versions Grid Dedicated 2.0 2.1 2.2 3.1 None available To specify a .NET Core container, use the type property in your .platform.app.yaml. type:\u0026#39;dotnet:3.1\u0026#39; Building the application For simple applications, using the dotnet publish default framework-dependent deployment method is sufficient for building applications in .NET containers: hooks:build:| set -xedotnetpublish--output $PLATFORM_OUTPUT_DIR -p:UseRazorBuildServer=false-p:UseSharedCompilation=falsewhere PLATFORM_OUTPUT_DIR is the output directory for compiled languages available at build time. Typically .NET Core builds will start a collection of build servers, which are helpful for repeated builds. On Platform.sh, however, if this process is not disabled, the build process will not finish until the idle timeout is reached. As a result, it is recommended to include -p toggles that disable the Razor compiler for dynamic cshtml pages (UseRazorBuildServer) and the .NET msbuild compiler (UseSharedCompilation). If making multiple builds is desired for your application, make sure to call dotnet build-server shutdown at the end of your build hook. Running the application .NET Core applications should be started using the web.commands.start directive in .platform.app.yaml. This ensures that the command starts at the right moment and stops gracefully when a re-deployment needs to be executed. Also, should the program terminate for any reason, it will be automatically restarted. Note that the start command must run in the foreground. Incoming requests are passed to the application using either a TCP (default) or UNIX socket. The application must use the appropriate environment variable to determine the URI to listen on. In case of a TCP socket ( recommended ), the application must listen on http://127.0.0.1, using the PORT environment variable. There will be an Nginx server sitting in front of your application. Serving static content via Nginx is recommended, as this allows easy control of headers (including cache headers) and also has marginal performance benefits. Note that HTTPS is also terminated at the Ngnix proxy, so the app.UseHttpsRedirection(); line in Startup.cs should be removed. To force HTTPS-only, please refer to the routes documentation . The following example configures an environment to serve the static content folders commonly found in ASP.NET MVC templates using Nginx, while routing other traffic to the .NET application. web:locations: / :root: wwwroot allow:truepassthru:truerules:# Serve these common asset types with customs cache WebApplication1.dll You can also route all requests to the application unconditionally: web:locations: / :allow:falsepassthru:truecommands:start: dotnet WebApplication1.dll Project templates Platform.sh offers project templates for .NET Core applications using the structure described above. They can be used as a starting point or reference for building your own website or web application. ASP.NET Core ASP.NET Core This template builds the ASP.NET Core framework for Platform.sh. It includes a minimalist application skeleton for demonstration, but you are free to alter it as needed. ASP.NET Core is an open-source and cross-platform .NET framework for building modern cloud-based web applications. Services: .NET 2.2 MariaDB 10.2 Redis 5.0 View the repository on GitHub.",
        "section": "Languages",
        "subsections": " Supported versions Building the application Running the application Project templates  ",
        "image": "",
        "url": "/languages/dotnet.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "6952a1a08417ce5b7038705e04998b98",
        "title": "Elixir",
        "description": "",
        "text": " Platform.sh supports building and deploying applications written in Elixir. There is no default flavor for the build phase, but you can define it explicitly in your build hook. Platform.sh Elixir images support both committed dependencies and download-on-demand. The underlying Erlang version is 22.0.7. Supported versions Grid Dedicated 1.9 None available To specify an Elixir container, use the type property in your .platform.app.yaml. type:\u0026#39;elixir:1.9\u0026#39; Platform.sh variables Platform.sh exposes relationships and other configuration as environment variables . Most notably, it allows a program to determine at runtime what HTTP port it should listen on and what the credentials are to access other services . To get the PORT environment variable (the port on which your web application is supposed to listen) you would: String.to_integer(System.get_env( PORT ) ||  8888 ) Some of the environment variables are in JSON format and are base64 encoded. You would need to import a JSON parsing library such as Jason or Poison to read those. (There is an example for doing this to decode the PLATFORM_RELATIONSHIPS environment variable in the section below .) Tip: Remember config/prod.exs is evaluated at build time and will not have access to runtime configuration. Use config/releases.exs to configure your runtime environment. Building and running the application If you are using Hex to manage your dependencies, it will be necessary to specify a set of environment variables in your .platform.app.yaml file that define the MIX_ENV and SECRET_KEY_BASE, which can be set to the Platform.sh-provided PLATFORM_PROJECT_ENTROPY environment variable: variables:env:SECRET_KEY_BASE:$PLATFORM_PROJECT_ENTROPYMIX_ENV:\u0026#39;prod\u0026#39;Include in your build hook the steps to retrieve a local Hex and rebar, and then run mix do deps.get, deps.compile, compile on your application to build a binary. hooks:build:| mix local.hex --forcemixlocal.rebar--forcemixdodeps.get--onlyprod,deps.compile,compile Note: The above build hook will work for most cases, and assumes that your mix.exs file is located at the root of your application. Assuming mix.exs is present at the root of your repository and your build hook matches the above, you can then start it from the web.commands.start directive. Note: The start command must run in the foreground, so you should set the --no-halt flag when calling mix run. The following basic .platform.app.yaml file is sufficient to run most Elixir applications. name:apptype:elixir:1.9variables:env:MIX_ENV:\u0026#39;prod\u0026#39;SECRET_KEY_BASE:$PLATFORM_PROJECT_ENTROPYhooks:build:| mix local.hex --forcemixlocal.rebar--forcemixdodeps.get--onlyprod,deps.compile,compileweb:commands:start:mixrun--no-haltlocations:/:allow:falsepassthru:trueNote that there will still be an Nginx proxy server sitting in front of your application. If desired, certain paths may be served directly by Nginx without hitting your application (for static files, primarily) or you may route all requests to the Elixir application unconditionally, as in the example above. Dependencies The recommended way to handle Elixir dependencies on Platform.sh is using Hex. You can commit a mix.exs file in your repository and the system will download the dependencies in your deps section using the build hook above. defp deps do [ {:platformshconfig,  ~\u0026gt; 0.1.0 } ] end Accessing Services The simplest possible way to go around this is to use the Platform.sh Config Reader library from hex. The libraray source is also available on GitHub . If you are building a Phoenix app for example, it would suffice to add a database to .platform/services.yaml and a relationship in .platform.app.yaml. Put the lib in your deps and, assuming you renamed the proc.secrets.exs to releases.exs per the Phoenix guide , change: System.get_env( DATABASE_URL ) to Platformsh.Config.ecto_dsn_formatter( database ) See Platform.sh Config Reader Documentation for the full API. Accessing Services Manually The services configuration is available in the environment variable PLATFORM_RELATIONSHIPS. Given a relationship defined in .platform.app.yaml: relationships:postgresdatabase: dbpostgres:postgresql  Assuming you have in mix.exs the Poison library to parse JSON: defp deps do [ {:poison,  ~\u0026gt; 3.0 } ] end And assuming you use ecto you could put in config/config.exs: relationships = Poison.decode!(Base.decode64!(System.get_env( PLATFORM_RELATIONSHIPS ))) [postgresql_config | _tail] = relationships[ postgresdatabase ] config :my_app, Repo, database: postgresql_config[ path ], username: postgresql_config[ username ], password: postgresql_config[ password ], hostname: postgresql_config[ host ] and setup Ecto during the deploy hook: deploy:| mix do ecto.setupProject templates Platform.sh offers a number of project templates using the structure described above. It can be used as a starting point or reference for building your own website or web application. Templates in development.",
        "section": "Languages",
        "subsections": " Supported versions Platform.sh variables Building and running the application Dependencies Accessing Services  Accessing Services Manually   Project templates  ",
        "image": "",
        "url": "/languages/elixir.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "0d409dfd98746e9ff89dfba775c49ce1",
        "title": "Example: Slack",
        "description": "",
        "text": " The following example activity script will post a message to a Slack channel every time it is triggered. To use it, paste it as-is into a .js file and then add it as a new integration. Be sure to specify which events it should trigger on using the --events switch, and if desired which --environments you want. Second, create a new Slack webhook through your Slack administrative interface. See the Slack documentation for how to do so. At the end you will be given a URL that points to https://hooks.slack.com/.... Third, add that URL to your project as a variable named SLACK_URL. Now, any activities that meet the events/environment criteria you specified will get reported to Slack. Once you have it working, you’re free to modify the code below as desired. See the Slack messaging documentation for how to format more complex messages. /** * Sends a color-coded formatted message to Slack. * * You must first configure a Platform.sh variable named  SLACK_URL . * That is the group and channel to which the message will be sent. * * To control what events it will run on, use the --events switch in * the Platform.sh CLI. * * @param {string} title * The title of the message block to send. * @param {string} message * The message body to send. */ function sendSlackMessage(title, message) { console.log((new Date).getDay()); if ((new Date).getDay() === 5) { message \u0026#43;= a Friday! :calendar: ; } var color = activity.result === \u0026#39;success\u0026#39; ? \u0026#39;#66c000\u0026#39; : \u0026#39;#ff0000\u0026#39;; var body = { \u0026#39;attachments\u0026#39;: [{  title : title,  text : message,  color : color, }], }; var url = variables()[\u0026#39;SLACK_URL\u0026#39;]; if (!url) { throw new Error(\u0026#39;You must define a SLACK_URL project variable.\u0026#39;); } var resp = fetch(url,{ method: \u0026#39;POST\u0026#39;, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, }, body: JSON.stringify(body), }); if (!resp.ok) { console.log( Sending slack message failed:   \u0026#43; resp.body.text()); } } function variables() { var vars = {}; activity.payload.deployment.variables.forEach(function(variable) { vars[variable.name] = variable.value; }); return vars; } sendSlackMessage(activity.text, activity.log); Common properties you may want to send to Slack (in the last line of the script) include: activity.text: A brief, one-line statement of what happened. activity.log: The complete build and deploy log output, as it would be seen in the Management Console log screen.",
        "section": "Activity scripts",
        "subsections": "",
        "image": "",
        "url": "/integrations/activity/slack.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "c81498fc4d42dce84431608e41d29e1c",
        "title": "External integrations",
        "description": "",
        "text": " While you can host your application repository entirely on Platform.sh, it’s likely that you will want to integrate your deployments with your pre-existing service. Platform.sh can be easily integrated with external services such as GitHub, Gitlab, or Bitbucket. Choose your current service, and this guide will take you through the steps to mirror your repository on Platform.sh and have environments created automatically for your pull requests and branches. GitHub Bitbucket GitLab These steps assume that you have already: Signed up for a free trial account with Platform.sh. If you have not completed these steps by now, click the links and do so before you begin.",
        "section": "Getting started",
        "subsections": "",
        "image": "",
        "url": "/gettingstarted/next-steps/integrations.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "14846ee4b8bc7a6521a3ec9e63e7da51",
        "title": "GitHub",
        "description": "",
        "text": " The GitHub integration allows you to manage your Platform.sh environments directly from your GitHub repository. Features supported: Create a new environment when creating a branch or opening a pull request on GitHub. Rebuild the environment when pushing new code to GitHub. Delete the environment when merging a pull request. Setup 1. Generate a token To integrate your Platform.sh project with an existing GitHub repository, you first need to generate a token on your GitHub user profile. Simply go to your Settings, then select Developer settings and click Personal access tokens. Here you can Generate a new token . Give it a description and then ensure the token has the following scopes: To integrate with public repositories: public_repo To integrate with your own private repositories: repo To integrate with your organization’s private repositories: repo and read:org Copy the token and make a note of it (temporarily). Note that for the integration to work, your GitHub user needs to have permission to push code to the repository. 2. Enable the integration Note that only the project owner can manage integrations. Open a terminal window (you need to have the Platform.sh CLI installed). Enable the GitHub integration as follows: platform integration:add --type=github --project=PLATFORMSH_PROJECT_ID --token=GITHUB-USER-TOKEN --repository=USER/REPOSITORY where PLATFORMSH_PROJECT_ID is the project ID for your Platform.sh project GITHUB-USER-TOKEN is the token you generated in step 1 USER is your github user name REPOSITORY is the name of the repository in github (not the git address) Note that if your repository belongs to an organization, use --repository=ORGANIZATION/REPOSITORY. e.g. platform integration:add --type=github --project=abcde12345 --token=xxx --repository=platformsh/platformsh-docs Optional parameters: --fetch-branches: Track and deploy branches (true by default) --prune-branches: Delete branches that do not exist in the remote GitHub repository (true by default) --build-pull-requests: Track and deploy pull-requests (true by default) --build-pull-requests-post-merge: false to have Platform.sh build the branch specified in a PR. true to build the result of merging the PR. (false by default) --pull-requests-clone-parent-data: Set to false to disable cloning of parent environment data when creating a PR environment, so each PR environment starts with no data. (true by default) --base-url: Only set if using GitHub Enterprise, hosted on your own server. If so, set this to the base URL of your private server (the part before the user and repository name). The CLI will create the necessary webhook for you when there’s correct permission set in the given token. Note that the --prune-branches option depends on --fetch-branches being enabled. If --fetch-branches is disabled, --prune-branches will automatically be set to false, even if specifically set to true. 3. Add the webhook If you see the message Failed to read or write webhooks, you will need to add a webhook manually: Copy the hook URL shown in the message. Go to your GitHub repository and click Settings, select the Webhooks and Services tab, and click Add webhook. Paste the hook URL, choose application/json for the content type, choose “Send me everything” for the events you want to receive, and click Add webhook. You can now start pushing code, creating new branches or opening pull requests directly on your GitHub repository. Note that if you have created your account using the GitHub oAuth Login then in order to use the Platform CLI, you will need to setup a password . 4. Validate the integration You can then verify that your integration is functioning properly using the CLI command platform integration:validate Types of environments Environments based on GitHub pull requests will have the correct ‘parent’ environment on Platform.sh; they will be activated automatically with a copy of the parent’s data. However, environments based on (non-pull-request) branches cannot have parents; they will inherit directly from master and start inactive by default.",
        "section": "Source Integrations",
        "subsections": " Setup  1. Generate a token 2. Enable the integration 3. Add the webhook 4. Validate the integration   Types of environments  ",
        "image": "",
        "url": "/integrations/source/github.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "42f7310904b3bbc0ce94c8b6ae063d0f",
        "title": "GitLab",
        "description": "",
        "text": " The GitLab integration allows you to manage your Platform.sh environments directly from your GitLab repository. Features supported: Create a new environment when creating a branch or opening a pull request on GitLab. Rebuild the environment when pushing new code to GitLab. Delete the environment when merging a pull request. Setup 1. Generate a token To integrate your Platform.sh project with an existing GitLab repository, you first need to generate a token on your GitLab user profile. Simply go to your Settings page on GitLab and click Access Tokens. Fill the Name field for example with “Platform.sh Integration” and optionally set an expiration time. Give it a description and then ensure the token has the following scopes: api - Access your API read_user - Read user information read_repository - Read repositories Copy the token and make a note of it (temporarily). Note that for the integration to work, your GitLab user needs to have permission to push code to the repository. 2. Enable the integration Note that only project owner or project admin can manage the integrations. Open a terminal window (you need to have the Platform.sh CLI installed). Enable the GitLab integration as follows: platform integration:add --type=gitlab --token=GITLAB-ACCESS-TOKEN --base-url=https://THE-URL-OF-YOUR-GITLAB --server-project=MY-NAMESPACE/MY-PROJECTNAME --project=PLATFORMSH_PROJECT_ID where PLATFORMSH_PROJECT_ID is the project ID for your Platform.sh project GITLAB-ACCESS-TOKEN is the token you generated in step 1 --base-url is used as the base to call the Gitlab API; you should point it to https://gitlab.com if your project is hosted on Gitlab, or the URL for your own Gitlab instance otherwise. It should not include your namespace and project name. MY-NAMESPACE/MY-PROJECTNAME describes the namespace of your GitLab project, not including the base url. For example, if your repository is located at https://gitlab.com/sandbox/my_application, the integration command would be platform integration:add --type=gitlab --token=GITLAB-ACCESS-TOKEN --base-url=https://gitlab.com --server-project=sandbox/my_application --project=PLATFORMSH_PROJECT_ID Optional parameters: --build-merge-requests: Track and deploy merge-requests (true by default) --merge-requests-clone-parent-data : should merge requests clone the data from the parent environment (true by default) --fetch-branches: Track and deploy branches (true by default) --prune-branches: Delete branches that do not exist in the remote GitLab repository (true by default) --base-url: Only set if using self-hosted GitLab on your own server. If so, set this to the base URL of your private server (the part before the user and repository name). Note that the --prune-branches option depends on --fetch-branches being enabled. If --fetch-branches is disabled, --prune-branches will automatically be set to false, even if specifically set to true. 3. Add the webhook The previous command, if successful should output the configuration of the integration. The last element would look like: | hook_url | https://{region}.platform.sh/api/projects/{projectid}/integrations/{hook_id}/hook | The CLI will create the necessary webhook using the above URL for you when there’s correct permission set in the given token. If you see the message Failed to read or write webhooks, you will need to add a webhook manually: Copy the hook URL shown in the message. Go to your GitLab repository and click Settings \u0026gt; Integrations. Paste the hook URL. In the Triggers section choose Push events, Tag push events and Merge Request events. Click on Add webhook. You can now start pushing code, creating new branches or opening merge requests directly on your GitLab repository. You will see environments get automatically created and updated on the Platform.sh side. 4. Validate the integration You can then verify that your integration is functioning properly using the CLI command platform integration:validate Types of environments Environments based on GitLab merge requests will have the correct ‘parent’ environment on Platform.sh; they will be activated automatically with a copy of the parent’s data (unless you have set the option merge-requests-clone-parent-data to false). However, environments based on (non-merge-request) branches cannot have parents; they will inherit directly from master and start inactive by default.",
        "section": "Source Integrations",
        "subsections": " Setup  1. Generate a token 2. Enable the integration 3. Add the webhook 4. Validate the integration   Types of environments  ",
        "image": "",
        "url": "/integrations/source/gitlab.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "19c5bc510de6c4ad7ec751f70cdfdaf6",
        "title": "Go",
        "description": "",
        "text": " Platform.sh supports building and deploying applications written in Go using Go modules. They are compiled during the Build hook phase, and support both committed dependencies and download-on-demand. Supported versions Grid Dedicated 1.11 1.12 1.13 1.14 None available To specify a Go container, use the type property in your .platform.app.yaml. type:\u0026#39;golang:1.14\u0026#39; Deprecated versions The following container versions are also available. However, due to their lack of Go module support and the difficulties in supporting the GOPATH during the Platform.sh build they are not recommended. 1.10 1.8 1.9 Go modules The recommended way to handle Go dependencies on Platform.sh is using Go module support in Go 1.11 and later. That allows the build process to use go build directly without any extra steps, and you can specify an output executable file of your choice. (See the examples below.) Platform.sh variables Platform.sh exposes relationships and other configuration as environment variables . To make them easier to access you should use the provided Config Reader library . Most notably, it allows a program to determine at runtime what HTTP port it should listen on and what the credentials are to access other services . package main import ( _  github.com/go-sql-driver/mysql  psh  github.com/platformsh/gohelper   net/http  ) func main() { p, err := psh.NewPlatformInfo() if err != nil { panic( Not in a Platform.sh Environment. ) } http.HandleFunc( /bar , func(w http.ResponseWriter, r *http.Request) { // ... }) http.ListenAndServe( : \u0026#43;p.Port, nil) } Building and running the application Assuming your go.mod and go.sum files are present in your repository, the application may be built with a simple go build command that will produce a working executable. You can then start it from the web.commands.start directive. Note that the start command must run in the foreground. Should the program terminate for any reason it will be automatically restarted. The following basic .platform.app.yaml file is sufficient to run most Go applications. name:apptype:golang:1.14hooks:build:| # Modify this line if you want to build differently or use an alternate name for your executable.gobuild-obin/appweb:upstream:socket_family:tcpprotocol:httpcommands:# If you change the build output in the build hook above, update this line as well.start:./bin/applocations:/:# Route all requests to the Go app, unconditionally.# If you want some files served directly by the web server without hitting Go, see# https://docs.platform.sh/configuration/app/web.htmlallow:falsepassthru:truedisk:1024Note that there will still be an Nginx proxy server sitting in front of your application. If desired, certain paths may be served directly by Nginx without hitting your application (for static files, primarily) or you may route all requests to the Go application unconditionally, as in the example above. Accessing services To access various services with Go, see the following examples. The individual service pages have more information on configuring each service. Memcached MongoDB MySQL PostgreSQL RabbitMQ Solr package examples import (  fmt   github.com/bradfitz/gomemcache/memcache  psh  github.com/platformsh/config-reader-go/v2  gomemcache  github.com/platformsh/config-reader-go/v2/gomemcache  ) func UsageExampleMemcached() string { // Create a NewRuntimeConfig object to ease reading the Platform.sh environment variables. // You can alternatively use os.Getenv() yourself. config, err := psh.NewRuntimeConfig() checkErr(err) // Get the credentials to connect to the Solr service. credentials, err := config.Credentials( memcached ) checkErr(err) // Retrieve formatted credentials for gomemcache. formatted, err := gomemcache.FormattedCredentials(credentials) checkErr(err) // Connect to Memcached. mc := memcache.New(formatted) // Set a value. key :=  Deploy_day  value :=  Friday  err = mc.Set(\u0026amp;memcache.Item{Key: key, Value: []byte(value)}) // Read it back. test, err := mc.Get(key) return fmt.Sprintf( Found value \u0026lt;strong\u0026gt;%s\u0026lt;/strong\u0026gt; for key \u0026lt;strong\u0026gt;%s\u0026lt;/strong\u0026gt;. , test.Value, key) } package examples import (  context   fmt  psh  github.com/platformsh/config-reader-go/v2  mongoPsh  github.com/platformsh/config-reader-go/v2/mongo   go.mongodb.org/mongo-driver/bson   go.mongodb.org/mongo-driver/mongo   go.mongodb.org/mongo-driver/mongo/options   time  ) func UsageExampleMongoDB() string { // Create a NewRuntimeConfig object to ease reading the Platform.sh environment variables. // You can alternatively use os.Getenv() yourself. config, err := psh.NewRuntimeConfig() checkErr(err) // Get the credentials to connect to the Solr service. credentials, err := config.Credentials( mongodb ) checkErr(err) // Retrieve the formatted credentials for mongo-driver. formatted, err := mongoPsh.FormattedCredentials(credentials) checkErr(err) // Connect to MongoDB using the formatted credentials. ctx, _ := context.WithTimeout(context.Background(), 10*time.Second) client, err := mongo.Connect(ctx, options.Client().ApplyURI(formatted)) checkErr(err) // Create a new collection. collection := client.Database( main ).Collection( starwars ) // Clean up after ourselves. err = collection.Drop(context.Background()) checkErr(err) // Create an entry. res, err := collection.InsertOne(ctx, bson.M{ name :  Rey ,  occupation :  Jedi }) checkErr(err) id := res.InsertedID // Read it back. cursor, err := collection.Find(context.Background(), bson.M{ _id : id}) checkErr(err) var name string var occupation string for cursor.Next(context.Background()) { document := struct { Name string Occupation string }{} err := cursor.Decode(\u0026amp;document) checkErr(err) name = document.Name occupation = document.Occupation } return fmt.Sprintf( Found %s (%s) , name, occupation) } package examples import (  database/sql   fmt  _  github.com/go-sql-driver/mysql  psh  github.com/platformsh/config-reader-go/v2  sqldsn  github.com/platformsh/config-reader-go/v2/sqldsn  ) func UsageExampleMySQL() string { // Create a NewRuntimeConfig object to ease reading the Platform.sh environment variables. // You can alternatively use os.Getenv() yourself. config, err := psh.NewRuntimeConfig() checkErr(err) // The \u0026#39;database\u0026#39; relationship is generally the name of the primary SQL database of an application. // That\u0026#39;s not required, but much of our default automation code assumes it. credentials, err := config.Credentials( database ) checkErr(err) // Using the sqldsn formatted credentials package. formatted, err := sqldsn.FormattedCredentials(credentials) checkErr(err) db, err := sql.Open( mysql , formatted) checkErr(err) defer db.Close() // Force MySQL into modern mode. db.Exec( SET NAMES=utf8 ) sql_mode = \u0026#39;ANSI,STRICT_TRANS_TABLES,STRICT_ALL_TABLES, NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO, // Creating a table. sqlCreate := CREATE TABLE IF NOT EXISTS PeopleGo ( id SERIAL PRIMARY KEY, name VARCHAR(30) NOT NULL, city VARCHAR(30) NOT _, err = db.Exec(sqlCreate) checkErr(err) // Insert data. sqlInsert := INSERT INTO PeopleGo (name, city) VALUES (\u0026#39;Neil Armstrong\u0026#39;, \u0026#39;Moon\u0026#39;), (\u0026#39;Buzz Aldrin\u0026#39;, \u0026#39;Glen Ridge\u0026#39;), (\u0026#39;Sally Ride\u0026#39;, \u0026#39;La _, err = db.Exec(sqlInsert) checkErr(err) table := \u0026lt;thead\u0026gt; \u0026lt;tr\u0026gt;\u0026lt;th\u0026gt;Name\u0026lt;/th\u0026gt;\u0026lt;th\u0026gt;City\u0026lt;/th\u0026gt;\u0026lt;/tr\u0026gt; \u0026lt;/thead\u0026gt; var id int var name string var city string rows, err := db.Query( SELECT * FROM PeopleGo ) if err != nil { panic(err) } else { for rows.Next() { err = rows.Scan(\u0026amp;id, \u0026amp;name, \u0026amp;city) checkErr(err) table \u0026#43;= name, city) } table \u0026#43;= } _, err = db.Exec( DROP TABLE PeopleGo; ) checkErr(err) return table } package examples import (  database/sql   fmt  _  github.com/lib/pq  psh  github.com/platformsh/config-reader-go/v2  libpq  github.com/platformsh/config-reader-go/v2/libpq  ) func UsageExamplePostgreSQL() string { // Create a NewRuntimeConfig object to ease reading the Platform.sh environment variables. // You can alternatively use os.Getenv() yourself. config, err := psh.NewRuntimeConfig() checkErr(err) // The \u0026#39;database\u0026#39; relationship is generally the name of the primary SQL database of an application. // It could be anything, though, as in the case here where it\u0026#39;s called  postgresql . credentials, err := config.Credentials( postgresql ) checkErr(err) // Retrieve the formatted credentials. formatted, err := libpq.FormattedCredentials(credentials) checkErr(err) // Connect. db, err := sql.Open( postgres , formatted) checkErr(err) defer db.Close() // Creating a table. sqlCreate := CREATE TABLE IF NOT EXISTS PeopleGo ( id SERIAL PRIMARY KEY, name VARCHAR(30) NOT NULL, city VARCHAR(30) NOT _, err = db.Exec(sqlCreate) checkErr(err) // Insert data. sqlInsert := INSERT INTO PeopleGo(name, city) VALUES (\u0026#39;Neil Armstrong\u0026#39;, \u0026#39;Moon\u0026#39;), (\u0026#39;Buzz Aldrin\u0026#39;, \u0026#39;Glen Ridge\u0026#39;), (\u0026#39;Sally Ride\u0026#39;, \u0026#39;La _, err = db.Exec(sqlInsert) checkErr(err) table := \u0026lt;thead\u0026gt; \u0026lt;tr\u0026gt;\u0026lt;th\u0026gt;Name\u0026lt;/th\u0026gt;\u0026lt;th\u0026gt;City\u0026lt;/th\u0026gt;\u0026lt;/tr\u0026gt; \u0026lt;/thead\u0026gt; var id int var name string var city string // Read it back. rows, err := db.Query( SELECT * FROM PeopleGo ) if err != nil { panic(err) } else { for rows.Next() { err = rows.Scan(\u0026amp;id, \u0026amp;name, \u0026amp;city) checkErr(err) table \u0026#43;= name, city) } table \u0026#43;= } _, err = db.Exec( DROP TABLE PeopleGo; ) checkErr(err) return table } package examples import (  fmt  psh  github.com/platformsh/config-reader-go/v2  amqpPsh  github.com/platformsh/config-reader-go/v2/amqp   github.com/streadway/amqp   sync  ) func UsageExampleRabbitMQ() string { // Create a NewRuntimeConfig object to ease reading the Platform.sh environment variables. // You can alternatively use os.Getenv() yourself. config, err := psh.NewRuntimeConfig() checkErr(err) // Get the credentials to connect to RabbitMQ. credentials, err := config.Credentials( rabbitmq ) checkErr(err) // Use the amqp formatted credentials package. formatted, err := amqpPsh.FormattedCredentials(credentials) checkErr(err) // Connect to the RabbitMQ server. connection, err := amqp.Dial(formatted) checkErr(err) defer connection.Close() // Make a channel. channel, err := connection.Channel() checkErr(err) defer channel.Close() // Create a queue. q, err := channel.QueueDeclare(  deploy_days , // name false, // durable false, // delete when unused false, // exclusive false, // no-wait nil, // arguments ) body :=  Friday  msg := fmt.Sprintf( Deploying on %s , body) // Publish a message. err = channel.Publish(   , // exchange q.Name, // routing key false, // mandatory false, // immediate amqp.Publishing{ ContentType:  text/plain , Body: []byte(msg), }) checkErr(err) outputMSG := fmt.Sprintf( [x] Sent \u0026#39;%s\u0026#39; \u0026lt;br\u0026gt; , body) // Consume the message. msgs, err := channel.Consume( q.Name, // queue   , // consumer true, // auto-ack false, // exclusive false, // no-local false, // no-wait nil, // args ) checkErr(err) var received string var wg sync.WaitGroup wg.Add(1) go func() { for d := range msgs { received = fmt.Sprintf( [x] Received message: \u0026#39;%s\u0026#39; \u0026lt;br\u0026gt; , d.Body) wg.Done() } }() wg.Wait() outputMSG \u0026#43;= received return outputMSG } package examples import (  fmt  psh  github.com/platformsh/config-reader-go/v2  gosolr  github.com/platformsh/config-reader-go/v2/gosolr  solr  github.com/rtt/Go-Solr  ) func UsageExampleSolr() string { // Create a NewRuntimeConfig object to ease reading the Platform.sh environment variables. // You can alternatively use os.Getenv() yourself. config, err := psh.NewRuntimeConfig() checkErr(err) // Get the credentials to connect to the Solr service. credentials, err := config.Credentials( solr ) checkErr(err) // Retrieve Solr formatted credentials. formatted, err := gosolr.FormattedCredentials(credentials) checkErr(err) // Connect to Solr using the formatted credentials. connection := \u0026amp;solr.Connection{URL: formatted} // Add a document and commit the operation. docAdd := map[string]interface{}{  add : []interface{}{ map[string]interface{}{ id : 123,  name :  Valentina Tereshkova }, }, } respAdd, err := connection.Update(docAdd, true) checkErr(err) // Select the document. q := \u0026amp;solr.Query{ Params: solr.URLParamMap{  q : []string{ id:123 }, }, } resSelect, err := connection.CustomSelect(q,  query ) checkErr(err) // Delete the document and commit the operation. docDelete := map[string]interface{}{  delete : map[string]interface{}{  id : 123, }, } resDel, err := connection.Update(docDelete, true) checkErr(err) message := one document - %s\u0026lt;br\u0026gt; Selecting document (1 expected): %d\u0026lt;br\u0026gt; Deleting document - %s\u0026lt;br\u0026gt; respAdd, resSelect.Results.NumFound, resDel) return message } Project templates Platform.sh offers a project templates for Go applications using the structure described above. It can be used as a starting point or reference for building your own website or web application. Basic Go Basic Go This template provides the most basic configuration for running a custom Go project. Go is a statically typed, compiled language with an emphasis on easy concurrency and network services. Services: Go 1.14 MariaDB 10.2 View the repository on GitHub. Beego Beego This template builds the Beego framework for Platform.sh. It includes a minimalist application skeleton for demonstration, but you are free to alter it as needed. Beego is a popular web framework written in Go. Services: Go 1.14 MariaDB 10.2 View the repository on GitHub. Echo Echo This template builds the Echo framework for Platform.sh. It includes a minimalist application skeleton for demonstration, but you are free to alter it as needed. Echo is a lightweight, minimalist web framework written in Go. Services: Go 1.14 MariaDB 10.2 View the repository on GitHub. Gin Gin This template builds the Gin framework for Platform.sh. It includes a minimalist application skeleton for demonstration, but you are free to alter it as needed. Gin is a lightweight web framework written in Go that emphasizes performance. Services: Go 1.14 MariaDB 10.2 View the repository on GitHub. Hugo Hugo This template provides a basic Hugo skeleton. All files are generated at build time, so at runtime only static files need to be served. Hugo is a static site generator written in Go, using Go\u0026#39;s native template packages for formatting. Services: Go 1.14 View the repository on GitHub. Mattermost Mattermost This template builds Mattermost on Platform.sh, configuring the deployment through user-defined environment variables. Mattermost is an open-source messaging framework written in Go and React. Services: Go 1.14 PostgreSQL 12 Elasticsearch 7.2 View the repository on GitHub.",
        "section": "Languages",
        "subsections": " Supported versions Deprecated versions Go modules Platform.sh variables Building and running the application Accessing services Project templates  ",
        "image": "",
        "url": "/languages/go.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "fac8ed7c2573b3f346514947e35e542e",
        "title": "Going live",
        "description": "",
        "text": " You’ve set up a project on Platform.sh by either pushing your code directly or by setting up an integration to an external repository. Now it’s time to take your site live. This guide will take you through the process configuring your project for production, setting up a domain, and configuring DNS so that your users can reach the application the way you want them to. Take your site live!",
        "section": "Introduction",
        "subsections": "",
        "image": "",
        "url": "/gettingstarted/next-steps/going-live.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "8d005add4555dd120e963ec5a2445bb4",
        "title": "HipChat",
        "description": "",
        "text": " The HipChat integration allows you to send notifications about your Platform.sh activity directly to HipChat. Setup 1. Find the HipChat ROOM-ID. In the HipChat web administrative UI, go to Admin \u0026gt; Rooms and click on the room to link notifications. Note down the “APP ID” listed in the Room Details on the Room’s ‘Summary’ page (you can also find the ID from the URL). 2. Generate a room-specific HIPCHAT-TOKEN. Click on the Room’s ‘Tokens’ page in the sidebar. In the Create New Token section specify ‘PlatformSH’ as the token’s label and click “Create” button. Note down the Token value. 3. Create the HipChat webhook with Platform CLI. platform integration:add --type=hipchat --room=ROOM-ID --token=HIPCHAT-TOKEN There are a number of optional parameters as well who’s default values include: --events=* (All Events) --environments=* (All Environments) --excluded-environments= (Empty) --states=complete (Complete state only) You’re given a chance to customize these parameters in an interactive shell prompt, or you may override the defaults on the command line: --states=pending,in_progress,complete (All states) Validate the integration You can then verify that your integration is functioning properly using the CLI command platform integration:validate",
        "section": "Activity scripts",
        "subsections": " Setup  1. Find the HipChat ROOM-ID. 2. Generate a room-specific HIPCHAT-TOKEN. 3. Create the HipChat webhook with Platform CLI.   Validate the integration  ",
        "image": "",
        "url": "/integrations/activity/hipchat.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "3976528693a0108357f4928017600865",
        "title": "Introduction",
        "description": "",
        "text": "",
        "section": "Platform.sh",
        "subsections": " Git Driven Infrastructure  Infrastructure as code Full stack management    ",
        "image": "",
        "url": "/",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "0377a167a2e4769e2bf0380ef510e9bf",
        "title": "Java",
        "description": "",
        "text": " Java is a general-purpose programming language, and one of the most popular in the world today. Platform.sh supports Java runtimes that can be used with build management tools such as Gradle, Maven, and Ant. Supported versions OpenJDK versions: Grid Dedicated 8 11 12 13 None available To specify a Java container, use the type property in your .platform.app.yaml. type:'java:13' Support libraries While it is possible to read the environment directly from your application, it is generally easier and more robust to use the platformsh/config-reader which handles decoding of service credential information for you. Support build automation Platform.sh supports the most common project management tools in the Java ecosystem, including: Gradle Maven Ant Accessing services To access various services with Java, see the following examples. The individual service pages have more information on configuring each service. Elasticsearch Kafka Memcached MongoDB MySQL PostgreSQL RabbitMQ Redis Solr package sh.platform.languages.sample; import org.elasticsearch.action.admin.indices.refresh.RefreshRequest; import org.elasticsearch.action.admin.indices.refresh.RefreshResponse; import org.elasticsearch.action.delete.DeleteRequest; import org.elasticsearch.action.index.IndexRequest; import org.elasticsearch.action.search.SearchRequest; import org.elasticsearch.action.search.SearchResponse; import org.elasticsearch.client.RequestOptions; import org.elasticsearch.client.RestHighLevelClient; import org.elasticsearch.index.query.QueryBuilders; import org.elasticsearch.search.SearchHit; import org.elasticsearch.search.builder.SearchSourceBuilder; import sh.platform.config.Config; import sh.platform.config.Elasticsearch; import java.io.IOException; import java.util.Arrays; import java.util.HashMap; import java.util.List; import java.util.Map; import java.util.function.Supplier; import static java.util.concurrent.ThreadLocalRandom.current; public class ElasticsearchSample implements Supplier\u003cString\u003e { @Override public String get() { StringBuilder logger = new StringBuilder(); // Create a new config object to ease reading the Platform.sh environment variables. // You can alternatively use getenv() yourself. Config config = new Config(); Elasticsearch elasticsearch = config.getCredential( elasticsearch , Elasticsearch::new); // Create an Elasticsearch client object. RestHighLevelClient client = elasticsearch.get(); try { String index =  animals ; String type =  mammals ; // Index a few document. final List\u003cString\u003e animals = Arrays.asList( dog ,  cat ,  monkey ,  horse ); for (String animal : animals) { Map\u003cString, Object\u003e jsonMap = new HashMap\u003c\u003e(); jsonMap.put( name , animal); jsonMap.put( age , current().nextInt(1, 10)); jsonMap.put( is_cute , current().nextBoolean()); IndexRequest indexRequest = new IndexRequest(index, type) .id(animal).source(jsonMap); client.index(indexRequest, RequestOptions.DEFAULT); } RefreshRequest refresh = new RefreshRequest(index); // Force just-added items to be indexed RefreshResponse refreshResponse = client.indices().refresh(refresh, RequestOptions.DEFAULT); // Search for documents. SearchSourceBuilder sourceBuilder = new SearchSourceBuilder(); sourceBuilder.query(QueryBuilders.termQuery( name ,  dog )); SearchRequest searchRequest = new SearchRequest(); searchRequest.indices(index); searchRequest.source(sourceBuilder); SearchResponse search = client.search(searchRequest, RequestOptions.DEFAULT); for (SearchHit hit : search.getHits()) { String id = hit.getId(); final Map\u003cString, Object\u003e source = hit.getSourceAsMap(); logger.append(String.format( result id %s source: %s , id, } // Delete documents. for (String animal : animals) { client.delete(new DeleteRequest(index, type, animal), RequestOptions.DEFAULT); } } catch (IOException exp) { throw new RuntimeException( An error when execute Elasticsearch:   + exp.getMessage()); } return logger.toString(); } } package sh.platform.languages.sample; import org.apache.kafka.clients.consumer.Consumer; import org.apache.kafka.clients.consumer.ConsumerConfig; import org.apache.kafka.clients.consumer.ConsumerRecords; import org.apache.kafka.clients.producer.Producer; import org.apache.kafka.clients.producer.ProducerConfig; import org.apache.kafka.clients.producer.ProducerRecord; import org.apache.kafka.clients.producer.RecordMetadata; import sh.platform.config.Config; import sh.platform.config.Kafka; import java.time.Duration; import java.util.HashMap; import java.util.Map; import java.util.function.Supplier; public class KafkaSample implements Supplier\u003cString\u003e { @Override public String get() { StringBuilder logger = new StringBuilder(); // Create a new config object to ease reading the Platform.sh environment variables. // You can alternatively use getenv() yourself. Config config = new Config(); try { // Get the credentials to connect to the Kafka service. final Kafka kafka = config.getCredential( kafka , Kafka::new); Map\u003cString, Object\u003e configProducer = new HashMap\u003c\u003e(); configProducer.putIfAbsent(ProducerConfig.CLIENT_ID_CONFIG,  animals ); final Producer\u003cLong, String\u003e producer = kafka.getProducer(configProducer); // Sending data into the stream. RecordMetadata metadata = producer.send(new ProducerRecord\u003c\u003e( animals ,  lion )).get(); logger.append( Record sent with to partition  ).append(metadata.partition()) .append(  with offset metadata = producer.send(new ProducerRecord\u003c\u003e( animals ,  dog )).get(); logger.append( Record sent with to partition  ).append(metadata.partition()) .append(  with offset metadata = producer.send(new ProducerRecord\u003c\u003e( animals ,  cat )).get(); logger.append( Record sent with to partition  ).append(metadata.partition()) .append(  with offset // Consumer, read data from the stream. final HashMap\u003cString, Object\u003e configConsumer = new HashMap\u003c\u003e(); configConsumer.put(ConsumerConfig.GROUP_ID_CONFIG,  consumerGroup1 ); configConsumer.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG,  earliest ); Consumer\u003cLong, String\u003e consumer = kafka.getConsumer(configConsumer,  animals ); ConsumerRecords\u003cLong, String\u003e consumerRecords = consumer.poll(Duration.ofSeconds(3)); // Print each record. consumerRecords.forEach(record -\u003e { logger.append( Record: Key   + record.key()); logger.append(  value   + record.value()); logger.append(  partition   + record.partition()); logger.append(  offset   + }); // Commits the offset of record to broker. consumer.commitSync(); return logger.toString(); } catch (Exception exp) { throw new RuntimeException( An error when execute Kafka , exp); } } } package sh.platform.languages.sample; import net.spy.memcached.MemcachedClient; import sh.platform.config.Config; import java.util.function.Supplier; import sh.platform.config.Memcached; public class MemcachedSample implements Supplier\u003cString\u003e { @Override public String get() { StringBuilder logger = new StringBuilder(); // Create a new config object to ease reading the Platform.sh environment variables. // You can alternatively use getenv() yourself. Config config = new Config(); // Get the credentials to connect to the Memcached service. Memcached memcached = config.getCredential( memcached , Memcached::new); final MemcachedClient client = memcached.get(); String key =  cloud ; String value =  platformsh ; // Set a value. client.set(key, 0, value); // Read it back. Object test = client.get(key); logger.append(String.format( Found value %s for key %s. , test, key)); return logger.toString(); } } package sh.platform.languages.sample; import com.mongodb.MongoClient; import com.mongodb.client.MongoCollection; import com.mongodb.client.MongoDatabase; import org.bson.Document; import sh.platform.config.Config; import sh.platform.config.MongoDB; import java.util.function.Supplier; import static com.mongodb.client.model.Filters.eq; public class MongoDBSample implements Supplier\u003cString\u003e { @Override public String get() { StringBuilder logger = new StringBuilder(); // Create a new config object to ease reading the Platform.sh environment variables. // You can alternatively use getenv() yourself. Config config = new Config(); // The 'database' relationship is generally the name of primary database of an application. // It could be anything, though, as in the case here here where it's called  mongodb . MongoDB database = config.getCredential( mongodb , MongoDB::new); MongoClient mongoClient = database.get(); final MongoDatabase mongoDatabase = mongoClient.getDatabase(database.getDatabase()); MongoCollection\u003cDocument\u003e collection = mongoDatabase.getCollection( scientist ); Document doc = new Document( name ,  Ada Lovelace ) .append( city ,  London ); collection.insertOne(doc); Document myDoc = collection.find(eq( _id , doc.get( _id ))).first(); logger.append(collection.deleteOne(eq( _id , doc.get( _id )))); return logger.toString(); } } package sh.platform.languages.sample; import sh.platform.config.Config; import sh.platform.config.MySQL; import javax.sql.DataSource; import java.sql.Connection; import java.sql.ResultSet; import java.sql.SQLException; import java.sql.Statement; import java.util.function.Supplier; public class MySQLSample implements Supplier\u003cString\u003e { @Override public String get() { StringBuilder logger = new StringBuilder(); // Create a new config object to ease reading the Platform.sh environment variables. // You can alternatively use getenv() yourself. Config config = new Config(); // The 'database' relationship is generally the name of primary SQL database of an application. // That's not required, but much of our default automation code assumes it. MySQL database = config.getCredential( database , MySQL::new); DataSource dataSource = database.get(); // Connect to the database try (Connection connection = dataSource.getConnection()) { // Creating a table. String sql =  CREATE TABLE JAVA_PEOPLE (  +   id INT(6) UNSIGNED AUTO_INCREMENT PRIMARY KEY,  +  name VARCHAR(30) NOT NULL,  +  city VARCHAR(30) NOT NULL) ; final Statement statement = connection.createStatement(); statement.execute(sql); // Insert data. sql =  INSERT INTO JAVA_PEOPLE (name, city) VALUES  +  ('Neil Armstrong', 'Moon'),  +  ('Buzz Aldrin', 'Glen Ridge'),  +  ('Sally Ride', 'La Jolla') ; statement.execute(sql); // Show table. sql =  SELECT * FROM JAVA_PEOPLE ; final ResultSet resultSet = statement.executeQuery(sql); while (resultSet.next()) { int id = resultSet.getInt( id ); String name = resultSet.getString( name ); String city = resultSet.getString( city ); logger.append(String.format( the JAVA_PEOPLE id %d the name %s and city %s , id, name, city)); } statement.execute( DROP TABLE JAVA_PEOPLE ); return logger.toString(); } catch (SQLException exp) { throw new RuntimeException( An error when execute MySQL , exp); } } } package sh.platform.languages.sample; import sh.platform.config.Config; import sh.platform.config.MySQL; import sh.platform.config.PostgreSQL; import javax.sql.DataSource; import java.sql.Connection; import java.sql.ResultSet; import java.sql.SQLException; import java.sql.Statement; import java.util.function.Supplier; public class PostgreSQLSample implements Supplier\u003cString\u003e { @Override public String get() { StringBuilder logger = new StringBuilder(); // Create a new config object to ease reading the Platform.sh environment variables. // You can alternatively use getenv() yourself. Config config = new Config(); // The 'database' relationship is generally the name of primary SQL database of an application. // It could be anything, though, as in the case here here where it's called  postgresql . PostgreSQL database = config.getCredential( postgresql , PostgreSQL::new); DataSource dataSource = database.get(); // Connect to the database try (Connection connection = dataSource.getConnection()) { // Creating a table. String sql =  CREATE TABLE JAVA_FRAMEWORKS (  +   id SERIAL PRIMARY KEY,  +  name VARCHAR(30) NOT NULL) ; final Statement statement = connection.createStatement(); statement.execute(sql); // Insert data. sql =  INSERT INTO JAVA_FRAMEWORKS (name) VALUES  +  ('Spring'),  +  ('Jakarta EE'),  +  ('Eclipse JNoSQL') ; statement.execute(sql); // Show table. sql =  SELECT * FROM JAVA_FRAMEWORKS ; final ResultSet resultSet = statement.executeQuery(sql); while (resultSet.next()) { int id = resultSet.getInt( id ); String name = resultSet.getString( name ); logger.append(String.format( the JAVA_FRAMEWORKS id %d the name %s  , id, name)); } statement.execute( DROP TABLE JAVA_FRAMEWORKS ); return logger.toString(); } catch (SQLException exp) { throw new RuntimeException( An error when execute PostgreSQL , exp); } } } package sh.platform.languages.sample; import sh.platform.config.Config; import sh.platform.config.RabbitMQ; import javax.jms.Connection; import javax.jms.ConnectionFactory; import javax.jms.MessageConsumer; import javax.jms.MessageProducer; import javax.jms.Queue; import javax.jms.Session; import javax.jms.TextMessage; import java.util.function.Supplier; public class RabbitMQSample implements Supplier\u003cString\u003e { @Override public String get() { StringBuilder logger = new StringBuilder(); // Create a new config object to ease reading the Platform.sh environment variables. // You can alternatively use getenv() yourself. Config config = new Config(); try { // Get the credentials to connect to the RabbitMQ service. final RabbitMQ credential = config.getCredential( rabbitmq , RabbitMQ::new); final ConnectionFactory connectionFactory = credential.get(); // Connect to the RabbitMQ server. final Connection connection = connectionFactory.createConnection(); connection.start(); final Session session = connection.createSession(false, Session.AUTO_ACKNOWLEDGE); Queue queue = session.createQueue( cloud ); MessageConsumer consumer = session.createConsumer(queue); // Sending a message into the queue. TextMessage textMessage = session.createTextMessage( Platform.sh ); textMessage.setJMSReplyTo(queue); MessageProducer producer = session.createProducer(queue); producer.send(textMessage); // Receive the message. TextMessage replyMsg = (TextMessage) consumer.receive(100); logger.append( Message:  ).append(replyMsg.getText()); // close connections. producer.close(); consumer.close(); session.close(); connection.close(); return logger.toString(); } catch (Exception exp) { throw new RuntimeException( An error when execute RabbitMQ , exp); } } } package sh.platform.languages.sample; import redis.clients.jedis.Jedis; import redis.clients.jedis.JedisPool; import sh.platform.config.Config; import sh.platform.config.Redis; import java.util.Set; import java.util.function.Supplier; public class RedisSample implements Supplier\u003cString\u003e { @Override public String get() { StringBuilder logger = new StringBuilder(); // Create a new config object to ease reading the Platform.sh environment variables. // You can alternatively use getenv() yourself. Config config = new Config(); // The 'database' relationship is generally the name of primary database of an application. // It could be anything, though, as in the case here here where it's called  redis . Redis database = config.getCredential( redis , Redis::new); JedisPool dataSource = database.get(); // Get a Redis Client final Jedis jedis = dataSource.getResource(); // Set a values jedis.sadd( cities ,  Salvador ); jedis.sadd( cities ,  London ); jedis.sadd( cities ,  São Paulo ); // Read it back. Set\u003cString\u003e cities = jedis.smembers( cities ); logger.append( cities:   + cities); jedis.del( cities ); return logger.toString(); } } package sh.platform.languages.sample; import org.apache.solr.client.solrj.SolrQuery; import org.apache.solr.client.solrj.SolrServerException; import org.apache.solr.client.solrj.impl.HttpSolrClient; import org.apache.solr.client.solrj.impl.XMLResponseParser; import org.apache.solr.client.solrj.response.QueryResponse; import org.apache.solr.client.solrj.response.UpdateResponse; import org.apache.solr.common.SolrDocumentList; import org.apache.solr.common.SolrInputDocument; import sh.platform.config.Config; import sh.platform.config.Solr; import java.io.IOException; import java.util.function.Supplier; public class SolrSample implements Supplier\u003cString\u003e { @Override public String get() { StringBuilder logger = new StringBuilder(); // Create a new config object to ease reading the Platform.sh environment variables. // You can alternatively use getenv() yourself. Config config = new Config(); Solr solr = config.getCredential( solr , Solr::new); try { final HttpSolrClient solrClient = solr.get(); solrClient.setParser(new XMLResponseParser()); // Add a document SolrInputDocument document = new SolrInputDocument(); final String id =  123456 ; document.addField( id , id); document.addField( name ,  Ada Lovelace ); document.addField( city ,  London ); solrClient.add(document); final UpdateResponse response = solrClient.commit(); logger.append( Adding one document. Status (0 is success):  ) SolrQuery query = new SolrQuery(); query.set( q ,  city:London ); QueryResponse queryResponse = solrClient.query(query); SolrDocumentList results = queryResponse.getResults(); logger.append(String.format( Selecting documents (1 expected): %d results.getNumFound())); // Delete one document solrClient.deleteById(id); logger.append(String.format( Deleting one document. Status (0 is success): %s solrClient.commit().getStatus())); } catch (SolrServerException | IOException exp) { throw new RuntimeException( An error when execute Solr  , exp); } return logger.toString(); } } Project templates A number of project templates for major Java applications are available on GitHub. Not all of them are proactively maintained but all can be used as a starting point or reference for building your own website or web application. Apache Tomcat Apache Tomcat This project provides a starter kit for Apache Tomcat hosted on Platform.sh. Apache Tomcat is an open-source implementation of the Java Servlet, JavaServer Pages, Java Expression Language and WebSocket technologies. Services: Java 8 Maven Eclipse MicroProfile Apache Tomcat View the repository on GitHub. Apache TomEE Apache TomEE This project provides a starter kit for Apache TomEE Eclipse MicroProfile projects hosted on Platform.sh. Apache TomEE is the Eclipse MicroProfile implementation that uses several Apache Project flavors such as Apache Tomcat, Apache OpenWebBeans and so on. Services: Java 8 Maven Eclipse MicroProfile Apache TomEE View the repository on GitHub. Helidon Helidon This project provides a starter kit for Helidon Eclipse MicroProfile projects hosted on Platform.sh. Helidon is a collection of Java libraries for writing microservices that run on a fast web core powered by Netty. Helidon is designed to be simple to use, with tooling and examples to get you going quickly. Since Helidon is just a collection of libraries running on a fast Netty core, there is no extra overhead or bloat. Services: Java 8 Maven Eclipse MicroProfile Helidon View the repository on GitHub. Jenkins Jenkins This project provides a starter kit for Jenkins projects hosted on Platform.sh. Jenkins is an open source automation server written in Java. Jenkins helps to automate the non-human part of the software development process, with continuous integration and facilitating technical aspects of continuous delivery. Services: Java 8 Jenkins View the repository on GitHub. Jetty Jetty Eclipse Jetty provides a Web server and javax.servlet container, plus support for HTTP/2, WebSocket, OSGi, JMX, JNDI, JAAS and many other integrations. These components are open source and available for commercial use and distribution. Eclipse Jetty is used in a wide variety of projects and products, both in development and production. Jetty can be easily embedded in devices, tools, frameworks, application servers, and clusters. Services: Java 8 Maven Eclipse Jetty View the repository on GitHub. KumuluzEE KumuluzEE This project provides a starter kit for KumuluzEE Eclipse MicroProfile projects hosted on Platform.sh. KumuluzEE is a lightweight framework for developing microservices using standard Java, Java EE / Jakarta EE technologies and migrating existing Java applications to microservices. KumuluzEE packages microservices as standalone JARs. KumuluzEE microservices are lightweight and optimized for size and start-up time. Services: Java 8 Maven Eclipse MicroProfile KumuluzEE View the repository on GitHub. Micronaut Micronaut This project provides a starter kit for Micronaut projects hosted on Platform.sh. Micronaut is a modern, JVM-based, full-stack framework for building modular, easily testable microservice and serverless applications. Services: Java 8 Maven Micronaut View the repository on GitHub. Open Liberty Open Liberty This project provides a starter kit for Open Liberty Eclipse MicroProfile projects hosted on Platform.sh. Open Liberty is a highly composable, fast to start, dynamic application server runtime environment. Services: Java 8 Maven Eclipse MicroProfile Open Liberty View the repository on GitHub. Payara Micro Payara Micro This project provides a starter kit for Payara Micro projects hosted on Platform.sh. Payara Micro is an Open Source, lightweight Java EE (Jakarta EE) microservices deployments. Services: Java 8 Maven Eclipse MicroProfile Payara Micro View the repository on GitHub. Quarkus Quarkus QuarkusIO, the Supersonic Subatomic Java, promises to deliver small artifacts, extremely fast boot time, and lower time-to-first-request. Services: Java 8 Maven Eclipse MicroProfile Quarkus View the repository on GitHub. Spring Boot, Gradle, Mysql Spring Boot, Gradle, Mysql This project provides a starter kit for Spring Boot Gradle with MySQL projects hosted on Platform.sh. The Spring Framework is an application framework and inversion of control container for the Java platform. Services: Java 8 Gradle Spring Boot Oracle MySQL 8.0 View the repository on GitHub. Spring Boot, Maven, Mysql Spring Boot, Maven, Mysql This project provides a starter kit for Spring Boot Maven with MySQL projects hosted on Platform.sh. The Spring Framework is an application framework and inversion of control container for the Java platform. Services: Java 8 Maven Spring Boot Oracle MySQL 8.0 View the repository on GitHub. Spring MVC, Maven, MongoDB Spring MVC, Maven, MongoDB This project provides a starter kit for Spring MVC Maven with MongoDB projects hosted on Platform.sh. The Spring Framework is an application framework and inversion of control container for the Java platform. Services: Java 8 Maven Spring MVC MongoDB View the repository on GitHub. Spring, Kotlin, Maven Spring, Kotlin, Maven This project provides a starter kit for Spring Boot Maven with Kotlin projects hosted on Platform.sh. The Spring Framework is an application framework and inversion of control container for the Java platform. Services: Java 8 View the repository on GitHub. Thorntail Thorntail This project provides a starter kit for Thorntail Eclipse MicroProfile projects hosted on Platform.sh. Thorntail offers an innovative approach to packaging and running Java EE applications by packaging them with just enough of the server runtime to  java -jar  your application. It's MicroProfile compatible, too. Services: Java 8 Maven Eclipse MicroProfile Thorntail View the repository on GitHub. xwiki xwiki This project provides a starter kit for XWiki projects hosted on Platform.sh. XWiki is a free wiki software platform written in Java with a design emphasis on extensibility. XWiki is an enterprise wiki. It includes WYSIWYG editing, OpenDocument based document import/export, semantic annotations and tagging, and advanced permissions management. Services: Java 8 View the repository on GitHub.",
        "section": "Languages",
        "subsections": " Supported versions  OpenJDK versions:   Support libraries Support build automation Accessing services Project templates  ",
        "image": "",
        "url": "/languages/java.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "d5424971de8165dbf5a0caf913b7e384",
        "title": "Lisp",
        "description": "",
        "text": " Platform.sh supports building and deploying applications written in Lisp using Common Lisp (the SBCL version) with ASDF and Quick Lisp support. They are compiled during the Build phase, and support both committed dependencies and download-on-demand. Supported versions Grid Dedicated 1.5 None available To specify a Lisp container, use the type property in your .platform.app.yaml. type:\u0026#39;lisp:1.5\u0026#39; Assumptions Platform.sh is making assumptions about your application to provide a more streamlined experience. These assumptions are the following: Your .asd file is named like your system name. E.g. example.asd will have (defsystem example ...). Platform.sh will then run (asdf:make :example) on your system to build a binary. If you don’t want these assumptions, you can disable this behavior by specifying in your .platform.app.yaml: build:flavor:noneDependencies The recommended way to handle Lisp dependencies on Platform.sh is using ASDF. Commit a .asd file in your repository and the system will automatically download the dependencies using QuickLisp. QuickLisp options If you wish to change the distributions that QuickLisp is using, you can specify those as follows, specifying a distribution name, its URL and, an optional version: runtime:quicklisp:\u0026lt;distribution name\u0026gt;:url: ... version: ... For example: runtime:quicklisp:quicklisp:url:\u0026#39;http://beta.quicklisp.org/dist/quicklisp.txt\u0026#39;version:\u0026#39;2019-07-11\u0026#39;Platform.sh variables Platform.sh exposes relationships and other configuration as environment variables . Most notably, it allows a program to determine at runtime what HTTP port it should listen on and what the credentials are to access other services . To get the PORT environment variable (the port on which your web application is supposed to listen) you would: (parse-integer (uiop:getenv  PORT )) Building and running the application Assuming example.lisp and example.asd are present in your repository, the application will be automatically built on push. You can then start it from the web.commands.start directive. Note that the start command must run in the foreground. Should the program terminate for any reason it will be automatically restarted. In the example below we sleep for a very, very long time. You could also choose to join the thread of your web server, or use other methods to make sure the program does not terminate. The following basic .platform.app.yaml file is sufficient to run most Lisp applications. name:apptype:lisp:1.5web:commands:start:./examplelocations:/:allow:falsepassthru:truedisk:512Note that there will still be a proxy server in front of your application. If desired, certain paths may be served directly by our router without hitting your application (for static files, primarily) or you may route all requests to the Lisp application unconditionally, as in the example above. Accessing Services The services configuration is available in the environment variable PLATFORM_RELATIONSHIPS. To parse them, add the dependencies to your .asd file: :depends-on (:jsown :babel :s-base64) The following is an example of accessing a PostgreSQL instance: (defun relationships () (jsown:parse (babel:octets-to-string (with-input-from-string (in (uiop:getenv  PLATFORM_RELATIONSHIPS )) (s-base64:decode-base64-bytes in))))) Given a relationship defined in .platform.app.yaml: relationships:pg:postgresql:postgresqlThe following would access that relationship, and provide your Lisp program the credentials to connect to a PostgreSQL instance. Add this to your .asd file: :depends-on (:postmodern) Then in your program you could access the PostgreSQL instance as follows: (defvar *pg-spec* nil) (defun setup-postgresql () (let* ((pg-relationship (first (jsown:val (relationships)  pg ))) (database (jsown:val pg-relationship  path )) (username (jsown:val pg-relationship  username )) (password (jsown:val pg-relationship  password )) (host (jsown:val pg-relationship  host ))) (setf *pg-spec* (list database username password host))) (postmodern:with-connection *pg-spec* (unless (member  example_table  (postmodern:list-tables t) :test #\u0026#39;string=) (postmodern:execute  create table example_table ( a_field TEXT NOT NULL UNIQUE, another_field TEXT NOT NULL UNIQUE  )))) Project templates Platform.sh offers a project template for Lisp applications using the structure described above. It can be used as a starting point or reference for building your own website or web application. The following is a simple example of a Hunchentoot based web application (you can find the corresponding .asd and Platform.sh .yaml files in the linked Github repository): (defpackage #:example (:use :hunchentoot :cl-who :cl) (:export main)) (in-package #:example) (define-easy-handler (greet :uri  /hello ) (name) (with-html-output-to-string (s) (htm (:body (:h1  hello,   (str name)))))) (defun main () (let ((acceptor (make-instance \u0026#39;easy-acceptor :port (parse-integer (uiop:getenv  PORT ))))) (start acceptor) (sleep most-positive-fixnum))) Notice how we get the PORT from the environment, and how we sleep at the end, as (start acceptor) will immediately yield and Platform.sh requires applications to run in the foreground. Lisp Hunchentoot Lisp Hunchentoot This template is a simple Lisp Hunchentoot web server on Platform.sh. It includes a minimalist application for demonstration, but you are free to alter it as needed. Hunchentoot is a web server written in Common Lisp and at the same time a toolkit for building dynamic websites. Services: Lisp 1.5 View the repository on GitHub.",
        "section": "Languages",
        "subsections": " Supported versions Assumptions Dependencies QuickLisp options Platform.sh variables Building and running the application Accessing Services Project templates  ",
        "image": "",
        "url": "/languages/lisp.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "2efb1733847bc7285022ba88f1437dcb",
        "title": "New Relic",
        "description": "",
        "text": " Platform.sh supports New Relic APM for profiling PHP applications. These instructions do not apply to other languages. On a Grid plan 1. Get your license key Sign up at https://newrelic.com and get your license key. 2. Add your license key Add your New Relic license key as a project level variable: platform variable:create --visible-build false php:newrelic.license --value \u0026#39;\u0026lt;your-new-relic-license-key\u0026gt;\u0026#39; 3. Enable the New Relic extension Enable the New Relic extension in your .platform.app.yaml as follows: runtime: extensions: - newrelic Push the changes to your Platform.sh environment to enable New Relic as follows: git add .platform.app.yaml git commit -m  Enable New Relic.  git push That’s it! You need to wait a little bit for your New Relic dashboard to be generated. On a Dedicated cluster Sign up at https://newrelic.com and get your license key. Then open a support ticket and let us know what your key is. Our support team will install it and let you know when it is complete. Troubleshoot Additionally, you can check that your application is properly connected to New Relic by looking at the /var/log/app.log file: platform log app 2017/04/19 14:00:16.706450 (93) Info: Reporting to: https://rpm.newrelic.com/accounts/xxx/applications/xxx 2017/04/19 14:00:16.706668 (93) Info: app \u0026#39;xxx-master-xxx.app\u0026#39; connected with run id \u0026#39;xxx\u0026#39;",
        "section": "Profiling",
        "subsections": " On a Grid plan  1. Get your license key 2. Add your license key 3. Enable the New Relic extension   On a Dedicated cluster Troubleshoot  ",
        "image": "",
        "url": "/integrations/profiling/new-relic.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "017ce07079a6d1ddd058801fae6a1278",
        "title": "Node.js",
        "description": "",
        "text": " Node.js is a popular JavaScript runtime built on Chrome’s V8 JavaScript engine. Platform.sh supports deploying Node.js applications quickly and easily. Using our Multi-App support you can build a micro-service oriented system mixing both Javascript and PHP applications. Supported versions Grid Dedicated 6 8 10 12 10 If you need other versions, take a look at our options for installing them with NVM . Deprecated versions Some versions with a minor (such as 8.9) are available but are not receiving security updates from upstream, so their use is not recommended. They will be removed at some point in the future. Grid Dedicated 0.12 4.7 4.8 6.1 6.9 8.2 9.8 Support libraries While it is possible to read the environment directly from your application, it is generally easier and more robust to use the platformsh-config NPM library which handles decoding of service credential information for you. Configuration To use Platform.sh and Node.js together, configure the .platform.app.yaml file with a few key settings, as described here (a complete example is included at the end). Specify the language of your application (available versions are listed above): type:'nodejs:12' Specify your dependencies under the nodejs key, like this: dependencies:nodejs:pm2: ^2.5.0 These are the global dependencies of your project (the ones you would have installed with npm install -g). Here we specify the pm2 process manager that will allow us to run the node process. Configure the command you use to start serving your application (this must be a foreground-running process) under the web section, e.g.: web:commands:start: PM2_HOME=/app/run pm2 start index.js --no-daemon If there is a package.json file present at the root of your repository, Platform.sh will automatically install the dependencies. We suggest including the platformsh-config helper npm module, which makes it trivial to access the running environment. {  dependencies : {  platformsh-config :  ^2.0.0  } } Note: If using the pm2 process manager to start your application, it is recommended that you do so directly in web.commands.start as described above, rather than by calling a separate script the contains that command. Calling pm2 start at web.commands.start from within a script, even with the --no-daemon flag, has been found to daemonize itself and block other processes (such as backups) with continuous respawns. Create any Read/Write mounts. The root file system is read only. You must explicitly describe writable mounts. In (3) we set the home of the process manager to /app/run so this needs to be writable. mounts:run:source:localsource_path:run Include any relevant commands needed to build and setup your application in the hooks section, e.g.: hooks:build:| npm installnpmrunbuildbowerinstall Setup the routes to your Node.js application in .platform/routes.yaml.  https://{default}/ :type:upstreamupstream: app:http  (Optional) If Platform.sh detects a package.json file in your repository, it will automatically include a default build flavor , that will run npm prune --userconfig .npmrc \u0026\u0026 npm install --userconfig .npmrc. You can modify that process to use an alternative package manager by including the following in your .platform.app.yaml file: build:flavor:noneConsult the documentation specific to Node.js builds for more information. Here’s a complete example that also serves static assets (.png from the /public directory): name:nodetype:nodejs:12web:commands:start: PM2_HOME=/app/run pm2 start index.js --no-daemon #in this setup you will find your application stdout and stderr in /app/run/logslocations: /public :passthru:falseroot: public # Whether to allow files not matching a rule or your application… Finally, make sure your Node.js application is configured to listen over the port given by the environment (here we use the platformsh helper and get it from config.port) that is available in the environment variable PORT. Here’s an example: // Load the http module to create an http server. const http = require('http'); // Load the Platform.sh configuration const config = require('platformsh-config').config(); const server = http.createServer(function (request, response) { response.writeHead(200, { Content-Type :  text/html }); response.end( \u003chtml\u003e\u003chead\u003e\u003ctitle\u003eHello Node.js\u003c/title\u003e\u003c/head\u003e\u003cbody\u003e\u003ch1\u003e\u003cimg src='public/js.png'\u003eHello Node.js\u003c/h1\u003e\u003ch3\u003ePlatform configuration:\u003c/h3\u003e\u003cpre\u003e +JSON.stringify(config, null, 4) +  \u003c/pre\u003e\u003c/body\u003e\u003c/html\u003e ); }); server.listen(config.port); Accessing services To access various services with Node.js, see the following examples. The individual service pages have more information on configuring each service. Elasticsearch Memcached MongoDB MySQL PostgreSQL Redis Solr const elasticsearch = require('elasticsearch'); const config = require( platformsh-config ).config(); exports.usageExample = async function() { const credentials = config.credentials('elasticsearch'); var client = new elasticsearch.Client({ host: `${credentials.host}:${credentials.port}`, }); let index = 'my_index'; let type = 'People'; // Index a few document. let names = ['Ada Lovelace', 'Alonzo Church', 'Barbara Liskov']; let message = { refresh:  wait_for , body: [] }; names.forEach((name) =\u003e { message.body.push({index: {_index: index, _type: type}}); message.body.push({name: name}); }); await client.bulk(message); // Search for documents. const response = await client.search({ index: index, q: 'name:Barbara Liskov' }); let output = ''; if(response.hits.total.value \u003e 0) { output += `\u003ctable\u003e \u003cthead\u003e \u003ctr\u003e\u003cth\u003eID\u003c/th\u003e\u003cth\u003eName\u003c/th\u003e\u003c/tr\u003e \u003c/thead\u003e \u003ctbody\u003e`; response.hits.hits.forEach((record) =\u003e { output += }); output += } else { output =  No records found. ; } // Clean up after ourselves. response.hits.hits.forEach((record) =\u003e { client.delete({ index: index, type: type, id: record._id, }); }); return output; }; const Memcached = require('memcached'); const config = require( platformsh-config ).config(); const { promisify } = require('util'); exports.usageExample = async function() { const credentials = config.credentials('memcached'); let client = new Memcached(`${credentials.host}:${credentials.port}`); // The MemcacheD client is not Promise-aware, so make it so. const memcachedGet = promisify(client.get).bind(client); const memcachedSet = promisify(client.set).bind(client); let key = 'Deploy-day'; let value = 'Friday'; // Set a value. await memcachedSet(key, value, 10); // Read it back. let test = await memcachedGet(key); let output = `Found value \u003cstrong\u003e${test}\u003c/strong\u003e for key \u003cstrong\u003e${key}\u003c/strong\u003e.`; return output; }; const mongodb = require('mongodb'); const config = require( platformsh-config ).config(); exports.usageExample = async function() { const credentials = config.credentials('mongodb'); const MongoClient = mongodb.MongoClient; var client = await MongoClient.connect(config.formattedCredentials('mongodb', 'mongodb')); let db = client.db(credentials[ path ]); let collection = db.collection( startrek ); const documents = [ {'name': 'James Kirk', 'rank': 'Admiral'}, {'name': 'Jean-Luc Picard', 'rank': 'Captain'}, {'name': 'Benjamin Sisko', 'rank': 'Prophet'}, {'name': 'Katheryn Janeway', 'rank': 'Captain'}, ]; await collection.insert(documents, {w: 1}); let result = await collection.find({rank: Captain }).toArray(); let output = ''; output += `\u003ctable\u003e \u003cthead\u003e \u003ctr\u003e\u003cth\u003eName\u003c/th\u003e\u003cth\u003eRank\u003c/th\u003e\u003c/tr\u003e \u003c/thead\u003e \u003ctbody\u003e`; Object.keys(result).forEach((key) =\u003e { output += }); output += // Clean up after ourselves. collection.remove(); return output; }; const mysql = require('mysql2/promise'); const config = require( platformsh-config ).config(); exports.usageExample = async function() { const credentials = config.credentials('database'); const connection = await mysql.createConnection({ host: credentials.host, port: credentials.port, user: credentials.username, password: credentials.password, database: credentials.path, }); let sql = ''; // Creating a table. sql = `CREATE TABLE IF NOT EXISTS People ( id INT(6) UNSIGNED AUTO_INCREMENT PRIMARY KEY, name VARCHAR(30) NOT NULL, city VARCHAR(30) NOT NULL )`; await connection.query(sql); // Insert data. sql = `INSERT INTO People (name, city) VALUES ('Neil Armstrong', 'Moon'), ('Buzz Aldrin', 'Glen Ridge'), ('Sally Ride', 'La Jolla');`; await connection.query(sql); // Show table. sql = `SELECT * FROM People`; let [rows] = await connection.query(sql); let output = ''; if (rows.length \u003e 0) { output +=`\u003ctable\u003e \u003cthead\u003e \u003ctr\u003e\u003cth\u003eName\u003c/th\u003e\u003cth\u003eCity\u003c/th\u003e\u003c/tr\u003e \u003c/thead\u003e \u003ctbody\u003e`; rows.forEach((row) =\u003e { output += }); output += } // Drop table. sql = `DROP TABLE People`; await connection.query(sql); return output; }; const pg = require('pg'); const config = require( platformsh-config ).config(); exports.usageExample = async function() { const credentials = config.credentials('postgresql'); const client = new pg.Client({ host: credentials.host, port: credentials.port, user: credentials.username, password: credentials.password, database: credentials.path, }); client.connect(); let sql = ''; // Creating a table. sql = `CREATE TABLE IF NOT EXISTS People ( id SERIAL PRIMARY KEY, name VARCHAR(30) NOT NULL, city VARCHAR(30) NOT NULL )`; await client.query(sql); // Insert data. sql = `INSERT INTO People (name, city) VALUES ('Neil Armstrong', 'Moon'), ('Buzz Aldrin', 'Glen Ridge'), ('Sally Ride', 'La Jolla');`; await client.query(sql); // Show table. sql = `SELECT * FROM People`; let result = await client.query(sql); let output = ''; if (result.rows.length \u003e 0) { output +=`\u003ctable\u003e \u003cthead\u003e \u003ctr\u003e\u003cth\u003eName\u003c/th\u003e\u003cth\u003eCity\u003c/th\u003e\u003c/tr\u003e \u003c/thead\u003e \u003ctbody\u003e`; result.rows.forEach((row) =\u003e { output += }); output += } // Drop table. sql = `DROP TABLE People`; await client.query(sql); return output; }; const redis = require('redis'); const config = require( platformsh-config ).config(); const { promisify } = require('util'); exports.usageExample = async function() { const credentials = config.credentials('redis'); var client = redis.createClient(credentials.port, credentials.host); // The Redis client is not Promise-aware, so make it so. const redisGet = promisify(client.get).bind(client); const redisSet = promisify(client.set).bind(client); let key = 'Deploy day'; let value = 'Friday'; // Set a value. await redisSet(key, value); // Read it back. let test = await redisGet(key); let output = `Found value \u003cstrong\u003e${test}\u003c/strong\u003e for key \u003cstrong\u003e${key}\u003c/strong\u003e.`; return output; }; const solr = require('solr-node'); const config = require( platformsh-config ).config(); exports.usageExample = async function() { let client = new solr(config.formattedCredentials('solr', 'solr-node')); let output = ''; // Add a document. let addResult = await client.update({ id: 123, name: 'Valentina Tereshkova', }); output +=  Adding one document. Status (0 is success):   + addResult.responseHeader.status +  \u003cbr // Flush writes so that we can query against them. await client.softCommit(); // Select one document: let strQuery = client.query().q(); let writeResult = await client.search(strQuery); output +=  Selecting documents (1 expected):   + writeResult.response.numFound +  \u003cbr // Delete one document. let deleteResult = await client.delete({id: 123}); output +=  Deleting one document. Status (0 is success):   + deleteResult.responseHeader.status +  \u003cbr return output; }; Project templates A number of project templates for Node.js applications and typical configurations are available on GitHub. Not all of them are proactively maintained but all can be used as a starting point or reference for building your own website or web application. Express Express This template builds the Express framework for Platform.sh. It includes a minimalist application skeleton for demonstration, but you are free to alter it as needed. Express is a minimalist web framework written in Node.js. Services: Node.js 10 MariaDB 10.2 View the repository on GitHub. Gatsby Gatsby This template builds a simple application using Gatsby hosted on Platform.sh. Gatsby is a free and open source framework based on React that helps developers build blazing fast websites and apps. Services: Node.js View the repository on GitHub. Gatsby with Wordpress Gatsby with Wordpress This template builds a multi-app project using Gatsby as its frontend and a Wordpress to store content. Gatsby is a free and open source framework based on React that helps developers build statically-generated websites and apps, and WordPress is a blogging and lightweight CMS written in PHP. Services: Node.js 12 PHP 7.3 MariaDB 10.4 View the repository on GitHub. Koa Koa This template builds a Koa project on Platform.sh. Koa is a lightweight web microframework for Node.js. Services: Node.js 10 MariaDB 10.2 View the repository on GitHub. Node.js Node.js This template builds a simple application using the Node.js built-in `http` web server. It includes a minimalist application skeleton for demonstration, but you are free to alter it as needed. Node.js is an open-source JavaScript runtime built on Chrome's V8 JavaScript engine. Services: Node.js 10 MariaDB 10.4 View the repository on GitHub. Probot Probot This template builds a simple GitHub App using Probot. Probot is a framework for building GitHub Apps in Node.js. Services: Node.js 12 View the repository on GitHub. strapi strapi This template builds a Strapi backend for Platform.sh. It does not include a frontend application, but you can add one of your choice and access Strapi by defining it in a relationship in your frontend's .platform.app.yaml file. Strapi is a Headless CMS framework written in Node.js. Services: Node.js 12 PostgreSQL 11 View the repository on GitHub.",
        "section": "Languages",
        "subsections": " Supported versions Deprecated versions Support libraries Configuration In your application\u0026hellip; Accessing services Project templates  ",
        "image": "",
        "url": "/languages/nodejs.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "026e36b88aa3e09df907be8c1fdce24d",
        "title": "PHP",
        "description": "",
        "text": " Supported versions Grid Dedicated 7.2 7.3 7.4 7.2 7.3 Note that as of PHP 7.1 we use the Zend Thread Safe (ZTS) version of PHP. To specify a PHP container, use the type property in your .platform.app.yaml. type:'php:7.4' Deprecated versions The following versions are available but are not receiving security updates from upstream, so their use is not recommended. They will be removed at some point in the future. Grid Dedicated 5.4 5.5 5.6 7.0 7.1 5.6 7.0 7.1 Support libraries While it is possible to read the environment directly from your application, it is generally easier and more robust to use the platformsh/config-reader Composer library which handles decoding of service credential information for you. Alternate start commands PHP is most commonly run in a CGI mode, using PHP-FPM. That is the default on Platform.sh. However, you can also start alternative processes if desired, such as if you’re running an Async PHP daemon, a thread-based worker process, etc. To do so, simply specify an alternative start command in platform.app.yaml, similar to the following: web:commands:start:phprun.phpupstream:socket_family:tcpprotocol:httpThe above configuration will execute the run.php script in the application root when the container starts using the PHP-CLI SAPI, just before the deploy hook runs, but will not launch PHP-FPM. It will also tell the front-controller (Nginx) to connect to your application via a TCP socket, which will be specified in the PORT environment variable. Note that the start command must run in the foreground. If not specified, the effective default start command varies by PHP version: On PHP 5.x, it’s /usr/sbin/php5-fpm. On PHP 7.0, it’s /usr/sbin/php-fpm7.0. On PHP 7.1, it’s /usr/sbin/php-fpm7.1-zts. On PHP 7.2, it’s /usr/sbin/php-fpm7.2-zts. On PHP 7.3, it’s /usr/sbin/php-fpm7.3-zts. On PHP 7.4, it’s /usr/sbin/php-fpm7.4-zts. While you can call it manually that is generally not necessary. Note that PHP-FPM cannot run simultaneously along with another persistent process (such as ReactPHP or Amp). If you need both they will have to run in separate containers. Expanded dependencies In addition to the standard dependencies format, it is also possible to specify alternative repositories for use by Composer. The standard format like so: dependencies:php: platformsh/client :  dev-master is equivalent to composer require platform/client dev-master. However, you can also specify explicit require and repositories blocks: dependencies:php:require: platformsh/client :  dev-master repositories:- type:vcsurl: git@github.com:platformsh/platformsh-client-php.git That would install platformsh/client from the alternate repository specified, as a global dependency. That is, it is equivalent to the following composer.json file: {  repositories : [ {  type :  vcs ,  url :  git@github.com:platformsh/platformsh-client-php.git  } ],  require : {  platformsh/client :  dev-master  } } That allows you to install a forked version of a global dependency from a custom repository. Opcache preloading PHP 7.4 introduced a new feature called Opcache Preloading, which allows you to load selected files into shared memory when PHP-FPM starts. That means functions and classes in those files are always available and do not need to be autoloaded, at the cost of any changes to those files requiring a PHP-FPM restart. Since PHP-FPM restarts anyway when a new deploy happens this feature is a major win on Platform.sh, and we recommend using it aggressively. To enable preloading, add a php.ini value that specifies a preload script. Any php.ini mechanism will work, but using a variable in .platform.app.yaml is the recommended approach: variables:php:opcache.preload:'preload.php'The opcache.preload value is evaluated as a file path relative to the application root (where .platform.app.yaml is), and it may be any PHP script that calls opcache_compile_file(). The following example will preload all .php files anywhere in the vendor directory: $directory = new RecursiveDirectoryIterator(getenv('PLATFORM_APP_DIR') . '/vendor'); $iterator = new RecursiveIteratorIterator($directory); $regex = new RegexIterator($iterator, RecursiveRegexIterator::GET_MATCH); foreach ($regex as $key =\u003e $file) { // This is the important part! opcache_compile_file($file[0]); } Note: Preloading all .php files may not be optimal for your application, and may even introduce errors. Your application framework may provide recommendations or a pre-made presload script to use instead. Determining an optimal preloading strategy is the user’s responsibility. FFI PHP 7.4 introduced support for Foreign Function Interfaces (FFI), which allows user-space code to bridge to existing C-ABI-compatible libraries. FFI is fully supported on Platform.sh. Note: FFI is only intended for advanced use cases, and is rarely a net win for routine web requests. Use with caution. There are a few steps to leveraging FFI: Enable the FFI extension in .platform.app.yaml: runtime:extensions:- ffi Specify a preload file in which you can call FFI::load(). Using FFI::load() in preload will be considerably faster than loading the linked library on each request or script run. Ensure the library is available locally, but not in a web-accessible directory. .so files may included in your repository, downloaded i your build hook, or compiled in your build hook. If compiling C code, gcc is available by default. If compiling Rust code, you can download the Rust compiler in the build hook . For running FFI from the command line, you will need to enable the opcache for command line scripts in addition to the preloader. The standard pattern for the command would be php -d opcache.preload= your-preload-script.php  -d opcache.enable_cli=true your-cli-script.php. A working FFI example is available online for both C and Rust. Debug PHP-FPM If you want to inspect what’s going on with PHP-FPM, you can install this small CLI : dependencies:php:wizaplace/php-fpm-status-cli: ^1.0 Then when you are connected to your project over SSH you can run: $ php-fpm-status --socket=unix://$SOCKET --path=/-/status --full Accessing services To access various services with PHP, see the following examples. The individual service pages have more information on configuring each service. Elasticsearch Memcached MongoDB MySQL PostgreSQL RabbitMQ Redis Solr \u003c?php declare(strict_types=1); use use // Create a new config object to ease reading the Platform.sh environment variables. // You can alternatively use getenv() yourself. $config = new Config(); // Get the credentials to connect to the Elasticsearch service. $credentials = $config-\u003ecredentials('elasticsearch'); try { // The Elasticsearch library lets you connect to multiple hosts. // On Platform.sh Standard there is only a single host so just // register that. $hosts = [ [ 'scheme' =\u003e $credentials['scheme'], 'host' =\u003e $credentials['host'], 'port' =\u003e $credentials['port'], ] ]; // Create an Elasticsearch client object. $builder = ClientBuilder::create(); $builder-\u003esetHosts($hosts); $client = $builder-\u003ebuild(); $index = 'my_index'; $type = 'People'; // Index a few document. $params = [ 'index' =\u003e $index, 'type' =\u003e $type, ]; $names = ['Ada Lovelace', 'Alonzo Church', 'Barbara Liskov']; foreach ($names as $name) { $params['body']['name'] = $name; $client-\u003eindex($params); } // Force just-added items to be indexed. $client-\u003eindices()-\u003erefresh(array('index' =\u003e $index)); // Search for documents. $result = $client-\u003esearch([ 'index' =\u003e $index, 'type' =\u003e $type, 'body' =\u003e [ 'query' =\u003e [ 'match' =\u003e [ 'name' =\u003e 'Barbara Liskov', ], ], ], ]); if (isset($result['hits']['hits'])) { print \u003c\u003c\u003cTABLE\u003ctable\u003e \u003cthead\u003e \u003ctr\u003e\u003cth\u003eID\u003c/th\u003e\u003cth\u003eName\u003c/th\u003e\u003c/tr\u003e \u003c/thead\u003e \u003ctbody\u003e TABLE; foreach ($result['hits']['hits'] as $record) { $record['_id'], $record['_source']['name']); } print } // Delete documents. $params = [ 'index' =\u003e $index, 'type' =\u003e $type, ]; $ids = array_map(function($row) { return $row['_id']; }, $result['hits']['hits']); foreach ($ids as $id) { $params['id'] = $id; $client-\u003edelete($params); } } catch (Exception $e) { print $e-\u003egetMessage(); } \u003c?php declare(strict_types=1); use // Create a new config object to ease reading the Platform.sh environment variables. // You can alternatively use getenv() yourself. $config = new Config(); // Get the credentials to connect to the Memcached service. $credentials = $config-\u003ecredentials('memcached'); try { // Connecting to Memcached server. $memcached = new Memcached(); $memcached-\u003eaddServer($credentials['host'], $credentials['port']); $memcached-\u003esetOption(Memcached::OPT_BINARY_PROTOCOL, true); $key =  Deploy day ; $value =  Friday ; // Set a value. $memcached-\u003eset($key, $value); // Read it back. $test = $memcached-\u003eget($key); printf('Found value \u003cstrong\u003e%s\u003c/strong\u003e for key \u003cstrong\u003e%s\u003c/strong\u003e.', $test, $key); } catch (Exception $e) { print $e-\u003egetMessage(); } \u003c?php declare(strict_types=1); use use // Create a new config object to ease reading the Platform.sh environment variables. // You can alternatively use getenv() yourself. $config = new Config(); // The 'database' relationship is generally the name of primary database of an application. // It could be anything, though, as in the case here here where it's called  mongodb . $credentials = $config-\u003ecredentials('mongodb'); try { $server = sprintf('%s://%s:%s@%s:%d/%s', $credentials['scheme'], $credentials['username'], $credentials['password'], $credentials['host'], $credentials['port'], $credentials['path'] ); $client = new Client($server); $collection = $client-\u003emain-\u003estarwars; $result = $collection-\u003einsertOne([ 'name' =\u003e 'Rey', 'occupation' =\u003e 'Jedi', ]); $id = $result-\u003egetInsertedId(); $document = $collection-\u003efindOne([ '_id' =\u003e $id, ]); // Clean up after ourselves. $collection-\u003edrop(); printf( Found %s (%s)\u003cbr $document-\u003ename, $document-\u003eoccupation); } catch $e) { print $e-\u003egetMessage(); } \u003c?php declare(strict_types=1); use // Create a new config object to ease reading the Platform.sh environment variables. // You can alternatively use getenv() yourself. $config = new Config(); // The 'database' relationship is generally the name of primary SQL database of an application. // That's not required, but much of our default automation code assumes it. $credentials = $config-\u003ecredentials('database'); try { // Connect to the database using PDO. If using some other abstraction layer you would // inject the values from $database into whatever your abstraction layer asks for. $dsn = sprintf('mysql:host=%s;port=%d;dbname=%s', $credentials['host'], $credentials['port'], $credentials['path']); $conn = new $credentials['username'], $credentials['password'], [ // Always use Exception error mode with PDO, as it's more reliable. =\u003e // So we don't have to mess around with cursors and unbuffered queries by default. =\u003e TRUE, // Make sure MySQL returns all matched rows on update queries including // rows that actually didn't have to be updated because the values didn't // change. This matches common behavior among other database systems. =\u003e TRUE, ]); // Creating a table. $sql =  CREATE TABLE People ( id INT(6) UNSIGNED AUTO_INCREMENT PRIMARY KEY, name VARCHAR(30) NOT NULL, city VARCHAR(30) NOT NULL ) ; $conn-\u003equery($sql); // Insert data. $sql =  INSERT INTO People (name, city) VALUES ('Neil Armstrong', 'Moon'), ('Buzz Aldrin', 'Glen Ridge'), ('Sally Ride', 'La Jolla'); ; $conn-\u003equery($sql); // Show table. $sql =  SELECT * FROM People ; $result = $conn-\u003equery($sql); if ($result) { print \u003c\u003c\u003cTABLE\u003ctable\u003e \u003cthead\u003e \u003ctr\u003e\u003cth\u003eName\u003c/th\u003e\u003cth\u003eCity\u003c/th\u003e\u003c/tr\u003e \u003c/thead\u003e \u003ctbody\u003e TABLE; foreach ($result as $record) { $record-\u003ename, $record-\u003ecity); } print } // Drop table $sql =  DROP TABLE People ; $conn-\u003equery($sql); } catch $e) { print $e-\u003egetMessage(); } \u003c?php declare(strict_types=1); use // Create a new config object to ease reading the Platform.sh environment variables. // You can alternatively use getenv() yourself. $config = new Config(); // The 'database' relationship is generally the name of primary SQL database of an application. // It could be anything, though, as in the case here here where it's called  postgresql . $credentials = $config-\u003ecredentials('postgresql'); try { // Connect to the database using PDO. If using some other abstraction layer you would // inject the values from $database into whatever your abstraction layer asks for. $dsn = sprintf('pgsql:host=%s;port=%d;dbname=%s', $credentials['host'], $credentials['port'], $credentials['path']); $conn = new $credentials['username'], $credentials['password'], [ // Always use Exception error mode with PDO, as it's more reliable. =\u003e // So we don't have to mess around with cursors and unbuffered queries by default. ]); $conn-\u003equery( DROP TABLE IF EXISTS People ); // Creating a table. $sql =  CREATE TABLE IF NOT EXISTS People ( id SERIAL PRIMARY KEY, name VARCHAR(30) NOT NULL, city VARCHAR(30) NOT NULL ) ; $conn-\u003equery($sql); // Insert data. $sql =  INSERT INTO People (name, city) VALUES ('Neil Armstrong', 'Moon'), ('Buzz Aldrin', 'Glen Ridge'), ('Sally Ride', 'La Jolla'); ; $conn-\u003equery($sql); // Show table. $sql =  SELECT * FROM People ; $result = $conn-\u003equery($sql); if ($result) { print \u003c\u003c\u003cTABLE\u003ctable\u003e \u003cthead\u003e \u003ctr\u003e\u003cth\u003eName\u003c/th\u003e\u003cth\u003eCity\u003c/th\u003e\u003c/tr\u003e \u003c/thead\u003e \u003ctbody\u003e TABLE; foreach ($result as $record) { $record-\u003ename, $record-\u003ecity); } print } // Drop table. $sql =  DROP TABLE People ; $conn-\u003equery($sql); } catch $e) { print $e-\u003egetMessage(); } \u003c?php declare(strict_types=1); use use use // Create a new config object to ease reading the Platform.sh environment variables. // You can alternatively use getenv() yourself. $config = new Config(); // Get the credentials to connect to the RabbitMQ service. $credentials = $config-\u003ecredentials('rabbitmq'); try { $queueName = 'deploy_days'; // Connect to the RabbitMQ server. $connection = new AMQPStreamConnection($credentials['host'], $credentials['port'], $credentials['username'], $credentials['password']); $channel = $connection-\u003echannel(); $channel-\u003equeue_declare($queueName, false, false, false, false); $msg = new AMQPMessage('Friday'); $channel-\u003ebasic_publish($msg, '', 'hello'); echo  [x] Sent // In a real application you't put the following in a separate script in a loop. $callback = function ($msg) { printf( [x] Deploying on %s\u003cbr $msg-\u003ebody); }; $channel-\u003ebasic_consume($queueName, '', false, true, false, false, $callback); // This blocks on waiting for an item from the queue, so comment it out in this demo script. //$channel-\u003ewait(); $channel-\u003eclose(); $connection-\u003eclose(); } catch (Exception $e) { print $e-\u003egetMessage(); } \u003c?php declare(strict_types=1); use // Create a new config object to ease reading the Platform.sh environment variables. // You can alternatively use getenv() yourself. $config = new Config(); // Get the credentials to connect to the Redis service. $credentials = $config-\u003ecredentials('redis'); try { // Connecting to Redis server. $redis = new Redis(); $redis-\u003econnect($credentials['host'], $credentials['port']); $key =  Deploy day ; $value =  Friday ; // Set a value. $redis-\u003eset($key, $value); // Read it back. $test = $redis-\u003eget($key); printf('Found value \u003cstrong\u003e%s\u003c/strong\u003e for key \u003cstrong\u003e%s\u003c/strong\u003e.', $test, $key); } catch (Exception $e) { print $e-\u003egetMessage(); } \u003c?php declare(strict_types=1); use use // Create a new config object to ease reading the Platform.sh environment variables. // You can alternatively use getenv() yourself. $config = new Config(); // Get the credentials to connect to the Solr service. $credentials = $config-\u003ecredentials('solr'); try { $config = [ 'endpoint' =\u003e [ 'localhost' =\u003e [ 'host' =\u003e $credentials['host'], 'port' =\u003e $credentials['port'], 'path' =\u003e  /  . $credentials['path'], ] ] ]; $client = new Client($config); // Add a document $update = $client-\u003ecreateUpdate(); $doc1 = $update-\u003ecreateDocument(); $doc1-\u003eid = 123; $doc1-\u003ename = 'Valentina Tereshkova'; $update-\u003eaddDocuments(array($doc1)); $update-\u003eaddCommit(); $result = $client-\u003eupdate($update); print  Adding one document. Status (0 is success):   .$result-\u003egetStatus().  \u003cbr // Select one document $query = $client-\u003ecreateQuery($client::QUERY_SELECT); $resultset = $client-\u003eexecute($query); print  Selecting documents (1 expected):   .$resultset-\u003egetNumFound() .  \u003cbr // Delete one document $update = $client-\u003ecreateUpdate(); $update-\u003eaddDeleteById(123); $update-\u003eaddCommit(); $result = $client-\u003eupdate($update); print  Deleting one document. Status (0 is success):   .$result-\u003egetStatus().  \u003cbr } catch (Exception $e) { print $e-\u003egetMessage(); } Runtime configuration It is possible to change the PHP-FPM runtime configuration via the runtime block on your .platform.app.yaml. The PHP-FPM options below are configurable: request_terminate_timeout - The timeout for serving a single request after which the PHP-FPM worker process will be killed. That is separate from the PHP runtime’s max_execution_time ini option, which is preferred. This option may be used if the PHP process is dying without cleaning up properly and causing the FPM process to hang. runtime:request_terminate_timeout:300 Project templates A number of project templates for major PHP applications are available on GitHub. Not all of them are proactively maintained but all can be used as a starting point or reference for building your own website or web application. Backdrop Backdrop This template builds a Backdrop site, with the entire site committed to Git. Backdrop is a PHP-based CMS, originally forked from Drupal 7. Services: PHP 7.3 MariaDB 10.4 View the repository on GitHub. Basic PHP Basic PHP This template provides the most basic configuration for running a custom PHP project. PHP is a high-performance scripting language especially well suited to web development. Services: PHP 7.3 MariaDB 10.2 View the repository on GitHub. Drupal 8 Drupal 8 This template builds Drupal 8 using the  Drupal Recommended  Composer project. It also includes configuration to use Redis for caching, although that must be enabled post-install in `.platform.app.yaml`. Drupal is a flexible and extensible PHP-based CMS framework. Services: PHP 7.4 MariaDB 10.4 Redis 5.0 View the repository on GitHub. Drupal 8 Multisite Drupal 8 Multisite This template builds Drupal 8 in a multisite configuration using the  Drupal Recommended  Composer project. It also includes configuration to use Redis for caching, although that must be enabled post-install per-site. Drupal is a flexible and extensible PHP-based CMS framework capable of hosting multiple sites on a single code base. Services: PHP 7.4 MariaDB 10.4 Redis 5.0 View the repository on GitHub. Drupal 9 Drupal 9 This template builds Drupal 9 Beta using the  Drupal Recommended  Composer project. It also includes configuration to use Redis for caching, although that must be enabled post-install in `.platform.app.yaml`. Drupal is a flexible and extensible PHP-based CMS framework. Services: PHP 7.4 MariaDB 10.4 Redis 5.0 View the repository on GitHub. GovCMS 8 GovCMS 8 This template builds the Australian government's GovCMS Drupal 8 distribution using the Drupal Composer project for better flexibility. It also includes configuration to use Redis for caching, although that must be enabled post-install in .platform.app.yaml. GovCMS is a Drupal distribution built for the Australian government, and includes configuration optimized for managing government websites. Services: PHP 7.2 MariaDB 10.2 Redis 5.0 View the repository on GitHub. Laravel Laravel This template provides a basic Laravel skeleton. It comes pre-configured to use a MariaDB database and Redis for caching and sessions. Laravel is an opinionated, integrated rapid-application-development framework for PHP. Services: PHP 7.3 MariaDB 10.2 Redis 5.0 View the repository on GitHub. Magento 2 Community Edition Magento 2 Community Edition This template builds Magento 2 CE on Platform.sh. It includes additional scripts to customize Magento to run effectively in a build-and-deploy environment. Magento is a fully integrated ecommerce system and web store written in PHP. This is the Open Source version. Services: PHP 7.2 MariaDB 10.2 Redis 3.2 View the repository on GitHub. Mautic Mautic TThis template provides a basic Mautic installation. Mautic is an Open SOurce marketing automation tool built on Symfony. Services: PHP 7.2 MariaDB 10.4 RabbitMQ 3.7 View the repository on GitHub. Nextcloud Nextcloud This template builds Nextcloud on Platform.sh. Nextcloud is a PHP-based groupware server with installable apps, file synchronization, and federated storage. An adminstrative user will be created automatically. See the deploy log after the project is installed for the name and password. Services: PHP 7.4 MariaDB 10.4 Redis 5.0 View the repository on GitHub. Opigno Opigno This template builds the Opigno Drupal 8 distribution using the Drupal Composer project for better flexibility. It also includes configuration to use Redis for caching, although that must be enabled post-install in .platform.app.yaml. Opigno is a Learning Management system built as a Drupal distribution. Services: PHP 7.3 MariaDB 10.4 Redis 5.0 View the repository on GitHub. Pimcore Pimcore Pimcore Digital Platform for Enterprises Services: PHP 7.4 MariaDB 10.2 Redis 5.0 View the repository on GitHub. Sculpin Sculpin This template provides a basic Sculpin skeleton. All files are generated at build time, so at runtime only static files need to be served. Sculpin is a static site generator written in PHP and using the Twig templating engine. Services: PHP 7.3 View the repository on GitHub. Symfony 3 Symfony 3 This template provides a basic Symfony 3 skeleton. It is configured for Production mode by default so the usual Symfony  welcome  page will not appear. Symfony is a high-performance loosely-coupled PHP web development framework. Version 3 is the legacy support version. Services: PHP 7.2 MariaDB 10.2 View the repository on GitHub. Symfony 4 Symfony 4 This template provides a basic Symfony 4 skeleton. It is configured for Production mode by default so the usual Symfony  welcome  page will not appear. That can be adjusted in .platform.app.yaml. Symfony is a high-performance loosely-coupled PHP web development framework. Services: PHP 7.3 MariaDB 10.4 View the repository on GitHub. Symfony 5 Symfony 5 This template provides a basic Symfony 5 skeleton. It is configured for Production mode by default so the usual Symfony  welcome  page will not appear. That can be adjusted in .platform.app.yaml. Symfony is a high-performance loosely-coupled PHP web development framework. Services: PHP 7.3 MariaDB 10.4 View the repository on GitHub. TYPO3 TYPO3 This template provides a basic TYPO3 installation. TYPO3 is a PHP-based Content Management System Services: PHP 7.4 MariaDB 10.4 Redis 5.0 View the repository on GitHub. Wordpress Wordpress This template builds WordPress on Platform.sh using the johnbolch/wordpress  Composer Fork  of WordPress. Plugins and themes should be managed with Composer exclusively. WordPress is a blogging and lightweight CMS written in PHP. Services: PHP 7.3 MariaDB 10.2 View the repository on GitHub.",
        "section": "Languages",
        "subsections": " Supported versions Deprecated versions Support libraries Alternate start commands Expanded dependencies Opcache preloading FFI Debug PHP-FPM Accessing services Runtime configuration Project templates  ",
        "image": "",
        "url": "/languages/php.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "b8f4f5f57583a0b82bbac52256f544cf",
        "title": "Python",
        "description": "",
        "text": " Platform.sh supports deploying Python applications. Your application can use WSGI-based (Gunicorn / uWSGI) application server, Tornado, Twisted, or Python 3.5\u0026#43; asyncio server. Supported Grid Dedicated 2.7 3.5 3.6 3.7 3.8 None available Support libraries While it is possible to read the environment directly from your application, it is generally easier and more robust to use the platformshconfig pip library which handles decoding of service credential information for you. WSGI-based configuration In this example, we use Gunicorn to run our WSGI application. Configure the .platform.app.yaml file with a few key settings as listed below, a complete example is included at the end of this section. Specify the language of your application (available versions are listed above): type:\u0026#39;python:3.8\u0026#39; Build your application with the build hook. Assuming you have your pip dependencies stored in requirements.txt and a setup.py at the root of your application folder to execute build steps: hooks:build:| pip install -r requirements.txtpipinstall-e.pipinstallgunicornThese are installed as global dependencies in your environment. Configure the command you use to start serving your application (this must be a foreground-running process) under the web section, e.g.: web:commands:start: gunicorn -b 0.0.0.0:$PORT project.wsgi:application This assumes the WSGI file is project/wsgi.py and the WSGI application object is named application in the WSGI file. Define the web locations your application is using: web:locations: / :root:  passthru:trueallow:false /static :root: static/ allow:trueThis configuration asks our web server to handle HTTP requests at “/static” to serve static files stored in /app/static/ folder while everything else is forwarded to your application server. Create any Read/Write mounts. The root file system is read only. You must explicitly describe writable mounts. mounts:tmp:source:localsource_path:tmplogs:source:localsource_path:logsThis setting allows your application writing files to /app/tmp and have logs stored in /app/logs. Then, set up the routes to your application in .platform/routes.yaml.  https://{default}/ :type:upstreamupstream: app:http Here is the complete .platform.app.yaml file: name:apptype:python:2.7web:commands:start: gunicorn -b $PORT project.wsgi:application locations: / :root:  passthru:trueallow:false /static :root: static/ allow:truehooks:build:| pip install -r requirements.txtpipinstall-e.pipinstallgunicornmounts:tmp:source:localsource_path:tmplogs:source:localsource_path:logsdisk:512Using the asyncio module The above Gunicorn based WSGI example can be modified to use the Python 3.5\u0026#43; asyncio module. Change the type to python:3.6. Change the start command to use asyncio. web:commands:start: gunicorn -b $PORT -k gaiohttp project.wsgi:application  Add aiohttp as pip dependency in your build hook. hooks:build:| pip install -r requirements.txtpipinstall-e.pipinstallgunicornaiohttp Accessing services To access various services with Python, see the following examples. The individual service pages have more information on configuring each service. Elasticsearch Kafka Memcached MongoDB MySQL PostgreSQL RabbitMQ Redis Solr import elasticsearch from platformshconfig import Config def usage_example(): # Create a new Config object to ease reading the Platform.sh environment variables. # You can alternatively use os.environ yourself. config = Config() # Get the credentials to connect to the Elasticsearch service. credentials = config.credentials(\u0026#39;elasticsearch\u0026#39;) try: # The Elasticsearch library lets you connect to multiple hosts. # On Platform.sh Standard there is only a single host so just register that. hosts = {  scheme : credentials[\u0026#39;scheme\u0026#39;],  host : credentials[\u0026#39;host\u0026#39;],  port : credentials[\u0026#39;port\u0026#39;] } # Create an Elasticsearch client object. client = elasticsearch.Elasticsearch([hosts]) # Index a few documents es_index = \u0026#39;my_index\u0026#39; es_type = \u0026#39;People\u0026#39; params = {  index : es_index,  type : es_type,  body : { name : \u0026#39;\u0026#39;} } names = [\u0026#39;Ada Lovelace\u0026#39;, \u0026#39;Alonzo Church\u0026#39;, \u0026#39;Barbara Liskov\u0026#39;] ids = {} for name in names: params[\u0026#39;body\u0026#39;][\u0026#39;name\u0026#39;] = name ids[name] = client.index(index=params[ index ], doc_type=params[ type ], body=params[\u0026#39;body\u0026#39;]) # Force just-added items to be indexed. client.indices.refresh(index=es_index) # Search for documents. result = client.search(index=es_index, body={ \u0026#39;query\u0026#39;: { \u0026#39;match\u0026#39;: { \u0026#39;name\u0026#39;: \u0026#39;Barbara Liskov\u0026#39; } } }) table = \u0026#39;\u0026#39;\u0026#39;\u0026lt;table\u0026gt; \u0026lt;thead\u0026gt; \u0026lt;tr\u0026gt;\u0026lt;th\u0026gt;ID\u0026lt;/th\u0026gt;\u0026lt;th\u0026gt;Name\u0026lt;/th\u0026gt;\u0026lt;/tr\u0026gt; \u0026lt;/thead\u0026gt; \u0026lt;tbody\u0026gt;\u0026#39;\u0026#39;\u0026#39; if result[\u0026#39;hits\u0026#39;][\u0026#39;hits\u0026#39;]: for record in result[\u0026#39;hits\u0026#39;][\u0026#39;hits\u0026#39;]: table \u0026#43;= record[\u0026#39;_source\u0026#39;][\u0026#39;name\u0026#39;]) table \u0026#43;= # Delete documents. params = {  index : es_index,  type : es_type, } for name in names: client.delete(index=params[\u0026#39;index\u0026#39;], doc_type=params[\u0026#39;type\u0026#39;], id=ids[name][\u0026#39;_id\u0026#39;]) return table except Exception as e: return e from json import dumps from json import loads from kafka import KafkaConsumer, KafkaProducer from platformshconfig import Config def usage_example(): # Create a new Config object to ease reading the Platform.sh environment variables. # You can alternatively use os.environ yourself. config = Config() # Get the credentials to connect to the Kafka service. credentials = config.credentials(\u0026#39;kafka\u0026#39;) try: kafka_server = \u0026#39;{}:{}\u0026#39;.format(credentials[\u0026#39;host\u0026#39;], credentials[\u0026#39;port\u0026#39;]) # Producer producer = KafkaProducer( bootstrap_servers=[kafka_server], value_serializer=lambda x: dumps(x).encode(\u0026#39;utf-8\u0026#39;) ) for e in range(10): data = {\u0026#39;number\u0026#39; : e} producer.send(\u0026#39;numtest\u0026#39;, value=data) # Consumer consumer = KafkaConsumer( bootstrap_servers=[kafka_server], auto_offset_reset=\u0026#39;earliest\u0026#39; ) consumer.subscribe([\u0026#39;numtest\u0026#39;]) output = \u0026#39;\u0026#39; # For demonstration purposes so it doesn\u0026#39;t block. for e in range(10): message = next(consumer) output \u0026#43;= str(loads(message.value.decode(\u0026#39;UTF-8\u0026#39;))[ number ]) \u0026#43; \u0026#39;, \u0026#39; # What a real implementation would do instead. # for message in consumer: # output \u0026#43;= loads(message.value.decode(\u0026#39;UTF-8\u0026#39;))[ number ] return output except Exception as e: return e import pymemcache from platformshconfig import Config def usage_example(): # Create a new Config object to ease reading the Platform.sh environment variables. # You can alternatively use os.environ yourself. config = Config() # Get the credentials to connect to the Memcached service. credentials = config.credentials(\u0026#39;memcached\u0026#39;) try: # Try connecting to Memached server. memcached = pymemcache.Client((credentials[\u0026#39;host\u0026#39;], credentials[\u0026#39;port\u0026#39;])) memcached.set(\u0026#39;Memcached::OPT_BINARY_PROTOCOL\u0026#39;, True) key =  Deploy_day  value =  Friday  # Set a value. memcached.set(key, value) # Read it back. test = memcached.get(key) return \u0026#39;Found value \u0026lt;strong\u0026gt;{0}\u0026lt;/strong\u0026gt; for key \u0026lt;strong\u0026gt;{1}\u0026lt;/strong\u0026gt;.\u0026#39;.format(test.decode( utf-8 ), key) except Exception as e: return e from pymongo import MongoClient from platformshconfig import Config def usage_example(): # Create a new Config object to ease reading the Platform.sh environment variables. # You can alternatively use os.environ yourself. config = Config() # The \u0026#39;database\u0026#39; relationship is generally the name of primary SQL database of an application. # It could be anything, though, as in the case here here where it\u0026#39;s called  mongodb . credentials = config.credentials(\u0026#39;mongodb\u0026#39;) try: formatted = config.formatted_credentials(\u0026#39;mongodb\u0026#39;, \u0026#39;pymongo\u0026#39;) server = \u0026#39;{0}://{1}:{2}@{3}\u0026#39;.format( credentials[\u0026#39;scheme\u0026#39;], credentials[\u0026#39;username\u0026#39;], credentials[\u0026#39;password\u0026#39;], formatted ) client = MongoClient(server) collection = client.main.starwars post = {  name :  Rey ,  occupation :  Jedi  } post_id = collection.insert_one(post).inserted_id document = collection.find_one( { _id : post_id} ) # Clean up after ourselves. collection.drop() return \u0026#39;Found {0} ({1})\u0026lt;br /\u0026gt;\u0026#39;.format(document[\u0026#39;name\u0026#39;], document[\u0026#39;occupation\u0026#39;]) except Exception as e: return e import pymysql from platformshconfig import Config def usage_example(): # Create a new Config object to ease reading the Platform.sh environment variables. # You can alternatively use os.environ yourself. config = Config() # The \u0026#39;database\u0026#39; relationship is generally the name of primary SQL database of an application. # That\u0026#39;s not required, but much of our default automation code assumes it.\u0026#39; credentials = config.credentials(\u0026#39;database\u0026#39;) try: # Connect to the database using PDO. If using some other abstraction layer you would inject the values # from `database` into whatever your abstraction layer asks for. conn = pymysql.connect(host=credentials[\u0026#39;host\u0026#39;], port=credentials[\u0026#39;port\u0026#39;], database=credentials[\u0026#39;path\u0026#39;], user=credentials[\u0026#39;username\u0026#39;], password=credentials[\u0026#39;password\u0026#39;]) sql = \u0026#39;\u0026#39;\u0026#39; CREATE TABLE People ( id SERIAL PRIMARY KEY, name VARCHAR(30) NOT NULL, city VARCHAR(30) NOT NULL ) \u0026#39;\u0026#39;\u0026#39; cur = conn.cursor() cur.execute(sql) sql = \u0026#39;\u0026#39;\u0026#39; INSERT INTO People (name, city) VALUES (\u0026#39;Neil Armstrong\u0026#39;, \u0026#39;Moon\u0026#39;), (\u0026#39;Buzz Aldrin\u0026#39;, \u0026#39;Glen Ridge\u0026#39;), (\u0026#39;Sally Ride\u0026#39;, \u0026#39;La Jolla\u0026#39;); \u0026#39;\u0026#39;\u0026#39; cur.execute(sql) # Show table. sql = \u0026#39;\u0026#39;\u0026#39;SELECT * FROM People\u0026#39;\u0026#39;\u0026#39; cur.execute(sql) result = cur.fetchall() table = \u0026#39;\u0026#39;\u0026#39;\u0026lt;table\u0026gt; \u0026lt;thead\u0026gt; \u0026lt;tr\u0026gt;\u0026lt;th\u0026gt;Name\u0026lt;/th\u0026gt;\u0026lt;th\u0026gt;City\u0026lt;/th\u0026gt;\u0026lt;/tr\u0026gt; \u0026lt;/thead\u0026gt; \u0026lt;tbody\u0026gt;\u0026#39;\u0026#39;\u0026#39; if result: for record in result: table \u0026#43;= record[2]) table \u0026#43;= # Drop table sql = \u0026#39;\u0026#39;\u0026#39;DROP TABLE People\u0026#39;\u0026#39;\u0026#39; cur.execute(sql) # Close communication with the database cur.close() conn.close() return table except Exception as e: return e import psycopg2 from platformshconfig import Config def usage_example(): # Create a new Config object to ease reading the Platform.sh environment variables. # You can alternatively use os.environ yourself. config = Config() # The \u0026#39;database\u0026#39; relationship is generally the name of primary SQL database of an application. # That\u0026#39;s not required, but much of our default automation code assumes it.\u0026#39; database = config.credentials(\u0026#39;postgresql\u0026#39;) try: # Connect to the database. conn_params = { \u0026#39;host\u0026#39;: database[\u0026#39;host\u0026#39;], \u0026#39;port\u0026#39;: database[\u0026#39;port\u0026#39;], \u0026#39;dbname\u0026#39;: database[\u0026#39;path\u0026#39;], \u0026#39;user\u0026#39;: database[\u0026#39;username\u0026#39;], \u0026#39;password\u0026#39;: database[\u0026#39;password\u0026#39;] } conn = psycopg2.connect(**conn_params) # Open a cursor to perform database operations. cur = conn.cursor() cur.execute( DROP TABLE IF EXISTS People ) # Creating a table. sql = \u0026#39;\u0026#39;\u0026#39; CREATE TABLE IF NOT EXISTS People ( id SERIAL PRIMARY KEY, name VARCHAR(30) NOT NULL, city VARCHAR(30) NOT NULL ) \u0026#39;\u0026#39;\u0026#39; cur.execute(sql) # Insert data. sql = \u0026#39;\u0026#39;\u0026#39; INSERT INTO People (name, city) VALUES (\u0026#39;Neil Armstrong\u0026#39;, \u0026#39;Moon\u0026#39;), (\u0026#39;Buzz Aldrin\u0026#39;, \u0026#39;Glen Ridge\u0026#39;), (\u0026#39;Sally Ride\u0026#39;, \u0026#39;La Jolla\u0026#39;); \u0026#39;\u0026#39;\u0026#39; cur.execute(sql) # Show table. sql = \u0026#39;\u0026#39;\u0026#39;SELECT * FROM People\u0026#39;\u0026#39;\u0026#39; cur.execute(sql) result = cur.fetchall() table = \u0026#39;\u0026#39;\u0026#39;\u0026lt;table\u0026gt; \u0026lt;thead\u0026gt; \u0026lt;tr\u0026gt;\u0026lt;th\u0026gt;Name\u0026lt;/th\u0026gt;\u0026lt;th\u0026gt;City\u0026lt;/th\u0026gt;\u0026lt;/tr\u0026gt; \u0026lt;/thead\u0026gt; \u0026lt;tbody\u0026gt;\u0026#39;\u0026#39;\u0026#39; if result: for record in result: table \u0026#43;= record[2]) table \u0026#43;= # Drop table sql =  DROP TABLE People  cur.execute(sql) # Close communication with the database cur.close() conn.close() return table except Exception as e: return e import pika from platformshconfig import Config def usage_example(): # Create a new Config object to ease reading the Platform.sh environment variables. # You can alternatively use os.environ yourself. config = Config() # Get the credentials to connect to the RabbitMQ service. credentials = config.credentials(\u0026#39;rabbitmq\u0026#39;) try: # Connect to the RabbitMQ server creds = pika.PlainCredentials(credentials[\u0026#39;username\u0026#39;], credentials[\u0026#39;password\u0026#39;]) parameters = pika.ConnectionParameters(credentials[\u0026#39;host\u0026#39;], credentials[\u0026#39;port\u0026#39;], credentials=creds) connection = pika.BlockingConnection(parameters) channel = connection.channel() # Check to make sure that the recipient queue exists channel.queue_declare(queue=\u0026#39;deploy_days\u0026#39;) # Try sending a message over the channel channel.basic_publish(exchange=\u0026#39;\u0026#39;, routing_key=\u0026#39;deploy_days\u0026#39;, body=\u0026#39;Friday!\u0026#39;) # Receive the message def callback(ch, method, properties, body): print(  [x] Received {} .format(body)) # Tell RabbitMQ that this particular function should receive messages from our \u0026#39;hello\u0026#39; queue channel.basic_consume(\u0026#39;deploy_days\u0026#39;, callback, auto_ack=False) # This blocks on waiting for an item from the queue, so comment it out in this demo script. # print(\u0026#39; [*] Waiting for messages. To exit press CTRL\u0026#43;C\u0026#39;) # channel.start_consuming() connection.close() return   [x] Sent \u0026#39;Friday!\u0026#39;\u0026lt;br/\u0026gt;  except Exception as e: return e from redis import Redis from platformshconfig import Config def usage_example(): # Create a new config object to ease reading the Platform.sh environment variables. # You can alternatively use os.environ yourself. config = Config() # Get the credentials to connect to the Redis service. credentials = config.credentials(\u0026#39;redis\u0026#39;) try: redis = Redis(credentials[\u0026#39;host\u0026#39;], credentials[\u0026#39;port\u0026#39;]) key =  Deploy day  value =  Friday  # Set a value redis.set(key, value) # Read it back test = redis.get(key) return \u0026#39;Found value \u0026lt;strong\u0026gt;{0}\u0026lt;/strong\u0026gt; for key \u0026lt;strong\u0026gt;{1}\u0026lt;/strong\u0026gt;.\u0026#39;.format(test.decode( utf-8 ), key) except Exception as e: return e import pysolr from xml.etree import ElementTree as et from platformshconfig import Config def usage_example(): # Create a new Config object to ease reading the Platform.sh environment variables. # You can alternatively use os.environ yourself. config = Config() # Get the credentials to connect to the Solr service. credentials = config.credentials(\u0026#39;solr\u0026#39;) try: formatted_url = config.formatted_credentials(\u0026#39;solr\u0026#39;, \u0026#39;pysolr\u0026#39;) # Create a new Solr Client using config variables client = pysolr.Solr(formatted_url) # Add a document message = \u0026#39;\u0026#39; doc_1 = {  id : 123,  name :  Valentina Tereshkova  } result0 = client.add([doc_1]) client.commit() message \u0026#43;= \u0026#39;Adding one document. Status (0 is success): {0} \u0026lt;br /\u0026gt;\u0026#39;.format(et.fromstring(result0)[0][0].text) # Select one document query = client.search(\u0026#39;*:*\u0026#39;) message \u0026#43;= documents (1 expected): {0} \u0026lt;br /\u0026gt;\u0026#39;.format(str(query.hits)) # Delete one document result1 = client.delete(doc_1[\u0026#39;id\u0026#39;]) client.commit() message \u0026#43;= one document. Status (0 is success): {0}\u0026#39;.format(et.fromstring(result1)[0][0].text) return message except Exception as e: return e Project templates A number of project templates for Python applications are available on GitHub. Not all of them are proactively maintained but all can be used as a starting point or reference for building your own website or web application. Basic Python 2 Basic Python 2 This template provides the most basic configuration for running a custom Python 2.7 project. It launches a Python application directly rather than using a runner. Python is a general purpose scripting language often used in web development. Services: Python 2 MariaDB 10.2 Redis 5 View the repository on GitHub. Basic Python 3 Basic Python 3 This template provides the most basic configuration for running a custom Python 3.7 project. It launches a Python application directly rather than using a runner. Python is a general purpose scripting language often used in web development. Services: Python 3 MariaDB 10.2 Redis 5 View the repository on GitHub. Django 1 Django 1 This template builds Django 1 on Platform.sh, using the gunicorn application runner. New projects should be built using Django 2, but this project is a reference for existing migrating sites. Django is a Python-based web application framework with a built-in ORM. Version 1 is the legacy support version. Services: Python 2 PostgreSQL 10 View the repository on GitHub. Django 2 Django 2 This template builds Django 2 on Platform.sh, using the gunicorn application runner. Django is a Python-based web application framework with a built-in ORM. Services: Python 3.7 PostgreSQL 10 View the repository on GitHub. Django 3 Django 3 This template builds Django 3 on Platform.sh, using the gunicorn application runner. Django is a Python-based web application framework with a built-in ORM. Services: Python 3.7 PostgreSQL 10 View the repository on GitHub. Flask Flask This template builds a Flask project on Platform.sh, run natively without a separate runner. Flask is a lightweight web microframework for Python. Services: Python 3.7 MariaDB 10.2 Redis 5.0 View the repository on GitHub. MoinMoin MoinMoin This template builds a Moin Moin wiki on Platform.sh. The project doesn\u0026#39;t include Moin Moin itself; rather, it includes build and deploy scripts that will download Moin Moin on the fly. Moin Moin is a Python-based Wiki system that uses flat files on disk for storage. Services: Python 2 View the repository on GitHub. Pelican Pelican This template provides a basic Pelican skeleton. All files are generated at build time, so at runtime only static files need to be served. Pelican is a static site generator written in Python and using Jinja for templating. Services: Python 3.7 View the repository on GitHub. Pyramid Pyramid This template builds Pyramid on Platform.sh. It includes some basic example code to demonstrate how to connect to the database. Pyramid is a web framework written in Python. Services: Python 3.7 View the repository on GitHub. Python 3 running UWSGI Python 3 running UWSGI This template provides the most basic configuration for running a custom Python 3.7 project. It launches the application using the UWSGI application runner. Python is a general purpose scripting language often used in web development. Services: Python 3.7 MariaDB 10.2 Redis 5.0 View the repository on GitHub. Wagtail Wagtail This template builds the Wagtail CMS on Platform.sh, using the gunicorn application runner. Wagtail is a web CMS built using the Django framework for Python. Services: Python 3.7 PostgreSQL 10 View the repository on GitHub.",
        "section": "Languages",
        "subsections": " Supported Support libraries WSGI-based configuration Using the asyncio module Accessing services Project templates  ",
        "image": "",
        "url": "/languages/python.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "07b0599aecee035e107a19f17cdcc6d1",
        "title": "Ruby",
        "description": "",
        "text": " Platform.sh supports deploying any Ruby application. Your application can use any Ruby application server such as Unicorn or Puma and deploying a Rails or a Sinatra app is very straight forward. Supported versions Ruby MRI Grid Dedicated 2.3 2.4 2.5 2.6 2.7 None available Unicorn based Rails configuration In this example, we use Unicorn to run our Ruby application. You could use any Ruby application server such as Puma or Thin. Configure the .platform.app.yaml file with a few key settings as listed below, a complete example is included at the end of this section. Specify the language of your application (available versions are listed above): type:\u0026#39;ruby:2.7\u0026#39; Build your application with the build hook. Assuming you have your dependencies stored in the Gemfile at the root of your application folder to execute build steps: hooks:build:bundleinstall--withoutdevelopmenttestdeploy:RACK_ENV=productionbundleexecrakedb:migrateThese are installed as your project dependencies in your environment. You can also use the dependencies key to install global dependencies theses can be Ruby, Python, NodeJS or PHP libraries. Configure the command you use to start serving your application (this must be a foreground-running process) under the web section, e.g.: web:upstream:socket_family:unixcommands:start: unicorn -l $SOCKET -E production config.ru This assumes you have Unicorn as a dependency in your Gemfile # Use Unicorn as the app server group :production do gem \u0026#39;unicorn\u0026#39; end and that you have a rackup file config.ru at the root of your repository, for example for a rails application you would put: require  rubygems  require ::File.expand_path(\u0026#39;../config/environment\u0026#39;, __FILE__) run Rails.application Define the web locations your application is using: web:locations: / :root: public passthru:trueexpires:1hallow:trueThis configuration asks our web server to handle HTTP requests at “/static” to serve static files stored in /app/static/ folder while everything else are forwarded to your application server. Create any Read/Write mounts. The root file system is read only. You must explicitly describe writable mounts. mounts:tmp:source:localsource_path:tmplogs:source:localsource_path:logsThis setting allows your application writing files to /app/tmp and have logs stored in /app/logs. You can define other read/write mounts (your application code itself being deployed to a read-only file system). Note that the file system is persistent, and when you backup your cluster these mounts get backed-up too. Then, setup the routes to your application in .platform/routes.yaml.  https://{default}/ :type:upstream# the first part should be your project nameupstream: app:http Here is the complete .platform.app.yaml file: name:\u0026#39;app\u0026#39;type: ruby:2.7 web:upstream:socket_family:unixcommands:start: unicorn -l $SOCKET -E production config.ru locations: / :root: public passthru:trueexpires:1hallow:truerelationships:database: database:mysql disk:2048hooks:build:bundleinstall--withoutdevelopmenttestdeploy:RACK_ENV=productionbundleexecrakedb:migratemounts:tmp:source:localsource_path:tmplogs:source:localsource_path:logs Configuring services In this example we assue in the relationships key that we have a mysql instance. To configure it we need to create a .platform/services.yaml with for eample: database:type:mysql:10.4disk:2048 Connecting to services You can define services in your environment. And, link to the services using .platform.app.yaml: relationships:database: mysqldb:mysql By using the following ruby function calls, you can obtain the database details. require  base64  require  json  relationships= JSON.parse(Base64.decode64(ENV[\u0026#39;PLATFORM_RELATIONSHIPS\u0026#39;])) Which should give you something like: {  database  : [ {  path  :  main ,  query  : {  is_master  : true },  port  : 3306,  username  :  user ,  password  :   ,  host  :  database.internal ,  ip  :  246.0.241.50 ,  scheme  :  mysql  } ] } Project templates A number of project templates for Ruby applications and typical configurations are available on GitHub. Not all of them are proactively maintained but all can be used as a starting point or reference for building your own website or web application. Platform.sh also provides a helper library for Ruby applications that simplifies presenting environment information to your application. It is not required to run Ruby applications on Platform.sh but is recommended. Ruby on Rails Ruby on Rails This template builds Ruby on Rails 5 on Platform.sh. It includes a bridge library that will auto-configure most databases and services. Rails is an opinionated rapid application development framework written in Ruby. Services: Ruby 2.6 Postgresql 11 View the repository on GitHub.",
        "section": "Languages",
        "subsections": " Supported versions  Ruby MRI   Unicorn based Rails configuration Configuring services Connecting to services Project templates  ",
        "image": "",
        "url": "/languages/ruby.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "36f0c5b59b7cfdce4688bb02ab9eb45f",
        "title": "Tideways",
        "description": "",
        "text": " Platform.sh supports Tideways APM for PHP. This functionality is only available on PHP 7.0 and later. Note: The upstream now maintains two versions for Tideways, and both plugins are available on Platform.sh: tideways_xhprof: The open source version, therefore no licensing is required (On the downside, less integration services are available). You can use it in combination with xhprof UI. tideways: The bundle proprietary full version of the product and plugins, which the rest of the guide is mostly aimed to cover. Get Started 1. Get your license key Sign up at https://tideways.com and get your license key. 2. Add your license key Add your Tideways license key as a project level variable: platform variable:create --visible-build false php:tideways.api_key --value \u0026#39;\u0026lt;your-license-key\u0026gt;\u0026#39; 3. Enable the Tideways extension Enable the Tideways extension in your .platform.app.yaml as follows: runtime:extensions:- tidewaysEnabling the extension will also activate the Tideways background process. Push the changes to your Platform.sh environment to enable Tideways as follows: git add .platform.app.yaml git commit -m  Enable Tideways.  git push Tideways should now be enabled. Give it a few hours to a day to get a decent set of data before checking your Tideways dashboard. Deployment Integration Tideways integrates with Platform.sh deployment hooks and provides performance comparisons before and after deployments were released. You can find the Platform.sh CLI command to register this hook for your application in Tideways “Application Settings” screen under the section “Exports \u0026amp; Integrations”. Here is an example: platform integration:add --type=webhook --url= https://app.tideways.io/api/events/external/1234/abcdefghijklmnopqrstuvwxyz1234567890 ",
        "section": "Profiling",
        "subsections": " Get Started  1. Get your license key 2. Add your license key 3. Enable the Tideways extension   Deployment Integration  ",
        "image": "",
        "url": "/integrations/profiling/tideways.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "rank": 1,
        "documentId": "cd3db3fee01d619f1561605536c55ac2",
        "title": "Webhooks",
        "description": "",
        "text": " Webhooks allow you to host a script yourself externally that receives the same payload as an activity script and responds to the same events, but can be hosted on your own server in your own language. Setup platform integration:add --type=webhook --url=A-URL-THAT-CAN-RECEIVE-THE-POSTED-JSON The webhook URL will receive a POST message for every “Activity” that is triggered, and the message will contain complete information about the entire state of the project at that time. In practice most of the message can be ignored but is available if needed. The most commonly used values are documented below. It’s also possible to set the integration to only send certain activity types, or only activities on certain branches. The CLI will prompt you to specify which to include or exclude. Leave at the default values to get all events on all environments in a project. Webhook schema See the activity script reference for a description of the webhook payload. Validate the integration You can then verify that your integration is functioning properly using the CLI command platform integration:validate",
        "section": "Activity scripts",
        "subsections": " Setup Webhook schema Validate the integration  ",
        "image": "",
        "url": "/integrations/activity/webhooks.html",
        "relurl": ""
    }
]